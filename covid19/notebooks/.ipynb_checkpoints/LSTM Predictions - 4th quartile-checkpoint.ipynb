{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import dns\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from h3 import h3\n",
    "import datetime\n",
    "import imageio\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras import backend as K \n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/346-ver19_ips_processed.pickle')\n",
    "\n",
    "geo_clinicas = {'FUNDACION VALLE DEL LILI':[3.372909 ,-76.526586] , \n",
    "                'CENTRO MEDICO IMBANACO': [3.423411, -76.543959], \n",
    "                'CLINICA VERSALLES SA': [3.464051, -76.528204],  \n",
    "                'HOSPITAL UNIVERSITARIO DEL VALLE EVARISTO GARCIA': [3.429724, -76.544600]}\n",
    "\n",
    "clinicas =['FUNDACION VALLE DEL LILI',\n",
    "          'CENTRO MEDICO IMBANACO',\n",
    "          'CLINICA VERSALLES SA',\n",
    "          'HOSPITAL UNIVERSITARIO DEL VALLE EVARISTO GARCIA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a map filling the city boundary with h3 hexagons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At resolution 8 the zone is filled with  153 hexagons\n"
     ]
    }
   ],
   "source": [
    "resolution = 8\n",
    "# Get the city boundary and fill it with hexagons\n",
    "city_boundary_link = \"https://raw.githubusercontent.com/finkej/santiago_de_cali/master/mapa_cali/limite_municipal/limite_municipal.json\"\n",
    "city_boundary = gpd.read_file(city_boundary_link)\n",
    "\n",
    "city_json = json.loads(city_boundary.to_json())['features'][0]['geometry']\n",
    "\n",
    "list_hexagons = list(h3.polyfill(city_json,\n",
    "                                 res=resolution, geo_json_conformant=True))\n",
    "\n",
    "print(\"At resolution {}\".format(resolution),\n",
    "      \"the zone is filled with \", len(list_hexagons), \"hexagons\")\n",
    "\n",
    "\n",
    "def reverse_lat_lon(hex_coords):\n",
    "    return [[coord[1], coord[0]] for coord in hex_coords]\n",
    "\n",
    "\n",
    "city_hex = gpd.GeoDataFrame({\"hex_id\": list_hexagons})\n",
    "city_hex['geometry'] = city_hex.hex_id.apply(lambda x: Polygon(\n",
    "    reverse_lat_lon(h3.h3_to_geo_boundary(x, geo_json=False))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_coordinates(x):\n",
    "    return Polygon(h3.h3_to_geo_boundary(x,geo_json=True))\n",
    "\n",
    "def number_cases_df(df):\n",
    "    data = df.groupby(['hex_id'])['id'].count().reset_index(name='cases')\n",
    "    data[\"geometry\"] = data['hex_id'].apply(hex_to_coordinates)\n",
    "    data = gpd.GeoDataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poblacion por resolución hexagono (tomado de Hernán)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = pd.read_excel('maps-cali/Pob_Mzna.xlsx', usecols=['ID_Mzna', 'Pob_total'])\n",
    "blocks_geo = pd.read_excel('maps-cali/Mzna_XY.xlsx', usecols=['MANZ_CCNCT', 'x', 'y'])\n",
    "\n",
    "blocks = pd.merge(left=blocks, right=blocks_geo, left_on='ID_Mzna', right_on='MANZ_CCNCT')\n",
    "blocks = blocks[['ID_Mzna', 'Pob_total', 'x', 'y']].rename(columns={'ID_Mzna':'id', 'Pob_total':'population', 'x':'lng', 'y':'lat'})\n",
    "\n",
    "blocks['hex_id'] = blocks.apply(lambda row: h3.geo_to_h3(row[\"lat\"], row[\"lng\"], resolution), axis = 1)\n",
    "\n",
    "population_per_hex = blocks.groupby('hex_id')['population'].sum().reset_index(name='population')\n",
    "\n",
    "population_per_hex[\"geometry\"] = population_per_hex['hex_id'].apply(hex_to_coordinates)\n",
    "population_per_hex = gpd.GeoDataFrame(population_per_hex)\n",
    "\n",
    "\n",
    "path_config_maps = 'data/configuraciones_mapas.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_metrics(y_total, preds):\n",
    "    ''' Función que permite calcular las métricas de error: mse, rmse, mae,mape'''\n",
    "    mse = mean_squared_error(y_total, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_total, preds)\n",
    "    mape = mean_absolute_percentage_error(y_total, preds)\n",
    "    print(\"MSE: %f\" % (mse))\n",
    "    print(\"RMSE: %f\" % (rmse))\n",
    "    print(\"MAE: %f\" % (mae))\n",
    "    print(\"MAPE: %f\" % (mape))\n",
    "    print()\n",
    "    \n",
    "def error_analysis(df, title, path,scaled = False,ac='semana -1',pr = 'predictions'):\n",
    "    ''' Función realiza un análisis de los errores en los diferentes cuartiles'''\n",
    "    \n",
    "    \n",
    "    #y_total = data_weeks_no_outlier['semana -1']\n",
    "    #preds = data_weeks_no_outlier['predictions']\n",
    "    \n",
    "\n",
    "    if scaled: actual=0\n",
    "    else: actual=ac\n",
    "    y_total = df[actual]\n",
    "    preds = df[pr]\n",
    "    error_metrics(y_total, preds)\n",
    "    \n",
    "    quantiles= list(y_total.quantile([0 ,.25, .5, .75, 1]))  \n",
    "    queries = [\n",
    "               y_total < quantiles[1], \n",
    "               (y_total >= quantiles[1]) & (y_total< quantiles[2]),\n",
    "               (y_total >= quantiles[2]) & (y_total < quantiles[3]),\n",
    "               y_total >= quantiles[3]\n",
    "              ]\n",
    "    \n",
    "    df_q = [df[queries[0]],df[queries[1]],df[queries[2]],df[queries[3]]]\n",
    "    for i,dfm in enumerate(df_q):\n",
    "        print('Quantile {}, between {} and {}'.format(i+1,quantiles[i],quantiles[i+1]))\n",
    "        error_metrics(dfm[actual], dfm[pr])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    y = y_total\n",
    "    ax.scatter(y, preds)\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "    ax.plot([y.min(), y.max()], [y.min()*1.25, y.max()*1.25], 'b--', lw=1)\n",
    "    ax.plot([y.min(), y.max()], [y.min()*0.75, y.max()*0.75], 'b--', lw=1)\n",
    "    ax.set_xlabel('Real')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.title(title)\n",
    "    plt.savefig(path, dpi=500)\n",
    "    plt.show()\n",
    "    \n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    ''' métrica MAPE hecha manualmente para trabajar personalmente con las divisiones por cero'''\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    ape = (y_true - y_pred) / y_true\n",
    "    ape_no_inf = [x if np.abs(x) != float('inf') else 0 for x in ape]\n",
    "    return np.mean(np.abs(ape_no_inf)) * 100\n",
    "\n",
    "def plt_map_image(data,col,title,ax):\n",
    "    '''Función que permite graficar rápidamente los hexagonos de Cali en blanco con algún otro dato'''\n",
    "    city_hex.plot(color=\"white\", edgecolor='0.1', linewidth=0.4, figsize=(12, 7),ax=ax)\n",
    "    data.plot(column=col, edgecolor='0.6', linewidth=0.3, cmap='YlOrRd', legend=True, ax=ax);\n",
    "    ax.set_title(title)\n",
    "    \n",
    "### UTILITY FUNCTIONS FOR TARGET SCALING ###\n",
    "def scale_target(y, mean, std):\n",
    "    return (y - mean)/std\n",
    "\n",
    "def reverse_target(pred, mean, std): \n",
    "    return pred*std + mean\n",
    "\n",
    "def plot_results(predicted_data, true_data): \n",
    "    fig = plt.figure(facecolor='white') \n",
    "    ax = fig.add_subplot(111) \n",
    "    ax.plot(true_data, label='True Data') \n",
    "    plt.plot(predicted_data, label='Prediction') \n",
    "    plt.legend() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de hexágonos del ultimo quartil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cases = number_cases_df(df)\n",
    "quantiles= list(total_cases.cases.quantile([0 ,.25, .5, .75, 1]))  \n",
    "quartile4_names = list(total_cases[total_cases.cases > quantiles[3]].hex_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long short term model - LSTM modifications and analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_columns(df,n_col,days):\n",
    "    '''Función que separa los datos de test y training, no uso la función de sklearn debido a que no necesito datos aleatorios,\n",
    "        si no que debo seguir las series de tiempo.'''\n",
    "    data_days = df.copy()\n",
    "    y= data_days.iloc[:,n_col]\n",
    "    x= data_days.drop(data_days.columns[n_col], axis=1)\n",
    "    X_test,y_test = x.iloc[:days],y.iloc[:days]\n",
    "    X_train,y_train = x.iloc[days:],y.iloc[days:]\n",
    "    X_train = X_train.values.reshape((X_train.shape[0],1, X_train.shape[1]))\n",
    "    X_test= X_test.values.reshape((X_test.shape[0],1, X_test.shape[1]))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def gen_train_data_per_block(df,block,clinica=None, population=False):\n",
    "    ''' Función que genera un df con toda la información separada por bloques de días para usar en LSTM fácilmente'''\n",
    "    if clinica==None: filtered = df.copy()\n",
    "    else: filtered = df[df['nom_upgd']==clinica]\n",
    "        \n",
    "    if population: change_name = 'normalized_cases'\n",
    "    else: change_name = 'cases'\n",
    "    \n",
    "    df_outer = city_hex.copy()\n",
    "    df_dates = df.sort_values(by='date',ascending=False)\n",
    "    df_dates = df_dates[df_dates.date < '2020-07-01']\n",
    "    df_per_block = []\n",
    "    \n",
    "    for idx, day in df.groupby(pd.Grouper(key=\"date\", freq=block)):\n",
    "        df_per_block.append(number_cases_df(pd.DataFrame(day)))\n",
    "    \n",
    "        if population:\n",
    "            data = df_per_block[-1].iloc[:,:-1]\n",
    "            data = pd.merge(left=data, right=population_per_hex, on='hex_id')\n",
    "            data['normalized_cases'] = data['cases'] / data['population']\n",
    "            data.drop(columns=['cases','population'],inplace=True)\n",
    "            #data = df_outer.rename(columns={'normalized_cases':'cases'})\n",
    "            df_per_block[-1] = gpd.GeoDataFrame(data)     \n",
    "        \n",
    "        df_per_block[-1]=df_per_block[-1].rename(columns={change_name: idx})\n",
    "        \n",
    "    for i,daf in enumerate(df_per_block):        \n",
    "        df_outer = pd.merge(df_outer, daf, on='hex_id', how='outer')\n",
    "        df_outer['geometry'] = df_outer.apply(lambda row: row['geometry_y'] if row['geometry_y']!=None else row['geometry_x'], axis=1)\n",
    "        df_outer.drop(columns=['geometry_x', 'geometry_y'],inplace=True)\n",
    "        #df_outer = df_outer.rename(columns={change_name: 'semana {0}'.format(i)})\n",
    "\n",
    "        \n",
    "    df_outer.iloc[:,1:-1] = df_outer.iloc[:,1:-1].fillna(0)\n",
    "    \n",
    "    df_outer = df_outer.T\n",
    "    df_outer.columns = df_outer.iloc[0].values\n",
    "    df_outer = df_outer.drop(['hex_id','geometry'])\n",
    "    df_outer = df_outer.sort_index(ascending=False)\n",
    "    return df_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamado de la función para generar los datos de a un día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_days = gen_train_data_per_block(df,'1D',population = True)\n",
    "data_days = data_days[(data_days.index > datetime.date(2020, 3, 1)) & (data_days.index < datetime.date(2020, 6, 30))]\n",
    "data_days = data_days[quartile4_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_days.iloc[30:].sum(axis=1).plot(figsize=(16,4),legend=True)\n",
    "data_days.iloc[:30].sum(axis=1).plot(figsize=(16,4),legend=True) # 10% is used for thraining data which is approx 2017 data\n",
    "plt.legend(['Training set (Approx before June)','Test set (Approax June and beyond)'])\n",
    "plt.title('COVID cases in 4th quartile')\n",
    "plt.savefig(\"imgs/ips_predictions/cases_by_day_with_population_4q.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nota: se pueden mejorar la siguientes dos funciones convirtiendolas en una, pero queda mas complicado al entender.\n",
    "##       se propone usar herencia si se utilizarán en el código final.\n",
    "\n",
    "def lstm_feature_importance(data_days2,n_test,name_error,path_error,name_map,path_map):\n",
    "    '''Función que realiza las predicciones con los datos completos, entrega resultado de predicciones,\n",
    "       también devuelve los mejores hexágonos para la prediccion en cada sector.'''\n",
    "    data_days = data_days2.copy()\n",
    "    ## n_col is an int between 0 and len(data_days.columns)\n",
    "    dict_pred = {}\n",
    "    for n_col in range(len(data_days.columns)):\n",
    "        X_train, y_train, X_test, y_test = split_data_columns(data_days,n_col,n_test)\n",
    "\n",
    "        ### SCALE TARGET ###\n",
    "\n",
    "        mean_train = y_train.mean()\n",
    "        std_train = y_train.std()\n",
    "\n",
    "        y_train_seq = scale_target(y_train, mean_train, std_train)\n",
    "        y_test_seq = scale_target(y_test, mean_train, std_train)\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train.reshape(-1,len(X_train[0][0]))).reshape(X_train.shape).astype('float32')\n",
    "        X_test = scaler.transform(X_test.reshape(-1,len(X_train[0][0]))).reshape(X_test.shape).astype('float32')\n",
    "        print(y_train_seq.shape,  y_test_seq.shape)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(Dropout(0.2))\n",
    "        #   model.add(LSTM(70))\n",
    "        #    model.add(Dropout(0.3))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        print ('model compiled {}'.format(n_col))\n",
    "\n",
    "        print (model.summary())\n",
    "\n",
    "        es = EarlyStopping(patience=50, verbose=1, min_delta=0.001, monitor='val_loss', mode='auto')  \n",
    "        model.fit(X_train, y_train_seq, batch_size=8, epochs=100, validation_data = (X_test, y_test),callbacks=[es]) ## REVISAR ESTE VALIDATION DATA\n",
    "\n",
    "\n",
    "        ### MAKE PREDICTION ON TEST ###\n",
    "\n",
    "        predicted_cases = reverse_target(model.predict(X_test).ravel(), mean_train, std_train)\n",
    "\n",
    "\n",
    "        df_y = pd.DataFrame(y_test)\n",
    "        df_y = df_y.assign(pred=predicted_cases)\n",
    "\n",
    "        background = X_test[:]\n",
    "        explainer = shap.DeepExplainer(model,background)\n",
    "        shap_values = explainer.shap_values(background)\n",
    "        names = list(data_days.drop(data_days.columns[n_col], axis=1).columns)\n",
    "\n",
    "        reshaped_shap_values = np.reshape(shap_values[0], (len(background), len(data_days.columns)-1))\n",
    "        reshaped_background = np.reshape(background, (len(background), len(data_days.columns)-1))\n",
    "\n",
    "        vals= np.abs(reshaped_shap_values).mean(0)\n",
    "        feature_importance = pd.DataFrame(list(zip(names, vals)), columns=['col_name','feature_importance_vals'])\n",
    "        feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "        dict_pred[data_days.columns[n_col]] = [df_y.copy(),feature_importance]\n",
    "\n",
    "        K.clear_session()\n",
    "        \n",
    "    pred_lstm = city_hex.query('hex_id in (@quartile4_names)').copy()\n",
    "    pred_lstm = pred_lstm.assign(real = [float(1) for _ in range(len(data_days.columns))], pred = [float(1) for _ in range(len(data_days.columns))])\n",
    "\n",
    "    for index, row in pred_lstm.iterrows():\n",
    "        sum_real = dict_pred[row['hex_id']][0][row['hex_id']].sum()\n",
    "        sum_pred = dict_pred[row['hex_id']][0]['pred'].sum()\n",
    "\n",
    "        pred_lstm.at[index,'real'] = sum_real\n",
    "        pred_lstm.at[index,'pred'] = sum_pred\n",
    "\n",
    "    pred_lstm = pred_lstm[pred_lstm['hex_id'] != '8866f1c427fffff']\n",
    "\n",
    "    data = pd.merge(left=pred_lstm, right=population_per_hex.iloc[:,:-1], on='hex_id')\n",
    "    pred_lstm_cases = pred_lstm.assign(pred = list(data['pred']*data['population']),real = list(data['real']*data['population']))\n",
    "\n",
    "\n",
    "    error_analysis(pred_lstm_cases, name_error, path=path_error,scaled = False,ac='real',pr = 'pred')\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(12, 6))\n",
    "    plt_map_image(pred_lstm_cases,'real','Real'+name_map,axs[0])\n",
    "    plt_map_image(pred_lstm_cases,'pred','Pred '+name_map,axs[1])\n",
    "    plt.savefig(path_map, dpi=500)\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    return dict_pred,pred_lstm,pred_lstm_cases\n",
    "\n",
    "def lstm_hex_selected(data_days, dict_pred_sel_hexa,n_test,n_hexa, name_error,path_error,name_map,path_map,ring=False):\n",
    "    '''Función que realiza las predicciones con los mejores hexágonos que se generaron de la función <lstm_feature_importance>'''\n",
    "    ## n_col is an int between 0 and data_days.columns-1\n",
    "    dict_pred = {}\n",
    "    \n",
    "    for n_col in range(len(data_days.columns)):\n",
    "        \n",
    "        if not ring:\n",
    "            col_names =  list(dict_pred_sel_hexa[data_days.columns[n_col]][1][:n_hexa].col_name)\n",
    "            col_names.append(data_days.columns[n_col])\n",
    "        else: \n",
    "            col_names_temp = list(h3.hex_ring(data_days.columns[n_col],n_hexa))\n",
    "            col_names_temp.append(data_days.columns[n_col])\n",
    "            col_names = [x for x in col_names_temp if x in data_days.columns]\n",
    "            \n",
    "            if len(col_names)==1:\n",
    "                col_names =  list(dict_pred_sel_hexa[data_days.columns[n_col]][1][:n_hexa].col_name)\n",
    "                col_names.append(data_days.columns[n_col])\n",
    "                \n",
    "        X_train, y_train, X_test, y_test = split_data_columns(data_days[col_names],len(col_names)-1,n_test) ## Siempre es el 30 debido a que estoy tomando 30 mejores atributos \n",
    "\n",
    "        ### SCALE TARGET ###\n",
    "\n",
    "        mean_train = y_train.mean()\n",
    "        std_train = y_train.std()\n",
    "\n",
    "        y_train_seq = scale_target(y_train, mean_train, std_train)\n",
    "        y_test_seq = scale_target(y_test, mean_train, std_train)\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train.reshape(-1,len(X_train[0][0]))).reshape(X_train.shape).astype('float32')\n",
    "        X_test = scaler.transform(X_test.reshape(-1,len(X_train[0][0]))).reshape(X_test.shape).astype('float32')\n",
    "        print(y_train_seq.shape,  y_test_seq.shape)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(Dropout(0.2))\n",
    "        #   model.add(LSTM(70))\n",
    "        #    model.add(Dropout(0.3))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        print ('model compiled {}'.format(n_col))\n",
    "\n",
    "        print (model.summary())\n",
    "\n",
    "        es = EarlyStopping(patience=30, verbose=1, min_delta=0.001, monitor='val_loss', mode='auto')  \n",
    "        model.fit(X_train, y_train_seq, batch_size=4, epochs=50, validation_data = (X_test, y_test),callbacks=[es]) ## REVISAR ESTE VALIDATION DATA\n",
    "\n",
    "\n",
    "        ### MAKE PREDICTION ON TEST ###\n",
    "\n",
    "        predicted_cases = reverse_target(model.predict(X_test).ravel(), mean_train, std_train)\n",
    "\n",
    "\n",
    "        df_y = pd.DataFrame(y_test)\n",
    "        df_y = df_y.assign(pred=predicted_cases)\n",
    "\n",
    "\n",
    "        dict_pred[data_days.columns[n_col]] = df_y.copy()\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "    pred_lstm = city_hex.query('hex_id in (@quartile4_names)').copy()\n",
    "    pred_lstm = pred_lstm.assign(real = [float(1) for _ in range(len(data_days.columns))], pred = [float(1) for _ in range(len(data_days.columns))])\n",
    "\n",
    "    for index, row in pred_lstm.iterrows():\n",
    "        sum_real = dict_pred[row['hex_id']][row['hex_id']].sum()\n",
    "        sum_pred = dict_pred[row['hex_id']]['pred'].sum()\n",
    "\n",
    "        pred_lstm.at[index,'real'] = sum_real\n",
    "        pred_lstm.at[index,'pred'] = sum_pred\n",
    "\n",
    "\n",
    "    pred_lstm = pred_lstm[pred_lstm['hex_id'] != '8866f1c427fffff']\n",
    "\n",
    "    data = pd.merge(left=pred_lstm, right=population_per_hex.iloc[:,:-1], on='hex_id')\n",
    "    pred_lstm_cases = pred_lstm.assign(pred = list(data['pred']*data['population']),real = list(data['real']*data['population']))\n",
    "\n",
    "\n",
    "    error_analysis(pred_lstm_cases, name_error, path=path_error,scaled = False,ac='real',pr = 'pred')\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(12, 6))\n",
    "    plt_map_image(pred_lstm_cases,'real','Real'+name_map,axs[0])\n",
    "    plt_map_image(pred_lstm_cases,'pred','Pred '+name_map,axs[1])\n",
    "    plt.savefig(path_map, dpi=500)\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    return dict_pred,pred_lstm,pred_lstm_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,) (15,)\n",
      "model compiled 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 8s - loss: 1.148 - ETA: 0s - loss: 0.809 - 1s 8ms/step - loss: 0.7749 - val_loss: 0.4801\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - 0s 579us/step - loss: 0.5483 - val_loss: 0.6591\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.545 - 0s 494us/step - loss: 0.5239 - val_loss: 0.6924\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - 0s 684us/step - loss: 0.5118 - val_loss: 0.8186\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - 0s 484us/step - loss: 0.4579 - val_loss: 0.6484\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.383 - ETA: 0s - loss: 0.465 - 0s 665us/step - loss: 0.4609 - val_loss: 0.6129\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - 0s 522us/step - loss: 0.4329 - val_loss: 0.6590\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.401 - 0s 541us/step - loss: 0.4210 - val_loss: 0.6529\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - ETA: 0s - loss: 0.416 - 0s 807us/step - loss: 0.4026 - val_loss: 0.6723\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.415 - 0s 827us/step - loss: 0.3951 - val_loss: 0.6523\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 598us/step - loss: 0.3567 - val_loss: 0.6604\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.385 - 0s 570us/step - loss: 0.3595 - val_loss: 0.7923\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.299 - 0s 532us/step - loss: 0.3241 - val_loss: 0.8627\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.490 - 0s 570us/step - loss: 0.3101 - val_loss: 0.8401\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.286 - 0s 636us/step - loss: 0.3032 - val_loss: 1.0402\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.332 - 0s 1ms/step - loss: 0.3200 - val_loss: 1.1934\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - 0s 475us/step - loss: 0.2885 - val_loss: 0.9294\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - 0s 484us/step - loss: 0.2830 - val_loss: 1.2440\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.248 - 0s 627us/step - loss: 0.2464 - val_loss: 1.0875\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - 0s 617us/step - loss: 0.2515 - val_loss: 1.1400\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.506 - 0s 579us/step - loss: 0.2496 - val_loss: 1.2033\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.232 - 0s 684us/step - loss: 0.2312 - val_loss: 1.2043\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.212 - 0s 798us/step - loss: 0.2365 - val_loss: 1.4943\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - 0s 684us/step - loss: 0.2154 - val_loss: 1.2439\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.210 - 0s 760us/step - loss: 0.2196 - val_loss: 1.4746\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.214 - 0s 703us/step - loss: 0.2123 - val_loss: 1.7084\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - 0s 475us/step - loss: 0.2060 - val_loss: 1.3878\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.196 - 0s 798us/step - loss: 0.1919 - val_loss: 1.3395\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.179 - 0s 760us/step - loss: 0.1775 - val_loss: 1.3282\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.548 - ETA: 0s - loss: 0.201 - 0s 912us/step - loss: 0.1802 - val_loss: 1.4829\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - 0s 589us/step - loss: 0.1721 - val_loss: 1.4533\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.178 - 0s 883us/step - loss: 0.1844 - val_loss: 1.4790\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.177 - 0s 836us/step - loss: 0.1745 - val_loss: 1.4990\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.179 - 0s 693us/step - loss: 0.1714 - val_loss: 1.6117\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - 0s 542us/step - loss: 0.1621 - val_loss: 1.4573\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.151 - 0s 750us/step - loss: 0.1537 - val_loss: 1.4581\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - 0s 465us/step - loss: 0.1573 - val_loss: 1.5906\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 437us/step - loss: 0.1497 - val_loss: 1.4279\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 456us/step - loss: 0.1373 - val_loss: 1.6949\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 456us/step - loss: 0.1362 - val_loss: 1.5015\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - 0s 579us/step - loss: 0.1380 - val_loss: 1.5498\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 542us/step - loss: 0.1253 - val_loss: 1.6633\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - 0s 513us/step - loss: 0.1302 - val_loss: 1.5411\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 522us/step - loss: 0.1376 - val_loss: 1.4807\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - 0s 503us/step - loss: 0.1348 - val_loss: 1.6591\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 522us/step - loss: 0.1078 - val_loss: 1.5107\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 447us/step - loss: 0.1078 - val_loss: 1.6751\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 465us/step - loss: 0.1211 - val_loss: 1.4965\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - 0s 551us/step - loss: 0.1163 - val_loss: 1.5381\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 456us/step - loss: 0.1157 - val_loss: 1.5568\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 427us/step - loss: 0.1793 - val_loss: 1.6366\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.578 - 1s 7ms/step - loss: 0.7443 - val_loss: 0.5593\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.831 - 0s 503us/step - loss: 0.4814 - val_loss: 1.1291\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.007 - 0s 494us/step - loss: 0.4385 - val_loss: 0.9563\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.786 - 0s 494us/step - loss: 0.3639 - val_loss: 0.8554\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - 0s 437us/step - loss: 0.3362 - val_loss: 0.9682\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - 0s 418us/step - loss: 0.3120 - val_loss: 0.8913\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.594 - 0s 447us/step - loss: 0.3005 - val_loss: 0.8939\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.609 - 0s 456us/step - loss: 0.2746 - val_loss: 0.9238\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - 0s 484us/step - loss: 0.2587 - val_loss: 1.0563\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - 0s 484us/step - loss: 0.2485 - val_loss: 0.6493\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 484us/step - loss: 0.2389 - val_loss: 0.8554\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - 0s 484us/step - loss: 0.2344 - val_loss: 0.9238\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 466us/step - loss: 0.2104 - val_loss: 0.8789\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 485us/step - loss: 0.2008 - val_loss: 0.9107\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - 0s 485us/step - loss: 0.2054 - val_loss: 0.7670\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - 0s 437us/step - loss: 0.1849 - val_loss: 0.9425\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - 0s 589us/step - loss: 0.1817 - val_loss: 0.8145\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 475us/step - loss: 0.1727 - val_loss: 0.8310\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.441 - 0s 484us/step - loss: 0.1649 - val_loss: 0.8718\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 456us/step - loss: 0.1576 - val_loss: 0.8310\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.442 - 0s 494us/step - loss: 0.1642 - val_loss: 0.8713\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - 0s 608us/step - loss: 0.1523 - val_loss: 1.0222\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 437us/step - loss: 0.1427 - val_loss: 0.8214\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - 0s 418us/step - loss: 0.1492 - val_loss: 0.7677\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 418us/step - loss: 0.1316 - val_loss: 0.8575\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - 0s 437us/step - loss: 0.1346 - val_loss: 0.8818\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 409us/step - loss: 0.1217 - val_loss: 0.7110\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 475us/step - loss: 0.1280 - val_loss: 0.7984\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 465us/step - loss: 0.1344 - val_loss: 0.6780\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 494us/step - loss: 0.1372 - val_loss: 1.2509\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - 0s 456us/step - loss: 0.1249 - val_loss: 0.7304\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - 0s 532us/step - loss: 0.1223 - val_loss: 1.0195\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - 0s 456us/step - loss: 0.1193 - val_loss: 0.9170\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - 0s 494us/step - loss: 0.1053 - val_loss: 0.8002\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 465us/step - loss: 0.1037 - val_loss: 1.1635\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - 0s 465us/step - loss: 0.0971 - val_loss: 0.9200\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 437us/step - loss: 0.0969 - val_loss: 0.9672\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 428us/step - loss: 0.0937 - val_loss: 0.8959\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - 0s 456us/step - loss: 0.0939 - val_loss: 1.1944\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - 0s 418us/step - loss: 0.0851 - val_loss: 0.7726\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 475us/step - loss: 0.0856 - val_loss: 0.9735\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - 0s 409us/step - loss: 0.0767 - val_loss: 0.9709\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 456us/step - loss: 0.0745 - val_loss: 0.9267\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 390us/step - loss: 0.0833 - val_loss: 0.9603\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 428us/step - loss: 0.0794 - val_loss: 1.0225\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 418us/step - loss: 0.0733 - val_loss: 0.9704\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 456us/step - loss: 0.0594 - val_loss: 1.1411\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 494us/step - loss: 0.0714 - val_loss: 1.0764\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - 0s 446us/step - loss: 0.0656 - val_loss: 0.9443\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 427us/step - loss: 0.0592 - val_loss: 0.9119\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - 0s 446us/step - loss: 0.0749 - val_loss: 0.9962\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 6s - loss: 0.315 - 1s 7ms/step - loss: 0.7167 - val_loss: 0.2910\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - 0s 513us/step - loss: 0.5344 - val_loss: 0.6548\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.324 - 0s 437us/step - loss: 0.4754 - val_loss: 0.5589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 427us/step - loss: 0.4294 - val_loss: 0.5538\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.585 - 0s 437us/step - loss: 0.4276 - val_loss: 0.4337\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - 0s 408us/step - loss: 0.3874 - val_loss: 0.5456\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.273 - 0s 418us/step - loss: 0.3679 - val_loss: 0.5935\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.142 - 0s 921us/step - loss: 0.3254 - val_loss: 0.5083\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - 0s 408us/step - loss: 0.3150 - val_loss: 0.5871\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.119 - 0s 427us/step - loss: 0.2900 - val_loss: 0.5428\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 428us/step - loss: 0.2716 - val_loss: 0.6265\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - 0s 427us/step - loss: 0.2614 - val_loss: 0.6654\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - 0s 437us/step - loss: 0.2313 - val_loss: 0.4827\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - 0s 408us/step - loss: 0.2425 - val_loss: 0.5889\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 399us/step - loss: 0.2305 - val_loss: 0.6972\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 446us/step - loss: 0.2108 - val_loss: 0.5835\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 437us/step - loss: 0.1989 - val_loss: 0.5278\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - 0s 446us/step - loss: 0.1862 - val_loss: 0.7571\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.725 - 0s 399us/step - loss: 0.1765 - val_loss: 0.5964\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 446us/step - loss: 0.1846 - val_loss: 0.6720\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 418us/step - loss: 0.1717 - val_loss: 0.6468\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - 0s 437us/step - loss: 0.1507 - val_loss: 0.6820\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 532us/step - loss: 0.1494 - val_loss: 0.6510\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - 0s 447us/step - loss: 0.1502 - val_loss: 0.7102\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - 0s 418us/step - loss: 0.1440 - val_loss: 0.8142\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - 0s 551us/step - loss: 0.1367 - val_loss: 0.7601\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - 0s 446us/step - loss: 0.1361 - val_loss: 0.7361\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - 0s 418us/step - loss: 0.1181 - val_loss: 0.7352\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - 0s 427us/step - loss: 0.1244 - val_loss: 0.7936\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 418us/step - loss: 0.1076 - val_loss: 0.6803\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 399us/step - loss: 0.1091 - val_loss: 0.6578\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - 0s 456us/step - loss: 0.0916 - val_loss: 0.7865\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 437us/step - loss: 0.0956 - val_loss: 0.7059\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 437us/step - loss: 0.1004 - val_loss: 0.6871\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - 0s 418us/step - loss: 0.1018 - val_loss: 0.5791\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 409us/step - loss: 0.0970 - val_loss: 0.7442\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - 0s 456us/step - loss: 0.0903 - val_loss: 0.7435\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - 0s 513us/step - loss: 0.0903 - val_loss: 0.7682\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - 0s 475us/step - loss: 0.0867 - val_loss: 0.6803\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 466us/step - loss: 0.0781 - val_loss: 0.7759\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - 0s 456us/step - loss: 0.0732 - val_loss: 0.7421\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 446us/step - loss: 0.0718 - val_loss: 0.7070\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 418us/step - loss: 0.0708 - val_loss: 0.8367\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 447us/step - loss: 0.0739 - val_loss: 0.5838\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 409us/step - loss: 0.0663 - val_loss: 0.5720\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 513us/step - loss: 0.0687 - val_loss: 0.7313\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 427us/step - loss: 0.0603 - val_loss: 0.6338\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 465us/step - loss: 0.0695 - val_loss: 0.7514\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 475us/step - loss: 0.0628 - val_loss: 0.5868\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - 0s 484us/step - loss: 0.0608 - val_loss: 0.6700\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 437us/step - loss: 0.0558 - val_loss: 0.5582\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.477 - ETA: 0s - loss: 0.706 - 1s 7ms/step - loss: 0.7002 - val_loss: 0.7106\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.412 - 0s 456us/step - loss: 0.3360 - val_loss: 1.3024\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.726 - 0s 418us/step - loss: 0.2955 - val_loss: 1.3137\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - 0s 447us/step - loss: 0.2495 - val_loss: 1.2599\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - 0s 446us/step - loss: 0.2332 - val_loss: 1.3845\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - 0s 503us/step - loss: 0.2160 - val_loss: 1.5603\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - 0s 532us/step - loss: 0.2004 - val_loss: 1.6037\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.378 - 0s 437us/step - loss: 0.1868 - val_loss: 1.5649\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 427us/step - loss: 0.1893 - val_loss: 1.4397\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.384 - 0s 427us/step - loss: 0.1857 - val_loss: 1.5391\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 408us/step - loss: 0.1568 - val_loss: 2.1003\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - 0s 437us/step - loss: 0.1565 - val_loss: 1.9716\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - 0s 418us/step - loss: 0.1476 - val_loss: 1.8942\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - 0s 465us/step - loss: 0.1498 - val_loss: 2.0493\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 408us/step - loss: 0.1459 - val_loss: 2.0454\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 408us/step - loss: 0.1278 - val_loss: 1.9354\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 437us/step - loss: 0.1123 - val_loss: 1.9170\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 503us/step - loss: 0.1174 - val_loss: 2.2501\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 418us/step - loss: 0.1134 - val_loss: 2.2176\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 427us/step - loss: 0.1224 - val_loss: 2.2262\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 427us/step - loss: 0.0904 - val_loss: 2.2108\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - 0s 437us/step - loss: 0.0941 - val_loss: 2.1174\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 408us/step - loss: 0.0958 - val_loss: 2.3469\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 456us/step - loss: 0.0882 - val_loss: 2.5219\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 446us/step - loss: 0.0821 - val_loss: 2.1525\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - 0s 427us/step - loss: 0.0811 - val_loss: 2.0813\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - 0s 484us/step - loss: 0.0808 - val_loss: 2.3520\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - 0s 437us/step - loss: 0.0732 - val_loss: 2.4463\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 418us/step - loss: 0.0732 - val_loss: 2.3694\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 437us/step - loss: 0.0782 - val_loss: 2.5222\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 399us/step - loss: 0.0739 - val_loss: 2.2330\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 494us/step - loss: 0.0767 - val_loss: 2.5019\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 408us/step - loss: 0.0728 - val_loss: 2.2377\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 465us/step - loss: 0.0781 - val_loss: 2.4703\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 437us/step - loss: 0.0618 - val_loss: 2.1793\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 456us/step - loss: 0.0664 - val_loss: 2.3840\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 418us/step - loss: 0.0607 - val_loss: 2.4311\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - 0s 494us/step - loss: 0.0563 - val_loss: 2.3306\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 370us/step - loss: 0.0601 - val_loss: 2.6809\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 437us/step - loss: 0.0568 - val_loss: 2.4359\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 408us/step - loss: 0.0598 - val_loss: 2.6511\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 427us/step - loss: 0.0525 - val_loss: 2.3704\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 399us/step - loss: 0.0547 - val_loss: 2.6095\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - 0s 475us/step - loss: 0.0471 - val_loss: 2.8050\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 437us/step - loss: 0.0485 - val_loss: 2.3934\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 494us/step - loss: 0.0471 - val_loss: 2.6246\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.045 - 0s 1ms/step - loss: 0.0461 - val_loss: 2.8422\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - 0s 627us/step - loss: 0.0417 - val_loss: 2.7226\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 504us/step - loss: 0.0499 - val_loss: 3.1876\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.036 - 0s 921us/step - loss: 0.0464 - val_loss: 2.8153\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.057 - 0s 655us/step - loss: 0.0585 - val_loss: 2.9823\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.467 - 1s 7ms/step - loss: 0.7278 - val_loss: 0.5328\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.944 - 0s 456us/step - loss: 0.5753 - val_loss: 0.5669\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.408 - 0s 475us/step - loss: 0.5247 - val_loss: 0.4864\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - 0s 437us/step - loss: 0.5035 - val_loss: 0.3550\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.135 - 0s 475us/step - loss: 0.4940 - val_loss: 0.5093\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - 0s 465us/step - loss: 0.4315 - val_loss: 0.5663\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.612 - ETA: 0s - loss: 0.348 - 0s 969us/step - loss: 0.4284 - val_loss: 0.6388\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.677 - 0s 503us/step - loss: 0.3927 - val_loss: 0.6997\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - 0s 418us/step - loss: 0.3837 - val_loss: 0.6154\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - 0s 475us/step - loss: 0.3688 - val_loss: 0.5388\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - 0s 446us/step - loss: 0.3535 - val_loss: 0.5974\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - 0s 408us/step - loss: 0.3096 - val_loss: 0.5569\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - 0s 437us/step - loss: 0.3324 - val_loss: 0.7132\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - 0s 408us/step - loss: 0.2996 - val_loss: 0.7412\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - 0s 456us/step - loss: 0.2920 - val_loss: 0.6652\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - 0s 427us/step - loss: 0.2660 - val_loss: 0.6817\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 409us/step - loss: 0.2563 - val_loss: 0.7660\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - 0s 408us/step - loss: 0.2598 - val_loss: 0.7442\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.469 - 0s 408us/step - loss: 0.2572 - val_loss: 0.5555\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - 0s 456us/step - loss: 0.2176 - val_loss: 0.6639\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - 0s 446us/step - loss: 0.2204 - val_loss: 0.6467\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - 0s 418us/step - loss: 0.2230 - val_loss: 0.9225\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 427us/step - loss: 0.2132 - val_loss: 0.5946\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - 0s 408us/step - loss: 0.2096 - val_loss: 0.6960\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - 0s 437us/step - loss: 0.1897 - val_loss: 0.7839\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - 0s 504us/step - loss: 0.1905 - val_loss: 0.7403\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - 0s 389us/step - loss: 0.1839 - val_loss: 0.7471\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 408us/step - loss: 0.1636 - val_loss: 0.7852\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - 0s 446us/step - loss: 0.1646 - val_loss: 0.6668\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - 0s 428us/step - loss: 0.1943 - val_loss: 0.6658\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 437us/step - loss: 0.1604 - val_loss: 0.7705\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - 0s 427us/step - loss: 0.1618 - val_loss: 0.5524\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - 0s 446us/step - loss: 0.1577 - val_loss: 0.6244\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 428us/step - loss: 0.1522 - val_loss: 0.6650\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 427us/step - loss: 0.1336 - val_loss: 0.6583\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - 0s 437us/step - loss: 0.1352 - val_loss: 0.6211\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - 0s 437us/step - loss: 0.1353 - val_loss: 0.5852\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 427us/step - loss: 0.1703 - val_loss: 0.5857\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.472 - 0s 446us/step - loss: 0.1750 - val_loss: 0.6374\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 418us/step - loss: 0.1365 - val_loss: 0.5350\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - 0s 466us/step - loss: 0.1215 - val_loss: 0.7006\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 428us/step - loss: 0.1317 - val_loss: 0.5894\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - 0s 446us/step - loss: 0.1218 - val_loss: 0.5485\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 399us/step - loss: 0.1208 - val_loss: 0.5400\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - 0s 418us/step - loss: 0.1160 - val_loss: 0.6083\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 513us/step - loss: 0.1300 - val_loss: 0.6247\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 428us/step - loss: 0.1082 - val_loss: 0.6159\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.113 - 0s 950us/step - loss: 0.1131 - val_loss: 0.6432\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 446us/step - loss: 0.1139 - val_loss: 0.6211\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 446us/step - loss: 0.1056 - val_loss: 0.6207\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 427us/step - loss: 0.1064 - val_loss: 0.5647\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 494us/step - loss: 0.1038 - val_loss: 0.6175\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 446us/step - loss: 0.1051 - val_loss: 0.5411\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 446us/step - loss: 0.1018 - val_loss: 0.6426\n",
      "Epoch 00054: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 1.889 - 1s 7ms/step - loss: 0.7924 - val_loss: 0.5229\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.345 - 0s 494us/step - loss: 0.6213 - val_loss: 0.8696\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.562 - 0s 418us/step - loss: 0.5599 - val_loss: 0.8201\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.991 - 0s 484us/step - loss: 0.5180 - val_loss: 0.6968\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.481 - 0s 437us/step - loss: 0.4528 - val_loss: 0.8704\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.896 - 0s 427us/step - loss: 0.4211 - val_loss: 1.0610\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.902 - 0s 418us/step - loss: 0.3929 - val_loss: 0.8917\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.797 - 0s 465us/step - loss: 0.3660 - val_loss: 1.1840\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.360 - 0s 437us/step - loss: 0.3065 - val_loss: 1.0864\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 399us/step - loss: 0.2989 - val_loss: 1.1370\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.506 - 0s 446us/step - loss: 0.2954 - val_loss: 1.3012\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - 0s 427us/step - loss: 0.2681 - val_loss: 1.2510\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - 0s 408us/step - loss: 0.2589 - val_loss: 1.3200\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - 0s 418us/step - loss: 0.2399 - val_loss: 1.5859\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - 0s 465us/step - loss: 0.2108 - val_loss: 1.1980\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - 0s 466us/step - loss: 0.2049 - val_loss: 1.4524\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 494us/step - loss: 0.1907 - val_loss: 1.4031\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 475us/step - loss: 0.1974 - val_loss: 1.2994\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 408us/step - loss: 0.1929 - val_loss: 1.3718\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - 0s 427us/step - loss: 0.1783 - val_loss: 1.5411\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - 0s 446us/step - loss: 0.1662 - val_loss: 1.3558\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - 0s 427us/step - loss: 0.1737 - val_loss: 1.5114\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 446us/step - loss: 0.1466 - val_loss: 1.5179\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - 0s 437us/step - loss: 0.1508 - val_loss: 1.7665\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - 0s 456us/step - loss: 0.1452 - val_loss: 1.5227\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 427us/step - loss: 0.1396 - val_loss: 1.9260\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 427us/step - loss: 0.1464 - val_loss: 2.3129\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - 0s 437us/step - loss: 0.1218 - val_loss: 1.5919\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - 0s 427us/step - loss: 0.1373 - val_loss: 2.2584\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 456us/step - loss: 0.1314 - val_loss: 1.6693\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 503us/step - loss: 0.1255 - val_loss: 1.7105\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 418us/step - loss: 0.1148 - val_loss: 1.6149\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 408us/step - loss: 0.1115 - val_loss: 1.5854\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.112 - 0s 931us/step - loss: 0.1116 - val_loss: 1.5908\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 570us/step - loss: 0.1016 - val_loss: 1.4854\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - 0s 541us/step - loss: 0.1113 - val_loss: 1.4475\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 456us/step - loss: 0.1040 - val_loss: 1.5088\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 437us/step - loss: 0.1028 - val_loss: 1.4007\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - 0s 418us/step - loss: 0.0984 - val_loss: 1.5727\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 437us/step - loss: 0.0938 - val_loss: 1.3875\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - 0s 437us/step - loss: 0.0882 - val_loss: 1.5258\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 466us/step - loss: 0.0871 - val_loss: 1.2533\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - 0s 399us/step - loss: 0.1031 - val_loss: 1.6073\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 437us/step - loss: 0.0874 - val_loss: 1.4700\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - 0s 408us/step - loss: 0.0853 - val_loss: 1.4809\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 437us/step - loss: 0.0989 - val_loss: 1.7958\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - 0s 456us/step - loss: 0.0850 - val_loss: 1.7333\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 561us/step - loss: 0.0897 - val_loss: 1.3883\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 1ms/step - loss: 0.0884 - val_loss: 1.7086\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.072 - 0s 798us/step - loss: 0.0788 - val_loss: 1.6749\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 484us/step - loss: 0.0795 - val_loss: 1.6002\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.616 - 1s 7ms/step - loss: 0.7396 - val_loss: 0.4610\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.519 - 0s 494us/step - loss: 0.6228 - val_loss: 0.6873\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.006 - 0s 437us/step - loss: 0.5435 - val_loss: 0.6045\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.655 - 0s 513us/step - loss: 0.4859 - val_loss: 0.7025\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - 0s 427us/step - loss: 0.4667 - val_loss: 0.9446\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.461 - 0s 855us/step - loss: 0.4284 - val_loss: 0.8078\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.419 - 0s 484us/step - loss: 0.3868 - val_loss: 0.8844\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 428us/step - loss: 0.3735 - val_loss: 1.0472\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.758 - 0s 437us/step - loss: 0.3456 - val_loss: 0.9011\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - 0s 418us/step - loss: 0.3256 - val_loss: 0.9286\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - 0s 437us/step - loss: 0.3013 - val_loss: 1.0684\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - 0s 437us/step - loss: 0.3173 - val_loss: 1.1173\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.513 - 0s 465us/step - loss: 0.3184 - val_loss: 1.0592\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - 0s 484us/step - loss: 0.2906 - val_loss: 1.2245\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 437us/step - loss: 0.2541 - val_loss: 0.8775\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - 0s 428us/step - loss: 0.2545 - val_loss: 1.1665\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - 0s 456us/step - loss: 0.2539 - val_loss: 1.3102\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 428us/step - loss: 0.2226 - val_loss: 1.2154\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - 0s 551us/step - loss: 0.2430 - val_loss: 1.1103\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - 0s 446us/step - loss: 0.2218 - val_loss: 1.2891\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - 0s 437us/step - loss: 0.2212 - val_loss: 1.3632\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - 0s 513us/step - loss: 0.2226 - val_loss: 1.4785\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.134 - 0s 1ms/step - loss: 0.1848 - val_loss: 1.2184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 465us/step - loss: 0.1786 - val_loss: 1.2677\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - 0s 446us/step - loss: 0.1524 - val_loss: 1.3817\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - 0s 418us/step - loss: 0.1453 - val_loss: 1.2510\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - 0s 427us/step - loss: 0.1630 - val_loss: 1.3446\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 399us/step - loss: 0.1546 - val_loss: 1.3883\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 418us/step - loss: 0.1350 - val_loss: 1.4947\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 418us/step - loss: 0.1453 - val_loss: 1.4710\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - 0s 437us/step - loss: 0.1412 - val_loss: 1.6771\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - 0s 428us/step - loss: 0.1350 - val_loss: 1.4082\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 427us/step - loss: 0.1202 - val_loss: 1.4558\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - 0s 437us/step - loss: 0.1629 - val_loss: 1.1516\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - 0s 446us/step - loss: 0.1280 - val_loss: 1.4069\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 456us/step - loss: 0.1245 - val_loss: 1.1841\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 437us/step - loss: 0.1068 - val_loss: 1.4667\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - 0s 418us/step - loss: 0.1144 - val_loss: 1.3293\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - 0s 418us/step - loss: 0.1024 - val_loss: 1.5564\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - 0s 447us/step - loss: 0.1022 - val_loss: 1.2747\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - 0s 408us/step - loss: 0.1073 - val_loss: 1.2644\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 427us/step - loss: 0.0887 - val_loss: 1.4821\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 503us/step - loss: 0.0829 - val_loss: 1.5624\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 408us/step - loss: 0.0841 - val_loss: 1.4170\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - 0s 408us/step - loss: 0.0711 - val_loss: 1.3305\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 380us/step - loss: 0.0818 - val_loss: 1.2793\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 427us/step - loss: 0.0730 - val_loss: 1.3384\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 399us/step - loss: 0.0944 - val_loss: 1.4627\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 475us/step - loss: 0.0810 - val_loss: 1.2865\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 427us/step - loss: 0.0632 - val_loss: 1.6421\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - 0s 447us/step - loss: 0.0684 - val_loss: 1.3492\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 7\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.975 - 1s 7ms/step - loss: 0.8074 - val_loss: 0.4751\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - 0s 475us/step - loss: 0.5354 - val_loss: 0.9063\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.668 - 0s 427us/step - loss: 0.4811 - val_loss: 1.0304\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.445 - 0s 456us/step - loss: 0.4738 - val_loss: 1.0415\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - 0s 475us/step - loss: 0.4431 - val_loss: 1.0616\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - 0s 437us/step - loss: 0.4118 - val_loss: 1.1912\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.691 - 0s 409us/step - loss: 0.3754 - val_loss: 1.0430\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.338 - 0s 447us/step - loss: 0.3511 - val_loss: 1.1091\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - 0s 428us/step - loss: 0.3394 - val_loss: 1.3014\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.435 - 0s 446us/step - loss: 0.3226 - val_loss: 1.3385\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 446us/step - loss: 0.2992 - val_loss: 1.0766\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - 0s 504us/step - loss: 0.2885 - val_loss: 1.1849\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - 0s 494us/step - loss: 0.2672 - val_loss: 1.4027\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - 0s 418us/step - loss: 0.2716 - val_loss: 1.4670\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - 0s 513us/step - loss: 0.2684 - val_loss: 1.4185\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.236 - 0s 1ms/step - loss: 0.2492 - val_loss: 1.2824\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.254 - 0s 674us/step - loss: 0.2522 - val_loss: 1.5731\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.332 - 0s 532us/step - loss: 0.2244 - val_loss: 1.5343\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.432 - 0s 446us/step - loss: 0.2205 - val_loss: 1.6928\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.382 - 0s 427us/step - loss: 0.2070 - val_loss: 1.5235\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.495 - 0s 427us/step - loss: 0.2049 - val_loss: 1.5833\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - 0s 456us/step - loss: 0.2099 - val_loss: 1.6816\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 446us/step - loss: 0.1953 - val_loss: 1.8416\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - 0s 418us/step - loss: 0.1780 - val_loss: 1.7482\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - 0s 437us/step - loss: 0.1775 - val_loss: 1.8067\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - 0s 418us/step - loss: 0.1797 - val_loss: 1.7888\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 437us/step - loss: 0.1742 - val_loss: 1.6067\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - 0s 437us/step - loss: 0.1717 - val_loss: 1.8486\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 437us/step - loss: 0.1467 - val_loss: 1.6333\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.354 - 0s 532us/step - loss: 0.1469 - val_loss: 1.7830\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - 0s 408us/step - loss: 0.1427 - val_loss: 1.8664\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - 0s 447us/step - loss: 0.1531 - val_loss: 1.7836\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - 0s 437us/step - loss: 0.1480 - val_loss: 1.8991\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - 0s 437us/step - loss: 0.1425 - val_loss: 1.6631\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 446us/step - loss: 0.1271 - val_loss: 1.8957\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - 0s 408us/step - loss: 0.1322 - val_loss: 2.0633\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 484us/step - loss: 0.1332 - val_loss: 1.8958\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - 0s 427us/step - loss: 0.1174 - val_loss: 2.0170\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 465us/step - loss: 0.1121 - val_loss: 1.8867\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - 0s 437us/step - loss: 0.1038 - val_loss: 2.0156\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 465us/step - loss: 0.1077 - val_loss: 1.8475\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 427us/step - loss: 0.1169 - val_loss: 2.4707\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 418us/step - loss: 0.1088 - val_loss: 1.9428\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 447us/step - loss: 0.1171 - val_loss: 2.0397\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - 0s 428us/step - loss: 0.1005 - val_loss: 2.2671\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 428us/step - loss: 0.0869 - val_loss: 2.3530\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - 0s 428us/step - loss: 0.0944 - val_loss: 2.2291\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 437us/step - loss: 0.1023 - val_loss: 2.4673\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 437us/step - loss: 0.0899 - val_loss: 2.6754\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 456us/step - loss: 0.0938 - val_loss: 2.1893\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - 0s 437us/step - loss: 0.0865 - val_loss: 2.4320\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 8\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.323 - 1s 7ms/step - loss: 0.6776 - val_loss: 0.5925\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - 0s 494us/step - loss: 0.4628 - val_loss: 1.1679\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - 0s 427us/step - loss: 0.4037 - val_loss: 0.9416\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - 0s 408us/step - loss: 0.3702 - val_loss: 1.1587\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - 0s 437us/step - loss: 0.3425 - val_loss: 0.8248\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - 0s 361us/step - loss: 0.2895 - val_loss: 0.6117\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.395 - 0s 456us/step - loss: 0.2588 - val_loss: 0.8182\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - 0s 437us/step - loss: 0.2565 - val_loss: 0.7257\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.321 - 0s 522us/step - loss: 0.2428 - val_loss: 0.5618\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 399us/step - loss: 0.2300 - val_loss: 0.6309\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - 0s 408us/step - loss: 0.2008 - val_loss: 0.5861\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 427us/step - loss: 0.1910 - val_loss: 0.5038\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 446us/step - loss: 0.1628 - val_loss: 0.6759\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 437us/step - loss: 0.1608 - val_loss: 0.7237\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - 0s 418us/step - loss: 0.1802 - val_loss: 0.7730\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 427us/step - loss: 0.1438 - val_loss: 0.4872\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - 0s 427us/step - loss: 0.1220 - val_loss: 0.5713\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - 0s 428us/step - loss: 0.1278 - val_loss: 0.6266\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 456us/step - loss: 0.1100 - val_loss: 0.7378\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 437us/step - loss: 0.1137 - val_loss: 0.6191\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - 0s 427us/step - loss: 0.1101 - val_loss: 0.6428\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.285 - 0s 475us/step - loss: 0.1120 - val_loss: 0.6510\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - 0s 466us/step - loss: 0.1012 - val_loss: 0.6333\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - 0s 437us/step - loss: 0.1039 - val_loss: 0.8680\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - 0s 475us/step - loss: 0.0834 - val_loss: 0.5537\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 466us/step - loss: 0.0807 - val_loss: 0.6726\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 437us/step - loss: 0.0836 - val_loss: 0.7177\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - 0s 427us/step - loss: 0.0752 - val_loss: 0.6120\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 484us/step - loss: 0.0804 - val_loss: 0.7471\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 484us/step - loss: 0.0692 - val_loss: 0.6449\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 418us/step - loss: 0.0716 - val_loss: 0.6496\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - 0s 475us/step - loss: 0.0793 - val_loss: 0.7064\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 465us/step - loss: 0.0657 - val_loss: 0.6998\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 503us/step - loss: 0.0631 - val_loss: 0.5494\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - 0s 447us/step - loss: 0.0618 - val_loss: 0.6923\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 437us/step - loss: 0.0598 - val_loss: 0.7027\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 408us/step - loss: 0.0492 - val_loss: 0.6800\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - 0s 437us/step - loss: 0.0590 - val_loss: 0.6843\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 437us/step - loss: 0.0531 - val_loss: 0.5985\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 427us/step - loss: 0.0574 - val_loss: 0.6574\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 466us/step - loss: 0.0525 - val_loss: 0.7620\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - 0s 456us/step - loss: 0.0528 - val_loss: 0.6262\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 427us/step - loss: 0.0534 - val_loss: 0.5568\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 409us/step - loss: 0.0508 - val_loss: 0.7085\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 494us/step - loss: 0.0484 - val_loss: 0.6000\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 447us/step - loss: 0.0527 - val_loss: 0.6289\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 427us/step - loss: 0.0476 - val_loss: 0.5890\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 494us/step - loss: 0.0476 - val_loss: 0.5827\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 408us/step - loss: 0.0394 - val_loss: 0.6055\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - 0s 437us/step - loss: 0.0399 - val_loss: 0.6245\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 408us/step - loss: 0.0439 - val_loss: 0.5810\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - 0s 409us/step - loss: 0.0571 - val_loss: 0.6527\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - 0s 494us/step - loss: 0.0628 - val_loss: 0.8649\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 428us/step - loss: 0.0517 - val_loss: 0.5975\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 427us/step - loss: 0.0406 - val_loss: 0.6048\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 418us/step - loss: 0.0413 - val_loss: 0.5918\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - 0s 409us/step - loss: 0.0400 - val_loss: 0.5909\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 408us/step - loss: 0.0394 - val_loss: 0.5255\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.043 - 0s 579us/step - loss: 0.0434 - val_loss: 0.6400\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 437us/step - loss: 0.0384 - val_loss: 0.5847\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - 0s 418us/step - loss: 0.0350 - val_loss: 0.5775\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - 0s 418us/step - loss: 0.0395 - val_loss: 0.6152\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 437us/step - loss: 0.0338 - val_loss: 0.5608\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 427us/step - loss: 0.0369 - val_loss: 0.5638\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.033 - 0s 893us/step - loss: 0.0363 - val_loss: 0.5542\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 456us/step - loss: 0.0371 - val_loss: 0.5513\n",
      "Epoch 00066: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 9\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.915 - 1s 7ms/step - loss: 0.7391 - val_loss: 0.6905\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.370 - 0s 456us/step - loss: 0.5374 - val_loss: 1.0113\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - 0s 437us/step - loss: 0.4439 - val_loss: 0.6748\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - 0s 465us/step - loss: 0.3720 - val_loss: 0.6333\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 532us/step - loss: 0.3567 - val_loss: 0.7142\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.263 - 0s 446us/step - loss: 0.2859 - val_loss: 0.4381\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.614 - 0s 437us/step - loss: 0.2709 - val_loss: 0.5611\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.448 - 0s 485us/step - loss: 0.2266 - val_loss: 0.5143\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - 0s 446us/step - loss: 0.2127 - val_loss: 0.4268\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - 0s 446us/step - loss: 0.1749 - val_loss: 0.3783\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - 0s 466us/step - loss: 0.1612 - val_loss: 0.3372\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 465us/step - loss: 0.1519 - val_loss: 0.3401\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.596 - 0s 427us/step - loss: 0.1327 - val_loss: 0.4749\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - 0s 418us/step - loss: 0.1328 - val_loss: 0.3974\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - 0s 456us/step - loss: 0.1170 - val_loss: 0.3707\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - 0s 418us/step - loss: 0.1147 - val_loss: 0.4799\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - 0s 427us/step - loss: 0.0982 - val_loss: 0.4615\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 456us/step - loss: 0.1051 - val_loss: 0.4423\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 418us/step - loss: 0.1166 - val_loss: 0.5720\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - 0s 418us/step - loss: 0.0903 - val_loss: 0.5145\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - 0s 484us/step - loss: 0.0902 - val_loss: 0.5602\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 389us/step - loss: 0.0929 - val_loss: 0.5210\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 475us/step - loss: 0.0749 - val_loss: 0.5965\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 503us/step - loss: 0.0768 - val_loss: 0.5793\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 484us/step - loss: 0.0774 - val_loss: 0.6033\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - 0s 428us/step - loss: 0.0746 - val_loss: 0.7600\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 456us/step - loss: 0.0697 - val_loss: 0.6064\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 399us/step - loss: 0.0680 - val_loss: 0.8231\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - 0s 427us/step - loss: 0.0650 - val_loss: 0.7490\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - 0s 427us/step - loss: 0.0597 - val_loss: 0.7371\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 399us/step - loss: 0.0563 - val_loss: 0.7506\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 427us/step - loss: 0.0611 - val_loss: 0.7487\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 446us/step - loss: 0.0468 - val_loss: 0.8693\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 456us/step - loss: 0.0565 - val_loss: 0.9891\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - 0s 456us/step - loss: 0.0467 - val_loss: 0.7159\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 428us/step - loss: 0.0572 - val_loss: 1.1117\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - 0s 437us/step - loss: 0.0576 - val_loss: 0.9360\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 427us/step - loss: 0.0397 - val_loss: 0.9765\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - 0s 465us/step - loss: 0.0502 - val_loss: 1.1512\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - 0s 418us/step - loss: 0.0397 - val_loss: 0.9370\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - 0s 427us/step - loss: 0.0534 - val_loss: 1.2229\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - 0s 437us/step - loss: 0.0551 - val_loss: 0.8739\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.028 - 0s 874us/step - loss: 0.0423 - val_loss: 1.1174\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - 0s 399us/step - loss: 0.0391 - val_loss: 0.8883\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 399us/step - loss: 0.0498 - val_loss: 1.0854\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - 0s 408us/step - loss: 0.0397 - val_loss: 0.9214\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - 0s 427us/step - loss: 0.0410 - val_loss: 1.1162\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - 0s 409us/step - loss: 0.0444 - val_loss: 1.1651\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 408us/step - loss: 0.0384 - val_loss: 0.9702\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - 0s 446us/step - loss: 0.0409 - val_loss: 1.0461\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 427us/step - loss: 0.0353 - val_loss: 1.0279\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - 0s 437us/step - loss: 0.0306 - val_loss: 1.0751\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - 0s 465us/step - loss: 0.0276 - val_loss: 0.9638\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 408us/step - loss: 0.0391 - val_loss: 1.1074\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - 0s 437us/step - loss: 0.0311 - val_loss: 1.2175\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 408us/step - loss: 0.0272 - val_loss: 1.0319\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - 0s 475us/step - loss: 0.0278 - val_loss: 1.1177\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - 0s 446us/step - loss: 0.0327 - val_loss: 1.0569\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 456us/step - loss: 0.0365 - val_loss: 1.2700\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - 0s 446us/step - loss: 0.0305 - val_loss: 1.1245\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 437us/step - loss: 0.0322 - val_loss: 1.2173\n",
      "Epoch 00061: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.043 - 1s 7ms/step - loss: 0.6927 - val_loss: 0.4630\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - 0s 446us/step - loss: 0.5649 - val_loss: 0.8537\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.820 - 0s 418us/step - loss: 0.5181 - val_loss: 0.8766\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 428us/step - loss: 0.4423 - val_loss: 0.6329\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.135 - 0s 418us/step - loss: 0.4109 - val_loss: 1.0546\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - 0s 456us/step - loss: 0.3460 - val_loss: 0.9125\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 418us/step - loss: 0.3291 - val_loss: 0.7635\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 456us/step - loss: 0.2964 - val_loss: 0.6192\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.511 - 0s 522us/step - loss: 0.2527 - val_loss: 0.7491\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - 0s 475us/step - loss: 0.2381 - val_loss: 0.5374\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - 0s 465us/step - loss: 0.2271 - val_loss: 0.5323\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.637 - 0s 465us/step - loss: 0.2061 - val_loss: 0.5976\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - 0s 513us/step - loss: 0.1758 - val_loss: 0.5720\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - 0s 427us/step - loss: 0.1639 - val_loss: 0.6370\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - 0s 437us/step - loss: 0.1539 - val_loss: 0.5915\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - 0s 437us/step - loss: 0.1293 - val_loss: 0.4843\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 484us/step - loss: 0.1283 - val_loss: 0.7889\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 456us/step - loss: 0.1080 - val_loss: 0.6836\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 446us/step - loss: 0.1089 - val_loss: 0.7996\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 475us/step - loss: 0.0921 - val_loss: 0.7148\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 494us/step - loss: 0.0834 - val_loss: 0.6569\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - 0s 437us/step - loss: 0.0694 - val_loss: 0.7811\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 504us/step - loss: 0.0728 - val_loss: 0.6689\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 418us/step - loss: 0.0584 - val_loss: 0.7967\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 389us/step - loss: 0.0654 - val_loss: 0.8104\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 437us/step - loss: 0.0654 - val_loss: 0.8231\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 437us/step - loss: 0.0549 - val_loss: 0.9301\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 485us/step - loss: 0.0892 - val_loss: 0.6994\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 399us/step - loss: 0.0587 - val_loss: 1.0081\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 446us/step - loss: 0.0473 - val_loss: 0.8172\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - 0s 418us/step - loss: 0.0628 - val_loss: 0.8088\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 446us/step - loss: 0.0413 - val_loss: 0.7932\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 446us/step - loss: 0.0471 - val_loss: 0.7787\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 408us/step - loss: 0.0411 - val_loss: 0.6795\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 427us/step - loss: 0.0451 - val_loss: 0.9247\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 418us/step - loss: 0.0449 - val_loss: 0.7419\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 427us/step - loss: 0.0336 - val_loss: 0.7876\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 446us/step - loss: 0.0344 - val_loss: 0.8919\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - 0s 428us/step - loss: 0.0242 - val_loss: 0.7996\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 475us/step - loss: 0.0418 - val_loss: 0.8289\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - 0s 428us/step - loss: 0.0492 - val_loss: 0.7379\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 418us/step - loss: 0.0315 - val_loss: 0.8684\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 418us/step - loss: 0.0337 - val_loss: 0.6919\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 427us/step - loss: 0.0458 - val_loss: 1.0757\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 427us/step - loss: 0.0521 - val_loss: 0.6266\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 465us/step - loss: 0.0345 - val_loss: 0.7622\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 447us/step - loss: 0.0296 - val_loss: 0.7947\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 494us/step - loss: 0.0311 - val_loss: 0.6789\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - 0s 427us/step - loss: 0.0243 - val_loss: 0.8092\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - 0s 437us/step - loss: 0.0349 - val_loss: 0.7355\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 418us/step - loss: 0.0379 - val_loss: 0.6873\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 11\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 6s - loss: 0.200 - 1s 7ms/step - loss: 0.7686 - val_loss: 0.3888\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - 0s 456us/step - loss: 0.4417 - val_loss: 0.8646\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - 0s 427us/step - loss: 0.4004 - val_loss: 0.9613\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - 0s 399us/step - loss: 0.3213 - val_loss: 0.7442\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - 0s 475us/step - loss: 0.3078 - val_loss: 0.5622\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.798 - 0s 427us/step - loss: 0.2783 - val_loss: 0.7220\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - 0s 437us/step - loss: 0.2490 - val_loss: 1.0275\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 446us/step - loss: 0.2191 - val_loss: 0.6408\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - 0s 513us/step - loss: 0.2015 - val_loss: 0.6885\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - 0s 456us/step - loss: 0.1848 - val_loss: 0.8253\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - 0s 456us/step - loss: 0.1842 - val_loss: 0.7273\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - 0s 456us/step - loss: 0.1603 - val_loss: 0.5856\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - 0s 475us/step - loss: 0.1560 - val_loss: 0.6408\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 427us/step - loss: 0.1669 - val_loss: 0.6037\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 494us/step - loss: 0.1568 - val_loss: 0.6983\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - 0s 409us/step - loss: 0.1395 - val_loss: 0.8102\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 437us/step - loss: 0.1519 - val_loss: 0.5569\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - 0s 456us/step - loss: 0.1299 - val_loss: 0.6072\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 513us/step - loss: 0.1362 - val_loss: 0.5215\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 503us/step - loss: 0.1104 - val_loss: 0.7590\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 503us/step - loss: 0.1085 - val_loss: 0.7533\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - 0s 446us/step - loss: 0.1032 - val_loss: 0.7048\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - 0s 465us/step - loss: 0.1034 - val_loss: 0.5662\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - 0s 513us/step - loss: 0.0932 - val_loss: 0.6347\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - 0s 494us/step - loss: 0.0940 - val_loss: 0.5840\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 503us/step - loss: 0.0858 - val_loss: 0.7233\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 475us/step - loss: 0.0792 - val_loss: 0.6055\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 437us/step - loss: 0.0818 - val_loss: 0.5612\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 418us/step - loss: 0.0798 - val_loss: 0.7748\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 408us/step - loss: 0.0736 - val_loss: 0.7292\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 437us/step - loss: 0.0827 - val_loss: 0.6290\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 389us/step - loss: 0.0657 - val_loss: 0.9503\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 408us/step - loss: 0.0700 - val_loss: 0.8159\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 437us/step - loss: 0.0692 - val_loss: 0.8029\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 409us/step - loss: 0.0560 - val_loss: 0.8004\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 446us/step - loss: 0.0566 - val_loss: 0.8094\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 427us/step - loss: 0.0593 - val_loss: 0.7993\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 465us/step - loss: 0.0485 - val_loss: 0.7301\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 456us/step - loss: 0.0551 - val_loss: 0.8623\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 456us/step - loss: 0.0519 - val_loss: 0.6609\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - 0s 427us/step - loss: 0.0420 - val_loss: 0.7917\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 408us/step - loss: 0.0497 - val_loss: 0.7513\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 418us/step - loss: 0.0492 - val_loss: 0.9136\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 418us/step - loss: 0.0510 - val_loss: 0.8984\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 418us/step - loss: 0.0413 - val_loss: 0.7817\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 465us/step - loss: 0.0487 - val_loss: 0.7546\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - 0s 427us/step - loss: 0.0579 - val_loss: 0.8633\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 418us/step - loss: 0.0496 - val_loss: 0.7492\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 437us/step - loss: 0.0445 - val_loss: 0.7006\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - 0s 390us/step - loss: 0.0475 - val_loss: 1.0785\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - 0s 446us/step - loss: 0.0412 - val_loss: 0.9867\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 12\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 8s - loss: 1.369 - 1s 8ms/step - loss: 0.6927 - val_loss: 0.8305\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.627 - 0s 456us/step - loss: 0.3637 - val_loss: 1.5054\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - 0s 475us/step - loss: 0.3358 - val_loss: 1.4214\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.499 - 0s 437us/step - loss: 0.3090 - val_loss: 1.4257\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - 0s 408us/step - loss: 0.2687 - val_loss: 2.1501\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 418us/step - loss: 0.2119 - val_loss: 1.9034\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - 0s 409us/step - loss: 0.1861 - val_loss: 1.9233\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 570us/step - loss: 0.1831 - val_loss: 1.8762\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - 0s 446us/step - loss: 0.1870 - val_loss: 1.9814\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - 0s 456us/step - loss: 0.1714 - val_loss: 1.5128\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - 0s 504us/step - loss: 0.1646 - val_loss: 2.1211\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.267 - 0s 465us/step - loss: 0.1493 - val_loss: 2.6555\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - 0s 437us/step - loss: 0.1369 - val_loss: 2.2634\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - 0s 418us/step - loss: 0.1236 - val_loss: 2.2684\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - 0s 418us/step - loss: 0.1204 - val_loss: 2.5087\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - 0s 437us/step - loss: 0.1131 - val_loss: 2.4494\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - 0s 408us/step - loss: 0.1044 - val_loss: 2.5547\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 484us/step - loss: 0.1003 - val_loss: 2.4922\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - 0s 589us/step - loss: 0.0938 - val_loss: 2.5351\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 456us/step - loss: 0.0961 - val_loss: 2.3690\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 428us/step - loss: 0.1031 - val_loss: 2.5795\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - 0s 437us/step - loss: 0.0928 - val_loss: 2.4546\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 427us/step - loss: 0.0859 - val_loss: 2.5979\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 437us/step - loss: 0.0755 - val_loss: 2.7086\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - 0s 447us/step - loss: 0.0791 - val_loss: 2.6076\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 437us/step - loss: 0.0818 - val_loss: 2.5592\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 560us/step - loss: 0.0758 - val_loss: 2.4423\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 513us/step - loss: 0.0745 - val_loss: 2.5668\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - 0s 446us/step - loss: 0.0573 - val_loss: 2.6374\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 494us/step - loss: 0.0668 - val_loss: 2.8009\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 513us/step - loss: 0.0613 - val_loss: 2.3827\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 437us/step - loss: 0.0641 - val_loss: 2.5561\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 446us/step - loss: 0.0654 - val_loss: 2.8511\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 532us/step - loss: 0.0600 - val_loss: 2.7176\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 456us/step - loss: 0.0577 - val_loss: 2.7559\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 465us/step - loss: 0.0610 - val_loss: 2.4539\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 465us/step - loss: 0.0551 - val_loss: 2.6735\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 532us/step - loss: 0.0583 - val_loss: 2.5564\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 399us/step - loss: 0.0567 - val_loss: 2.7099\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 418us/step - loss: 0.0518 - val_loss: 2.6788\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 427us/step - loss: 0.0494 - val_loss: 2.7976\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 427us/step - loss: 0.0456 - val_loss: 2.8119\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - 0s 427us/step - loss: 0.0426 - val_loss: 2.4940\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - 0s 427us/step - loss: 0.0596 - val_loss: 2.6094\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 418us/step - loss: 0.0460 - val_loss: 2.6053\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 532us/step - loss: 0.0407 - val_loss: 2.7670\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 446us/step - loss: 0.0457 - val_loss: 2.8622\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 437us/step - loss: 0.0401 - val_loss: 2.8588\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 427us/step - loss: 0.0494 - val_loss: 2.4430\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 447us/step - loss: 0.0493 - val_loss: 2.7586\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 418us/step - loss: 0.0491 - val_loss: 2.5000\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 13\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 6s - loss: 1.115 - 1s 7ms/step - loss: 0.6821 - val_loss: 0.7696\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.798 - 0s 465us/step - loss: 0.4915 - val_loss: 1.0828\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.940 - 0s 447us/step - loss: 0.4541 - val_loss: 1.0056\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - 0s 503us/step - loss: 0.4147 - val_loss: 1.1263\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - 0s 466us/step - loss: 0.3811 - val_loss: 1.1326\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.490 - 0s 437us/step - loss: 0.3514 - val_loss: 1.3977\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.532 - 0s 427us/step - loss: 0.3277 - val_loss: 1.3119\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - 0s 503us/step - loss: 0.3283 - val_loss: 0.9532\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - 0s 465us/step - loss: 0.2660 - val_loss: 1.5356\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - 0s 427us/step - loss: 0.2655 - val_loss: 1.4736\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - 0s 465us/step - loss: 0.2597 - val_loss: 1.3563\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - 0s 447us/step - loss: 0.2308 - val_loss: 1.5667\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - 0s 427us/step - loss: 0.2160 - val_loss: 1.5003\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 418us/step - loss: 0.2052 - val_loss: 1.2934\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 427us/step - loss: 0.2347 - val_loss: 1.3849\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 418us/step - loss: 0.1849 - val_loss: 1.6143\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 456us/step - loss: 0.1609 - val_loss: 1.7575\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - 0s 437us/step - loss: 0.1656 - val_loss: 1.7765\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - 0s 437us/step - loss: 0.1624 - val_loss: 1.8698\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 437us/step - loss: 0.1449 - val_loss: 1.7820\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 475us/step - loss: 0.1235 - val_loss: 1.9207\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.361 - 0s 437us/step - loss: 0.1376 - val_loss: 1.7541\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - 0s 503us/step - loss: 0.1290 - val_loss: 1.6632\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 475us/step - loss: 0.1319 - val_loss: 1.8521\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - 0s 428us/step - loss: 0.1233 - val_loss: 1.5396\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - 0s 475us/step - loss: 0.1220 - val_loss: 1.8033\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - 0s 437us/step - loss: 0.1125 - val_loss: 1.7787\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 418us/step - loss: 0.1001 - val_loss: 1.6980\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 494us/step - loss: 0.1026 - val_loss: 1.6098\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 399us/step - loss: 0.1000 - val_loss: 1.4716\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 418us/step - loss: 0.1022 - val_loss: 1.7712\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - 0s 446us/step - loss: 0.1000 - val_loss: 1.7690\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 437us/step - loss: 0.0879 - val_loss: 1.5296\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 428us/step - loss: 0.0879 - val_loss: 1.6163\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 418us/step - loss: 0.0702 - val_loss: 1.7503\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - 0s 446us/step - loss: 0.0790 - val_loss: 1.6325\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 475us/step - loss: 0.0752 - val_loss: 1.4859\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 428us/step - loss: 0.0772 - val_loss: 1.7381\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 418us/step - loss: 0.1106 - val_loss: 1.7670\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - 0s 446us/step - loss: 0.0953 - val_loss: 1.8526\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 513us/step - loss: 0.0848 - val_loss: 1.5115\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - 0s 437us/step - loss: 0.0821 - val_loss: 1.4402\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - 0s 380us/step - loss: 0.0845 - val_loss: 1.7675\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - 0s 418us/step - loss: 0.0744 - val_loss: 1.6150\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 475us/step - loss: 0.0745 - val_loss: 1.5855\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - 0s 418us/step - loss: 0.0592 - val_loss: 1.7775\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 447us/step - loss: 0.0632 - val_loss: 1.6560\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 428us/step - loss: 0.0614 - val_loss: 1.5666\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 447us/step - loss: 0.0636 - val_loss: 1.5854\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - 0s 437us/step - loss: 0.0586 - val_loss: 1.6288\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - 0s 446us/step - loss: 0.0575 - val_loss: 1.7255\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 14\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 1.418 - 1s 7ms/step - loss: 0.8678 - val_loss: 0.2679\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - 0s 485us/step - loss: 0.6091 - val_loss: 0.5810\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.695 - 0s 446us/step - loss: 0.5476 - val_loss: 0.5733\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - 0s 465us/step - loss: 0.4907 - val_loss: 0.5696\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - 0s 437us/step - loss: 0.4261 - val_loss: 0.7474\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - 0s 475us/step - loss: 0.4009 - val_loss: 0.8898\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - 0s 409us/step - loss: 0.3898 - val_loss: 0.9505\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - 0s 428us/step - loss: 0.3252 - val_loss: 1.0507\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 437us/step - loss: 0.2777 - val_loss: 1.3579\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 437us/step - loss: 0.2995 - val_loss: 1.6162\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - 0s 399us/step - loss: 0.2617 - val_loss: 1.1695\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - 0s 513us/step - loss: 0.2207 - val_loss: 1.6803\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.311 - 0s 446us/step - loss: 0.2150 - val_loss: 1.9067\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 456us/step - loss: 0.2088 - val_loss: 1.9155\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - 0s 484us/step - loss: 0.1795 - val_loss: 1.9695\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - 0s 447us/step - loss: 0.1745 - val_loss: 2.0028\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - 0s 437us/step - loss: 0.1910 - val_loss: 2.0640\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 446us/step - loss: 0.1712 - val_loss: 2.1319\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 437us/step - loss: 0.1510 - val_loss: 2.4451\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - 0s 446us/step - loss: 0.1507 - val_loss: 2.0291\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - 0s 475us/step - loss: 0.1572 - val_loss: 2.6922\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - 0s 456us/step - loss: 0.1458 - val_loss: 2.5205\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - 0s 484us/step - loss: 0.1497 - val_loss: 2.8898\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - 0s 399us/step - loss: 0.1317 - val_loss: 2.8832\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 437us/step - loss: 0.1311 - val_loss: 2.5826\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 418us/step - loss: 0.1240 - val_loss: 2.7386\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - 0s 418us/step - loss: 0.1196 - val_loss: 2.7761\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - 0s 446us/step - loss: 0.1191 - val_loss: 2.9925\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 418us/step - loss: 0.1221 - val_loss: 2.8710\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 446us/step - loss: 0.1190 - val_loss: 2.8463\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 522us/step - loss: 0.1198 - val_loss: 2.9170\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 503us/step - loss: 0.0983 - val_loss: 3.1310\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 466us/step - loss: 0.0895 - val_loss: 3.2639\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 494us/step - loss: 0.1144 - val_loss: 2.9418\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - 0s 456us/step - loss: 0.0962 - val_loss: 3.4258\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 475us/step - loss: 0.1024 - val_loss: 3.7266\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 447us/step - loss: 0.0895 - val_loss: 3.2056\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 504us/step - loss: 0.0942 - val_loss: 3.5356\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 465us/step - loss: 0.0923 - val_loss: 3.5213\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.166 - 0s 940us/step - loss: 0.0803 - val_loss: 3.2878\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 408us/step - loss: 0.0772 - val_loss: 3.4443\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - 0s 408us/step - loss: 0.0808 - val_loss: 3.4355\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - 0s 380us/step - loss: 0.0847 - val_loss: 3.3488\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 513us/step - loss: 0.0792 - val_loss: 3.3624\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 427us/step - loss: 0.0679 - val_loss: 3.2030\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 484us/step - loss: 0.0651 - val_loss: 3.2584\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - 0s 437us/step - loss: 0.0701 - val_loss: 3.1783\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 389us/step - loss: 0.0702 - val_loss: 2.9984\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 513us/step - loss: 0.0787 - val_loss: 3.0900\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - 0s 408us/step - loss: 0.0680 - val_loss: 3.2576\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 418us/step - loss: 0.0722 - val_loss: 2.8304\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 15\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.466 - 1s 7ms/step - loss: 0.7189 - val_loss: 0.5529\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.294 - 0s 503us/step - loss: 0.5699 - val_loss: 0.8246\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - 0s 494us/step - loss: 0.4983 - val_loss: 0.6996\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.386 - 0s 399us/step - loss: 0.4591 - val_loss: 0.6288\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.709 - 0s 446us/step - loss: 0.4055 - val_loss: 0.8015\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.002 - 0s 399us/step - loss: 0.3684 - val_loss: 0.8399\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - 0s 427us/step - loss: 0.3436 - val_loss: 0.9118\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 437us/step - loss: 0.3420 - val_loss: 0.8054\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.722 - 0s 427us/step - loss: 0.3028 - val_loss: 0.8366\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.643 - 0s 437us/step - loss: 0.2766 - val_loss: 0.8695\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - 0s 446us/step - loss: 0.2521 - val_loss: 0.9332\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - 0s 418us/step - loss: 0.2491 - val_loss: 0.8651\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 437us/step - loss: 0.2337 - val_loss: 0.7523\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 427us/step - loss: 0.2099 - val_loss: 0.6974\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - 0s 418us/step - loss: 0.2017 - val_loss: 0.8179\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 418us/step - loss: 0.1933 - val_loss: 0.7209\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - 0s 418us/step - loss: 0.1819 - val_loss: 0.7319\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 465us/step - loss: 0.1625 - val_loss: 0.8253\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - 0s 408us/step - loss: 0.1686 - val_loss: 0.5864\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.340 - 0s 437us/step - loss: 0.1686 - val_loss: 0.7504\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - 0s 418us/step - loss: 0.1504 - val_loss: 0.7775\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - 0s 427us/step - loss: 0.1357 - val_loss: 0.6585\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 428us/step - loss: 0.1291 - val_loss: 0.5427\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 437us/step - loss: 0.1526 - val_loss: 0.6598\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - 0s 427us/step - loss: 0.1199 - val_loss: 0.6533\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 437us/step - loss: 0.1242 - val_loss: 0.6922\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - 0s 418us/step - loss: 0.1162 - val_loss: 0.7348\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - 0s 427us/step - loss: 0.1205 - val_loss: 0.6840\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - 0s 428us/step - loss: 0.1154 - val_loss: 0.8225\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - 0s 418us/step - loss: 0.1049 - val_loss: 0.6957\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 456us/step - loss: 0.0992 - val_loss: 0.7192\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - 0s 418us/step - loss: 0.1029 - val_loss: 0.7338\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 456us/step - loss: 0.0910 - val_loss: 0.7136\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - 0s 427us/step - loss: 0.0871 - val_loss: 0.7498\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 427us/step - loss: 0.1012 - val_loss: 0.7574\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 427us/step - loss: 0.0836 - val_loss: 0.7399\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 437us/step - loss: 0.0879 - val_loss: 0.7046\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 418us/step - loss: 0.0750 - val_loss: 0.7179\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - 0s 428us/step - loss: 0.0821 - val_loss: 0.7056\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 437us/step - loss: 0.0684 - val_loss: 0.6893\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 427us/step - loss: 0.0711 - val_loss: 0.6908\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - 0s 418us/step - loss: 0.0739 - val_loss: 0.7130\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 560us/step - loss: 0.0790 - val_loss: 0.7780\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 475us/step - loss: 0.0733 - val_loss: 0.6620\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 466us/step - loss: 0.0812 - val_loss: 0.7684\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 532us/step - loss: 0.0719 - val_loss: 0.7050\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 447us/step - loss: 0.0581 - val_loss: 0.6624\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 446us/step - loss: 0.0721 - val_loss: 0.6563\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 513us/step - loss: 0.0610 - val_loss: 0.6970\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 484us/step - loss: 0.0642 - val_loss: 0.6879\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 503us/step - loss: 0.0606 - val_loss: 0.7716\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 484us/step - loss: 0.0742 - val_loss: 0.7557\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 446us/step - loss: 0.0720 - val_loss: 0.7214\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - 0s 484us/step - loss: 0.0762 - val_loss: 0.6011\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 427us/step - loss: 0.0591 - val_loss: 0.6860\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 437us/step - loss: 0.0723 - val_loss: 0.7126\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 418us/step - loss: 0.0599 - val_loss: 0.6143\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 465us/step - loss: 0.0555 - val_loss: 0.7528\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 437us/step - loss: 0.0640 - val_loss: 0.7059\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - 0s 418us/step - loss: 0.0570 - val_loss: 0.7277\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - 0s 484us/step - loss: 0.0581 - val_loss: 0.6927\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 446us/step - loss: 0.0476 - val_loss: 0.6200\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - 0s 447us/step - loss: 0.0474 - val_loss: 0.6607\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 437us/step - loss: 0.0372 - val_loss: 0.6067\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 456us/step - loss: 0.0419 - val_loss: 0.6508\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 437us/step - loss: 0.0474 - val_loss: 0.6151\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 447us/step - loss: 0.0508 - val_loss: 0.5708\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 418us/step - loss: 0.0565 - val_loss: 0.6717\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 456us/step - loss: 0.0441 - val_loss: 0.6566\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 427us/step - loss: 0.0495 - val_loss: 0.7173\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 456us/step - loss: 0.0418 - val_loss: 0.6743\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 437us/step - loss: 0.0454 - val_loss: 0.7329\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - 0s 437us/step - loss: 0.0395 - val_loss: 0.6044\n",
      "Epoch 00073: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 16\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.818 - 1s 7ms/step - loss: 0.8992 - val_loss: 0.3953\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.898 - 0s 466us/step - loss: 0.5564 - val_loss: 0.8945\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.068 - 0s 428us/step - loss: 0.5463 - val_loss: 0.9429\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.533 - 0s 437us/step - loss: 0.4798 - val_loss: 0.8514\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.624 - 0s 418us/step - loss: 0.4479 - val_loss: 0.8268\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 408us/step - loss: 0.3998 - val_loss: 0.6161\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.404 - 0s 427us/step - loss: 0.4116 - val_loss: 0.7873\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - 0s 447us/step - loss: 0.3843 - val_loss: 0.7945\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 418us/step - loss: 0.3467 - val_loss: 0.6291\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - 0s 427us/step - loss: 0.3353 - val_loss: 0.7626\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - 0s 446us/step - loss: 0.3105 - val_loss: 0.6629\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - 0s 456us/step - loss: 0.3142 - val_loss: 1.4925\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 437us/step - loss: 0.2830 - val_loss: 0.9889\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - 0s 513us/step - loss: 0.2628 - val_loss: 0.7693\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - 0s 475us/step - loss: 0.2546 - val_loss: 0.9002\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.380 - 0s 437us/step - loss: 0.2506 - val_loss: 0.8801\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 427us/step - loss: 0.2306 - val_loss: 0.8790\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - 0s 427us/step - loss: 0.2334 - val_loss: 0.8370\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - 0s 437us/step - loss: 0.2085 - val_loss: 0.8236\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 437us/step - loss: 0.2244 - val_loss: 0.8548\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - 0s 427us/step - loss: 0.2104 - val_loss: 0.7660\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - 0s 428us/step - loss: 0.1939 - val_loss: 0.8563\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 427us/step - loss: 0.1861 - val_loss: 0.7815\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - 0s 551us/step - loss: 0.1872 - val_loss: 0.7392\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - 0s 494us/step - loss: 0.2008 - val_loss: 0.6658\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - 0s 437us/step - loss: 0.1907 - val_loss: 1.0331\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - 0s 446us/step - loss: 0.1858 - val_loss: 0.8171\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - 0s 504us/step - loss: 0.1649 - val_loss: 0.7242\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - 0s 503us/step - loss: 0.1645 - val_loss: 0.7009\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - 0s 494us/step - loss: 0.1738 - val_loss: 0.6157\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.170 - 0s 741us/step - loss: 0.1556 - val_loss: 0.7703\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - 0s 466us/step - loss: 0.1476 - val_loss: 0.7472\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 494us/step - loss: 0.1559 - val_loss: 0.6808\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - 0s 465us/step - loss: 0.1453 - val_loss: 0.7494\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.144 - 0s 1ms/step - loss: 0.1384 - val_loss: 0.7235\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - 0s 503us/step - loss: 0.1360 - val_loss: 0.6914\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 418us/step - loss: 0.1279 - val_loss: 0.7625\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 408us/step - loss: 0.1453 - val_loss: 0.9353\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - 0s 408us/step - loss: 0.1152 - val_loss: 0.8253\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 447us/step - loss: 0.1326 - val_loss: 0.7150\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 418us/step - loss: 0.1141 - val_loss: 0.7613\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 427us/step - loss: 0.1128 - val_loss: 0.7812\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - 0s 399us/step - loss: 0.1060 - val_loss: 0.6154\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 446us/step - loss: 0.1146 - val_loss: 0.6296\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 418us/step - loss: 0.1044 - val_loss: 0.6406\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - 0s 446us/step - loss: 0.1052 - val_loss: 0.5340\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 427us/step - loss: 0.0951 - val_loss: 0.5728\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - 0s 447us/step - loss: 0.0983 - val_loss: 0.6867\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 408us/step - loss: 0.0883 - val_loss: 0.6851\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - 0s 456us/step - loss: 0.0964 - val_loss: 0.5162\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 484us/step - loss: 0.0925 - val_loss: 0.5456\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 17\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.326 - 1s 7ms/step - loss: 0.7705 - val_loss: 0.3625\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - 0s 456us/step - loss: 0.6501 - val_loss: 0.5790\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.535 - 0s 447us/step - loss: 0.6041 - val_loss: 0.4752\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - 0s 475us/step - loss: 0.5657 - val_loss: 0.4328\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.487 - 0s 418us/step - loss: 0.5382 - val_loss: 0.3424\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.484 - 0s 408us/step - loss: 0.5155 - val_loss: 0.3860\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.705 - 0s 389us/step - loss: 0.4679 - val_loss: 0.3897\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.403 - 0s 428us/step - loss: 0.4315 - val_loss: 0.3932\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.544 - 0s 475us/step - loss: 0.4310 - val_loss: 0.7422\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - 0s 427us/step - loss: 0.3943 - val_loss: 0.6297\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.372 - 0s 437us/step - loss: 0.3656 - val_loss: 0.7172\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.221 - 0s 408us/step - loss: 0.3765 - val_loss: 0.7086\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 475us/step - loss: 0.3160 - val_loss: 0.6736\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - 0s 408us/step - loss: 0.3300 - val_loss: 0.7688\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - 0s 447us/step - loss: 0.2984 - val_loss: 0.8116\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.795 - 0s 409us/step - loss: 0.2989 - val_loss: 1.0519\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - 0s 399us/step - loss: 0.2680 - val_loss: 1.0312\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - 0s 456us/step - loss: 0.2918 - val_loss: 0.9295\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.704 - 0s 456us/step - loss: 0.2565 - val_loss: 1.2064\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - 0s 437us/step - loss: 0.2643 - val_loss: 0.9542\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - 0s 484us/step - loss: 0.2450 - val_loss: 1.0580\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - 0s 475us/step - loss: 0.2505 - val_loss: 0.9774\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.429 - 0s 427us/step - loss: 0.2379 - val_loss: 1.2521\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 465us/step - loss: 0.2003 - val_loss: 1.1659\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 427us/step - loss: 0.1947 - val_loss: 1.3026\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - 0s 427us/step - loss: 0.2087 - val_loss: 1.2111\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - 0s 408us/step - loss: 0.1989 - val_loss: 1.3939\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - 0s 494us/step - loss: 0.1987 - val_loss: 1.3131\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 427us/step - loss: 0.1787 - val_loss: 1.3427\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 408us/step - loss: 0.1780 - val_loss: 1.5170\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - 0s 437us/step - loss: 0.1868 - val_loss: 1.3002\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - 0s 399us/step - loss: 0.1750 - val_loss: 1.5485\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - 0s 465us/step - loss: 0.1834 - val_loss: 1.3475\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - 0s 427us/step - loss: 0.1736 - val_loss: 1.4757\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - 0s 418us/step - loss: 0.1602 - val_loss: 1.4688\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - 0s 437us/step - loss: 0.1630 - val_loss: 1.5651\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 408us/step - loss: 0.1529 - val_loss: 1.4017\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 494us/step - loss: 0.1532 - val_loss: 1.4302\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - 0s 380us/step - loss: 0.1556 - val_loss: 1.6376\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 446us/step - loss: 0.1547 - val_loss: 1.7115\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 427us/step - loss: 0.1392 - val_loss: 1.3403\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - 0s 446us/step - loss: 0.1406 - val_loss: 1.5331\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 466us/step - loss: 0.1371 - val_loss: 1.4512\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - 0s 475us/step - loss: 0.1382 - val_loss: 1.4773\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - 0s 418us/step - loss: 0.1279 - val_loss: 1.4667\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 428us/step - loss: 0.1301 - val_loss: 1.3948\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - 0s 418us/step - loss: 0.1343 - val_loss: 1.3943\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 409us/step - loss: 0.1294 - val_loss: 1.4242\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 389us/step - loss: 0.1258 - val_loss: 1.5892\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.041 - 0s 1ms/step - loss: 0.1143 - val_loss: 1.6012\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 446us/step - loss: 0.1255 - val_loss: 1.6868\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 437us/step - loss: 0.1163 - val_loss: 1.5988\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.412 - 0s 427us/step - loss: 0.1123 - val_loss: 1.5593\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - 0s 418us/step - loss: 0.1260 - val_loss: 1.5874\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 418us/step - loss: 0.1162 - val_loss: 1.5527\n",
      "Epoch 00055: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 18\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.513 - 1s 7ms/step - loss: 0.8571 - val_loss: 0.2819\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.948 - 0s 456us/step - loss: 0.5334 - val_loss: 0.7670\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.727 - 0s 418us/step - loss: 0.5379 - val_loss: 0.8649\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.752 - 0s 408us/step - loss: 0.4813 - val_loss: 0.7322\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 418us/step - loss: 0.4236 - val_loss: 0.7481\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - 0s 446us/step - loss: 0.4032 - val_loss: 0.8398\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - 0s 408us/step - loss: 0.3739 - val_loss: 0.7472\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.891 - 0s 427us/step - loss: 0.3571 - val_loss: 0.8310\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 446us/step - loss: 0.3440 - val_loss: 1.0098\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - 0s 427us/step - loss: 0.3077 - val_loss: 1.0309\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - 0s 503us/step - loss: 0.2838 - val_loss: 0.9669\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - 0s 465us/step - loss: 0.2976 - val_loss: 1.0190\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - 0s 427us/step - loss: 0.2425 - val_loss: 1.0457\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.437 - 0s 570us/step - loss: 0.2435 - val_loss: 1.0954\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.917 - 0s 447us/step - loss: 0.2232 - val_loss: 0.8361\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 475us/step - loss: 0.2287 - val_loss: 1.0160\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 437us/step - loss: 0.2076 - val_loss: 1.0886\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - 0s 446us/step - loss: 0.2025 - val_loss: 1.0096\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.185 - 0s 1ms/step - loss: 0.1958 - val_loss: 0.9040\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.190 - 0s 731us/step - loss: 0.1766 - val_loss: 1.0244\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.243 - 0s 408us/step - loss: 0.1747 - val_loss: 1.1177\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.191 - 0s 2ms/step - loss: 0.1696 - val_loss: 1.0027\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - 0s 494us/step - loss: 0.1552 - val_loss: 1.0961\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.127 - 0s 731us/step - loss: 0.1610 - val_loss: 0.9551\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.356 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.121 - 0s 1ms/step - loss: 0.1628 - val_loss: 1.0006\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.166 - 0s 646us/step - loss: 0.1713 - val_loss: 1.1020\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.144 - 0s 1ms/step - loss: 0.1438 - val_loss: 0.9375\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - 0s 551us/step - loss: 0.1384 - val_loss: 1.0146\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.136 - 0s 1ms/step - loss: 0.1354 - val_loss: 0.9870\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.117 - 0s 703us/step - loss: 0.1385 - val_loss: 0.9593\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 655us/step - loss: 0.1216 - val_loss: 0.9860\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 541us/step - loss: 0.1148 - val_loss: 0.9980\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - 0s 342us/step - loss: 0.1176 - val_loss: 0.9093\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - 0s 361us/step - loss: 0.1144 - val_loss: 0.9315\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 332us/step - loss: 0.1128 - val_loss: 0.9982\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 342us/step - loss: 0.1059 - val_loss: 0.8827\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 351us/step - loss: 0.1222 - val_loss: 0.8069\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 351us/step - loss: 0.1040 - val_loss: 1.1230\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.095 - 0s 1ms/step - loss: 0.0948 - val_loss: 0.9082\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.104 - 0s 769us/step - loss: 0.1155 - val_loss: 1.0459\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - 0s 352us/step - loss: 0.0876 - val_loss: 0.8987\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - 0s 361us/step - loss: 0.1060 - val_loss: 0.9679\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.088 - 0s 636us/step - loss: 0.0875 - val_loss: 0.9765\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.082 - 0s 2ms/step - loss: 0.0876 - val_loss: 0.9265\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.076 - 0s 940us/step - loss: 0.0856 - val_loss: 0.9045\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.080 - 0s 665us/step - loss: 0.0842 - val_loss: 0.9555\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 475us/step - loss: 0.0791 - val_loss: 0.9775\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 447us/step - loss: 0.0950 - val_loss: 1.0102\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - 0s 437us/step - loss: 0.0891 - val_loss: 1.0982\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 418us/step - loss: 0.0836 - val_loss: 0.8161\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - 0s 465us/step - loss: 0.0771 - val_loss: 1.1438\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 19\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.689 - 1s 7ms/step - loss: 0.7244 - val_loss: 0.5280\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 494us/step - loss: 0.4819 - val_loss: 1.0148\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - 0s 437us/step - loss: 0.4036 - val_loss: 1.0148\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - 0s 437us/step - loss: 0.3706 - val_loss: 1.1677\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - 0s 437us/step - loss: 0.3461 - val_loss: 1.1140\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - 0s 418us/step - loss: 0.3194 - val_loss: 1.1071\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - 0s 456us/step - loss: 0.2868 - val_loss: 0.9373\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.636 - 0s 522us/step - loss: 0.2775 - val_loss: 1.2823\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 475us/step - loss: 0.2595 - val_loss: 1.4405\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - 0s 447us/step - loss: 0.2572 - val_loss: 0.9624\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - 0s 427us/step - loss: 0.2355 - val_loss: 1.1115\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - 0s 418us/step - loss: 0.2077 - val_loss: 1.4194\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 427us/step - loss: 0.1983 - val_loss: 1.1860\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - 0s 427us/step - loss: 0.1815 - val_loss: 1.2860\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - 0s 447us/step - loss: 0.1878 - val_loss: 1.4066\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 437us/step - loss: 0.1758 - val_loss: 1.2650\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 437us/step - loss: 0.1635 - val_loss: 1.3156\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - 0s 427us/step - loss: 0.1662 - val_loss: 1.4254\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - 0s 437us/step - loss: 0.1641 - val_loss: 1.3578\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - 0s 408us/step - loss: 0.1432 - val_loss: 1.3682\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 427us/step - loss: 0.1396 - val_loss: 1.4026\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - 0s 475us/step - loss: 0.1316 - val_loss: 1.3012\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - 0s 446us/step - loss: 0.1300 - val_loss: 1.6500\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - 0s 418us/step - loss: 0.1368 - val_loss: 1.4556\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 447us/step - loss: 0.1169 - val_loss: 1.4842\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 428us/step - loss: 0.1226 - val_loss: 1.5370\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - 0s 456us/step - loss: 0.1036 - val_loss: 1.3592\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.134 - 0s 912us/step - loss: 0.1139 - val_loss: 1.6122\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 513us/step - loss: 0.1018 - val_loss: 1.6313\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - 0s 456us/step - loss: 0.1036 - val_loss: 1.5245\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - 0s 428us/step - loss: 0.0948 - val_loss: 1.2965\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - 0s 456us/step - loss: 0.0929 - val_loss: 1.6511\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 437us/step - loss: 0.0911 - val_loss: 1.5885\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 427us/step - loss: 0.0910 - val_loss: 1.3222\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 446us/step - loss: 0.0880 - val_loss: 1.5747\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 418us/step - loss: 0.0899 - val_loss: 1.4592\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 465us/step - loss: 0.0818 - val_loss: 1.7193\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 399us/step - loss: 0.0808 - val_loss: 1.5483\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 446us/step - loss: 0.0670 - val_loss: 1.6517\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 418us/step - loss: 0.0661 - val_loss: 1.2970\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 456us/step - loss: 0.0809 - val_loss: 1.9882\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 408us/step - loss: 0.0788 - val_loss: 1.1446\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 437us/step - loss: 0.0737 - val_loss: 1.5100\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 446us/step - loss: 0.0686 - val_loss: 1.4750\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 456us/step - loss: 0.0672 - val_loss: 1.5671\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.067 - 0s 779us/step - loss: 0.0630 - val_loss: 1.5351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.064 - 0s 902us/step - loss: 0.0596 - val_loss: 1.5618\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.057 - 0s 665us/step - loss: 0.0575 - val_loss: 1.7021\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - 0s 446us/step - loss: 0.0537 - val_loss: 1.5641\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 484us/step - loss: 0.0541 - val_loss: 1.7539\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 418us/step - loss: 0.0532 - val_loss: 1.6415\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 20\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.226 - 1s 7ms/step - loss: 0.7988 - val_loss: 0.4697\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.433 - 0s 437us/step - loss: 0.6242 - val_loss: 0.6918\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.654 - 0s 437us/step - loss: 0.5332 - val_loss: 0.4103\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.290 - 0s 446us/step - loss: 0.5046 - val_loss: 0.3665\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - 0s 437us/step - loss: 0.4450 - val_loss: 0.4650\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - 0s 427us/step - loss: 0.4366 - val_loss: 0.4744\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.258 - 0s 446us/step - loss: 0.4206 - val_loss: 0.5873\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - 0s 456us/step - loss: 0.3835 - val_loss: 0.4418\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - 0s 408us/step - loss: 0.3380 - val_loss: 0.5738\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - 0s 427us/step - loss: 0.3481 - val_loss: 0.4627\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - 0s 437us/step - loss: 0.3143 - val_loss: 0.4809\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - 0s 437us/step - loss: 0.3236 - val_loss: 0.4281\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - 0s 418us/step - loss: 0.3457 - val_loss: 0.2428\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 418us/step - loss: 0.2999 - val_loss: 0.4688\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.374 - 0s 446us/step - loss: 0.2835 - val_loss: 0.4387\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - 0s 447us/step - loss: 0.2597 - val_loss: 0.3620\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - 0s 513us/step - loss: 0.2506 - val_loss: 0.4171\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - 0s 475us/step - loss: 0.2334 - val_loss: 0.5687\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - 0s 427us/step - loss: 0.2243 - val_loss: 0.5367\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - 0s 408us/step - loss: 0.2277 - val_loss: 0.6264\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - 0s 418us/step - loss: 0.2065 - val_loss: 0.6629\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - 0s 437us/step - loss: 0.2053 - val_loss: 0.6406\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.298 - 0s 446us/step - loss: 0.2045 - val_loss: 0.6252\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.335 - 0s 456us/step - loss: 0.1898 - val_loss: 0.6163\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - 0s 437us/step - loss: 0.1910 - val_loss: 0.6505\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.267 - 0s 437us/step - loss: 0.1880 - val_loss: 0.6091\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 428us/step - loss: 0.1671 - val_loss: 0.6301\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - 0s 427us/step - loss: 0.1824 - val_loss: 0.6462\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - 0s 399us/step - loss: 0.1675 - val_loss: 0.7680\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - 0s 408us/step - loss: 0.1648 - val_loss: 0.6872\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - 0s 437us/step - loss: 0.1441 - val_loss: 0.7011\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 446us/step - loss: 0.1511 - val_loss: 0.7068\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - 0s 418us/step - loss: 0.1424 - val_loss: 0.7748\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - 0s 428us/step - loss: 0.1387 - val_loss: 0.7599\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 418us/step - loss: 0.1396 - val_loss: 0.7749\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - 0s 475us/step - loss: 0.1472 - val_loss: 0.7714\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 513us/step - loss: 0.1301 - val_loss: 0.7749\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - 0s 522us/step - loss: 0.1405 - val_loss: 0.8339\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 485us/step - loss: 0.1207 - val_loss: 0.8307\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - 0s 466us/step - loss: 0.1278 - val_loss: 0.8455\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - 0s 446us/step - loss: 0.1231 - val_loss: 0.8387\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - 0s 484us/step - loss: 0.1159 - val_loss: 0.8680\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 475us/step - loss: 0.1249 - val_loss: 0.8706\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 503us/step - loss: 0.1088 - val_loss: 0.8757\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 465us/step - loss: 0.1028 - val_loss: 0.9260\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 399us/step - loss: 0.1164 - val_loss: 0.8799\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - 0s 494us/step - loss: 0.1171 - val_loss: 0.8812\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 380us/step - loss: 0.1199 - val_loss: 0.8514\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - 0s 484us/step - loss: 0.1204 - val_loss: 0.8990\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 456us/step - loss: 0.1011 - val_loss: 0.9090\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - 0s 551us/step - loss: 0.1056 - val_loss: 0.9270\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 513us/step - loss: 0.1079 - val_loss: 0.9275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 446us/step - loss: 0.0928 - val_loss: 0.9337\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 446us/step - loss: 0.1041 - val_loss: 0.9676\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 503us/step - loss: 0.0928 - val_loss: 0.9839\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - 0s 427us/step - loss: 0.0859 - val_loss: 0.9905\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 484us/step - loss: 0.0902 - val_loss: 1.0277\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 475us/step - loss: 0.0999 - val_loss: 1.0137\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 456us/step - loss: 0.0848 - val_loss: 1.0036\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - 0s 456us/step - loss: 0.0917 - val_loss: 0.9842\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 428us/step - loss: 0.0954 - val_loss: 1.0550\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 456us/step - loss: 0.0848 - val_loss: 1.0238\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - 0s 475us/step - loss: 0.0893 - val_loss: 1.0592\n",
      "Epoch 00063: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 21\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 1.157 - 1s 7ms/step - loss: 0.7931 - val_loss: 0.3954\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.017 - ETA: 0s - loss: 0.724 - 0s 931us/step - loss: 0.6402 - val_loss: 0.7115\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.462 - 0s 408us/step - loss: 0.5971 - val_loss: 0.5452\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.438 - 0s 418us/step - loss: 0.5622 - val_loss: 0.5288\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.753 - 0s 484us/step - loss: 0.5369 - val_loss: 0.6025\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.682 - 0s 446us/step - loss: 0.5006 - val_loss: 0.7309\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.397 - 0s 418us/step - loss: 0.4882 - val_loss: 0.9098\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.850 - 0s 446us/step - loss: 0.4855 - val_loss: 0.7438\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - 0s 446us/step - loss: 0.4517 - val_loss: 0.7941\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.344 - 0s 427us/step - loss: 0.4455 - val_loss: 0.9283\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.490 - 0s 456us/step - loss: 0.4269 - val_loss: 0.9604\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.435 - 0s 532us/step - loss: 0.4153 - val_loss: 0.8999\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.550 - 0s 427us/step - loss: 0.3939 - val_loss: 0.9546\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.349 - 0s 1ms/step - loss: 0.3832 - val_loss: 0.9613\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.594 - 0s 427us/step - loss: 0.3782 - val_loss: 0.9284\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.690 - 0s 456us/step - loss: 0.3568 - val_loss: 0.8139\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - 0s 399us/step - loss: 0.3867 - val_loss: 0.9236\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.419 - 0s 446us/step - loss: 0.3379 - val_loss: 0.9676\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.410 - 0s 484us/step - loss: 0.3238 - val_loss: 1.0044\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - 0s 446us/step - loss: 0.3243 - val_loss: 0.9785\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 589us/step - loss: 0.3176 - val_loss: 0.9303\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.598 - 0s 437us/step - loss: 0.2944 - val_loss: 1.0680\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - 0s 446us/step - loss: 0.3192 - val_loss: 0.7996\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.609 - 0s 446us/step - loss: 0.2727 - val_loss: 1.1800\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - 0s 494us/step - loss: 0.2829 - val_loss: 0.9268\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.553 - 0s 484us/step - loss: 0.2810 - val_loss: 0.9576\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - 0s 437us/step - loss: 0.2537 - val_loss: 1.0580\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - 0s 446us/step - loss: 0.2481 - val_loss: 0.9254\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.410 - 0s 437us/step - loss: 0.2521 - val_loss: 1.0149\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 427us/step - loss: 0.2433 - val_loss: 1.0168\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - 0s 418us/step - loss: 0.2456 - val_loss: 0.9394\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - 0s 532us/step - loss: 0.2302 - val_loss: 1.0500\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - 0s 475us/step - loss: 0.2277 - val_loss: 0.9754\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 485us/step - loss: 0.2031 - val_loss: 1.0335\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.497 - 0s 437us/step - loss: 0.2072 - val_loss: 0.9620\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - 0s 446us/step - loss: 0.2218 - val_loss: 0.9966\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - 0s 446us/step - loss: 0.1977 - val_loss: 1.1950\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - 0s 513us/step - loss: 0.2115 - val_loss: 0.9951\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - 0s 513us/step - loss: 0.1997 - val_loss: 0.9015\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - 0s 494us/step - loss: 0.2087 - val_loss: 1.0929\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - 0s 513us/step - loss: 0.2013 - val_loss: 0.9037\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - 0s 513us/step - loss: 0.2145 - val_loss: 1.1818\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 513us/step - loss: 0.1852 - val_loss: 1.0955\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 513us/step - loss: 0.1722 - val_loss: 0.9525\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - 0s 446us/step - loss: 0.1715 - val_loss: 1.0659\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 437us/step - loss: 0.1649 - val_loss: 1.0483\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - 0s 446us/step - loss: 0.1572 - val_loss: 0.9959\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - 0s 418us/step - loss: 0.1574 - val_loss: 1.1563\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - 0s 466us/step - loss: 0.1412 - val_loss: 0.9727\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 484us/step - loss: 0.1413 - val_loss: 0.9868\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 447us/step - loss: 0.1538 - val_loss: 0.8488\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 22\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.273 - 1s 7ms/step - loss: 0.9133 - val_loss: 0.3624\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.713 - 0s 456us/step - loss: 0.6818 - val_loss: 0.7027\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - 0s 428us/step - loss: 0.6339 - val_loss: 0.6819\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.955 - 0s 503us/step - loss: 0.5723 - val_loss: 0.7367\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - 0s 570us/step - loss: 0.5319 - val_loss: 0.7873\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - 0s 532us/step - loss: 0.5435 - val_loss: 1.0171\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.243 - 0s 503us/step - loss: 0.5198 - val_loss: 0.8902\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.110 - 0s 532us/step - loss: 0.4361 - val_loss: 1.1460\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - 0s 475us/step - loss: 0.3864 - val_loss: 1.1910\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - 0s 494us/step - loss: 0.4121 - val_loss: 0.9826\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - 0s 485us/step - loss: 0.3494 - val_loss: 1.4373\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - 0s 446us/step - loss: 0.3249 - val_loss: 1.3428\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - 0s 513us/step - loss: 0.3072 - val_loss: 1.4654\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - 0s 481us/step - loss: 0.2901 - val_loss: 1.4927\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - 0s 427us/step - loss: 0.2739 - val_loss: 1.6505\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.403 - 0s 437us/step - loss: 0.2647 - val_loss: 1.1539\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - 0s 446us/step - loss: 0.2898 - val_loss: 1.3632\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 731us/step - loss: 0.1982 - val_loss: 1.6425\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 584us/step - loss: 0.1938 - val_loss: 2.0502\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - 0s 437us/step - loss: 0.1744 - val_loss: 1.7392\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - 0s 427us/step - loss: 0.1510 - val_loss: 1.9136\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.356 - 0s 418us/step - loss: 0.1505 - val_loss: 2.0146\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.292 - 0s 522us/step - loss: 0.1245 - val_loss: 1.8872\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 503us/step - loss: 0.1464 - val_loss: 1.8558\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 418us/step - loss: 0.1155 - val_loss: 1.9262\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 484us/step - loss: 0.1332 - val_loss: 1.8407\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 503us/step - loss: 0.1286 - val_loss: 2.6485\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - 0s 465us/step - loss: 0.1194 - val_loss: 1.4453\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - 0s 456us/step - loss: 0.1212 - val_loss: 2.3598\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - 0s 522us/step - loss: 0.1718 - val_loss: 2.4047\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - 0s 598us/step - loss: 0.1128 - val_loss: 2.2707\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 551us/step - loss: 0.0728 - val_loss: 2.9507\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.085 - 0s 684us/step - loss: 0.0851 - val_loss: 2.1310\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - 0s 646us/step - loss: 0.0869 - val_loss: 2.6017\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 427us/step - loss: 0.0656 - val_loss: 2.2438\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 427us/step - loss: 0.0800 - val_loss: 2.3546\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - 0s 483us/step - loss: 0.0549 - val_loss: 2.8115\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - 0s 503us/step - loss: 0.0623 - val_loss: 2.4413\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 491us/step - loss: 0.0612 - val_loss: 2.2131\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.070 - 0s 693us/step - loss: 0.0695 - val_loss: 2.3993\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.067 - 0s 959us/step - loss: 0.0633 - val_loss: 2.5168\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 361us/step - loss: 0.0590 - val_loss: 2.4970\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 408us/step - loss: 0.0616 - val_loss: 2.5603\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 399us/step - loss: 0.0481 - val_loss: 2.2301\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - 0s 361us/step - loss: 0.0477 - val_loss: 2.1170\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - 0s 430us/step - loss: 0.0490 - val_loss: 2.7676\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 370us/step - loss: 0.0632 - val_loss: 3.2275\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 380us/step - loss: 0.0907 - val_loss: 1.9170\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 351us/step - loss: 0.0647 - val_loss: 2.6219\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - 0s 351us/step - loss: 0.0493 - val_loss: 2.0785\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - 0s 351us/step - loss: 0.0434 - val_loss: 2.1848\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 23\n",
      "Model: \"sequential_1\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.147 - 1s 7ms/step - loss: 0.6639 - val_loss: 0.4698\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.960 - 0s 361us/step - loss: 0.5038 - val_loss: 0.7506\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - 0s 361us/step - loss: 0.4422 - val_loss: 0.6207\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - 0s 351us/step - loss: 0.3874 - val_loss: 0.7872\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.981 - 0s 456us/step - loss: 0.3175 - val_loss: 0.7925\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - 0s 380us/step - loss: 0.3127 - val_loss: 0.5402\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - 0s 342us/step - loss: 0.2483 - val_loss: 0.7646\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - 0s 352us/step - loss: 0.2118 - val_loss: 0.8016\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.234 - 0s 826us/step - loss: 0.2209 - val_loss: 0.7927\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 371us/step - loss: 0.1966 - val_loss: 0.8811\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - 0s 418us/step - loss: 0.1799 - val_loss: 0.8769\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 437us/step - loss: 0.1660 - val_loss: 0.8938\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 380us/step - loss: 0.1479 - val_loss: 1.0075\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 380us/step - loss: 0.1251 - val_loss: 1.0919\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 370us/step - loss: 0.1299 - val_loss: 1.2129\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - 0s 466us/step - loss: 0.1173 - val_loss: 1.1964\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 380us/step - loss: 0.1300 - val_loss: 1.3392\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - 0s 399us/step - loss: 0.1019 - val_loss: 1.1172\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 359us/step - loss: 0.0946 - val_loss: 1.1298\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 370us/step - loss: 0.0845 - val_loss: 1.1786\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 523us/step - loss: 0.0841 - val_loss: 1.2795\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 390us/step - loss: 0.0884 - val_loss: 1.3958\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - 0s 371us/step - loss: 0.0832 - val_loss: 1.2181\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 380us/step - loss: 0.0754 - val_loss: 1.2345\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - 0s 370us/step - loss: 0.0689 - val_loss: 1.5335\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - 0s 386us/step - loss: 0.0777 - val_loss: 1.0302\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - 0s 380us/step - loss: 0.0783 - val_loss: 1.4149\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 380us/step - loss: 0.0890 - val_loss: 1.6400\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 380us/step - loss: 0.0677 - val_loss: 1.1994\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - 0s 465us/step - loss: 0.0749 - val_loss: 1.4964\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - 0s 380us/step - loss: 0.0750 - val_loss: 1.3532\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 371us/step - loss: 0.0713 - val_loss: 1.5300\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 399us/step - loss: 0.0498 - val_loss: 1.4572\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - 0s 408us/step - loss: 0.0550 - val_loss: 1.5777\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 399us/step - loss: 0.0623 - val_loss: 1.4274\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 409us/step - loss: 0.0530 - val_loss: 1.7327\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - 0s 370us/step - loss: 0.0526 - val_loss: 1.4723\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 408us/step - loss: 0.0503 - val_loss: 1.3327\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 352us/step - loss: 0.0506 - val_loss: 1.5297\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 351us/step - loss: 0.0514 - val_loss: 1.3930\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 342us/step - loss: 0.0528 - val_loss: 1.4953\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 361us/step - loss: 0.0452 - val_loss: 1.4155\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 371us/step - loss: 0.0524 - val_loss: 1.4798\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 351us/step - loss: 0.0518 - val_loss: 1.5475\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 342us/step - loss: 0.0491 - val_loss: 1.3092\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 361us/step - loss: 0.0460 - val_loss: 1.7058\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 380us/step - loss: 0.0399 - val_loss: 1.1192\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 351us/step - loss: 0.0511 - val_loss: 1.4635\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 370us/step - loss: 0.0534 - val_loss: 1.5613\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 351us/step - loss: 0.0473 - val_loss: 1.2765\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 370us/step - loss: 0.0370 - val_loss: 1.3732\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 24\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 8s - loss: 0.873 - 1s 8ms/step - loss: 0.7215 - val_loss: 0.3223\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - 0s 399us/step - loss: 0.5684 - val_loss: 0.4674\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - 0s 513us/step - loss: 0.5157 - val_loss: 0.6977\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.832 - 0s 437us/step - loss: 0.4563 - val_loss: 0.5466\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.966 - 0s 428us/step - loss: 0.3962 - val_loss: 0.4718\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - 0s 427us/step - loss: 0.3797 - val_loss: 0.4340\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - 0s 380us/step - loss: 0.3599 - val_loss: 0.4826\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.409 - 0s 380us/step - loss: 0.3245 - val_loss: 0.5734\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.385 - 0s 370us/step - loss: 0.2806 - val_loss: 0.4401\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - 0s 390us/step - loss: 0.2616 - val_loss: 0.3966\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - 0s 456us/step - loss: 0.2434 - val_loss: 0.7120\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 361us/step - loss: 0.2515 - val_loss: 0.5746\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 361us/step - loss: 0.2419 - val_loss: 0.4369\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - 0s 351us/step - loss: 0.2223 - val_loss: 0.5761\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - 0s 351us/step - loss: 0.2004 - val_loss: 0.5409\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - 0s 370us/step - loss: 0.1937 - val_loss: 0.4616\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 465us/step - loss: 0.1786 - val_loss: 0.6147\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 361us/step - loss: 0.1714 - val_loss: 0.7979\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 370us/step - loss: 0.1560 - val_loss: 0.5726\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 361us/step - loss: 0.1447 - val_loss: 0.7336\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 418us/step - loss: 0.1296 - val_loss: 0.7132\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 598us/step - loss: 0.1430 - val_loss: 0.8106\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - 0s 399us/step - loss: 0.1164 - val_loss: 0.8598\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 418us/step - loss: 0.1332 - val_loss: 0.8083\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - 0s 389us/step - loss: 0.1247 - val_loss: 0.8355\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 475us/step - loss: 0.1126 - val_loss: 0.9054\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.117 - 0s 655us/step - loss: 0.1096 - val_loss: 0.9324\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - 0s 409us/step - loss: 0.1042 - val_loss: 1.0096\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - 0s 418us/step - loss: 0.1024 - val_loss: 0.9676\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - 0s 389us/step - loss: 0.0874 - val_loss: 0.9936\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - 0s 437us/step - loss: 0.0853 - val_loss: 1.0490\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - 0s 408us/step - loss: 0.0918 - val_loss: 1.0337\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 418us/step - loss: 0.0830 - val_loss: 1.0186\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 361us/step - loss: 0.0664 - val_loss: 1.2999\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - 0s 351us/step - loss: 0.0766 - val_loss: 1.1055\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 380us/step - loss: 0.0781 - val_loss: 1.0447\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - 0s 446us/step - loss: 0.0762 - val_loss: 1.2790\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 361us/step - loss: 0.0675 - val_loss: 1.0578\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 370us/step - loss: 0.0678 - val_loss: 0.9951\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - 0s 342us/step - loss: 0.0604 - val_loss: 1.2333\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - 0s 351us/step - loss: 0.0615 - val_loss: 1.0361\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 380us/step - loss: 0.0620 - val_loss: 0.9978\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 475us/step - loss: 0.0631 - val_loss: 1.0980\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 399us/step - loss: 0.0753 - val_loss: 1.2405\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 399us/step - loss: 0.0606 - val_loss: 1.0671\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - 0s 475us/step - loss: 0.0560 - val_loss: 1.0887\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 389us/step - loss: 0.0630 - val_loss: 1.3065\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 399us/step - loss: 0.0524 - val_loss: 1.0917\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 361us/step - loss: 0.0479 - val_loss: 1.2478\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 380us/step - loss: 0.0500 - val_loss: 1.0145\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 380us/step - loss: 0.0489 - val_loss: 1.2168\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 25\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.175 - 1s 7ms/step - loss: 0.8674 - val_loss: 0.3781\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - 0s 342us/step - loss: 0.6288 - val_loss: 0.4967\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.575 - 0s 352us/step - loss: 0.5712 - val_loss: 0.3965\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 351us/step - loss: 0.4926 - val_loss: 0.4848\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 361us/step - loss: 0.4360 - val_loss: 0.5963\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.292 - 0s 351us/step - loss: 0.3678 - val_loss: 0.5635\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 1.446 - 0s 342us/step - loss: 0.3444 - val_loss: 0.7252\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - 0s 361us/step - loss: 0.3091 - val_loss: 0.6443\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 437us/step - loss: 0.2762 - val_loss: 0.6210\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - 0s 418us/step - loss: 0.2566 - val_loss: 0.5127\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - 0s 427us/step - loss: 0.2355 - val_loss: 0.6862\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - 0s 389us/step - loss: 0.2298 - val_loss: 0.7206\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 427us/step - loss: 0.1764 - val_loss: 0.7162\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 389us/step - loss: 0.1857 - val_loss: 0.7442\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 352us/step - loss: 0.1671 - val_loss: 0.6434\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - 0s 351us/step - loss: 0.1460 - val_loss: 0.6471\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.137 - 0s 874us/step - loss: 0.1276 - val_loss: 0.8599\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 408us/step - loss: 0.1368 - val_loss: 0.7773\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - 0s 370us/step - loss: 0.1430 - val_loss: 0.8786\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 389us/step - loss: 0.1247 - val_loss: 0.8648\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 418us/step - loss: 0.1242 - val_loss: 0.7008\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - 0s 361us/step - loss: 0.1080 - val_loss: 0.8721\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 323us/step - loss: 0.0971 - val_loss: 1.0099\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - 0s 323us/step - loss: 0.0986 - val_loss: 0.8180\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - 0s 332us/step - loss: 0.0952 - val_loss: 1.1876\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - 0s 446us/step - loss: 0.0803 - val_loss: 0.9791\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 342us/step - loss: 0.1050 - val_loss: 0.9814\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 332us/step - loss: 0.0820 - val_loss: 0.9534\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 342us/step - loss: 0.0737 - val_loss: 0.7816\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 332us/step - loss: 0.0638 - val_loss: 0.8389\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 333us/step - loss: 0.0683 - val_loss: 0.8569\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - 0s 351us/step - loss: 0.0621 - val_loss: 0.8722\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 351us/step - loss: 0.0665 - val_loss: 0.8678\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 361us/step - loss: 0.0627 - val_loss: 0.7741\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 342us/step - loss: 0.0670 - val_loss: 0.7863\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 342us/step - loss: 0.0611 - val_loss: 0.7824\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 351us/step - loss: 0.0506 - val_loss: 0.7372\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 399us/step - loss: 0.0524 - val_loss: 0.9065\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 371us/step - loss: 0.0474 - val_loss: 0.8178\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.046 - 0s 693us/step - loss: 0.0477 - val_loss: 0.8167\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.042 - 0s 788us/step - loss: 0.0390 - val_loss: 0.8372\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - 0s 541us/step - loss: 0.0514 - val_loss: 0.7387\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 646us/step - loss: 0.0423 - val_loss: 0.9115\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.056 - 0s 712us/step - loss: 0.0519 - val_loss: 0.9228\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 456us/step - loss: 0.0484 - val_loss: 0.7924\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - 0s 503us/step - loss: 0.0442 - val_loss: 0.9422\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - 0s 503us/step - loss: 0.0366 - val_loss: 0.7617\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - 0s 494us/step - loss: 0.0390 - val_loss: 0.8253\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 494us/step - loss: 0.0313 - val_loss: 0.8494\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 389us/step - loss: 0.0355 - val_loss: 0.8661\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 361us/step - loss: 0.0338 - val_loss: 0.8859\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 26\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 1.936 - 1s 7ms/step - loss: 0.7938 - val_loss: 0.5932\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 342us/step - loss: 0.4590 - val_loss: 1.4214\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.601 - 0s 361us/step - loss: 0.4168 - val_loss: 1.3977\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - 0s 399us/step - loss: 0.3823 - val_loss: 1.3638\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - 0s 437us/step - loss: 0.3433 - val_loss: 1.5004\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - 0s 342us/step - loss: 0.3350 - val_loss: 1.5792\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.321 - 0s 399us/step - loss: 0.3167 - val_loss: 1.5454\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.454 - 0s 361us/step - loss: 0.3005 - val_loss: 1.5215\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 437us/step - loss: 0.2595 - val_loss: 1.7765\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.278 - 0s 475us/step - loss: 0.2581 - val_loss: 1.6426\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - 0s 352us/step - loss: 0.2473 - val_loss: 1.6211\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 351us/step - loss: 0.2343 - val_loss: 1.5897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.420 - 0s 370us/step - loss: 0.2303 - val_loss: 1.7618\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - 0s 370us/step - loss: 0.2140 - val_loss: 2.0527\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.338 - 0s 351us/step - loss: 0.2077 - val_loss: 1.9035\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.519 - 0s 361us/step - loss: 0.1899 - val_loss: 2.1116\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - 0s 351us/step - loss: 0.1815 - val_loss: 1.9297\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - 0s 351us/step - loss: 0.1711 - val_loss: 1.7963\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 332us/step - loss: 0.1604 - val_loss: 2.0330\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.365 - 0s 352us/step - loss: 0.1628 - val_loss: 2.0230\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 361us/step - loss: 0.1598 - val_loss: 1.9245\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - 0s 342us/step - loss: 0.1367 - val_loss: 1.8936\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - 0s 361us/step - loss: 0.1402 - val_loss: 2.5899\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - 0s 342us/step - loss: 0.1390 - val_loss: 2.1238\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - 0s 370us/step - loss: 0.1296 - val_loss: 1.8895\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 361us/step - loss: 0.1165 - val_loss: 2.2930\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - 0s 446us/step - loss: 0.1150 - val_loss: 2.4154\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.093 - 0s 627us/step - loss: 0.1101 - val_loss: 2.1202\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 323us/step - loss: 0.0984 - val_loss: 2.0130\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - 0s 456us/step - loss: 0.0921 - val_loss: 2.2866\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - 0s 399us/step - loss: 0.0961 - val_loss: 2.2790\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - 0s 361us/step - loss: 0.0980 - val_loss: 2.3184\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - 0s 370us/step - loss: 0.0730 - val_loss: 2.3362\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 361us/step - loss: 0.0873 - val_loss: 2.6021\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 361us/step - loss: 0.0911 - val_loss: 2.7207\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 351us/step - loss: 0.0694 - val_loss: 2.6402\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 399us/step - loss: 0.0735 - val_loss: 2.4073\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 399us/step - loss: 0.0697 - val_loss: 2.5932\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 399us/step - loss: 0.0674 - val_loss: 2.5820\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 399us/step - loss: 0.0815 - val_loss: 2.4670\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - 0s 380us/step - loss: 0.0686 - val_loss: 2.8729\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 351us/step - loss: 0.0631 - val_loss: 2.4413\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - 0s 456us/step - loss: 0.0593 - val_loss: 2.7081\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - 0s 342us/step - loss: 0.0640 - val_loss: 2.2935\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 351us/step - loss: 0.0653 - val_loss: 2.6086\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 332us/step - loss: 0.0640 - val_loss: 2.4740\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 361us/step - loss: 0.0472 - val_loss: 2.4711\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 361us/step - loss: 0.0589 - val_loss: 2.2354\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 437us/step - loss: 0.0752 - val_loss: 3.1998\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 351us/step - loss: 0.0571 - val_loss: 2.0264\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 342us/step - loss: 0.0568 - val_loss: 2.5298\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 27\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 11s - loss: 0.65 - ETA: 2s - loss: 0.8642 - ETA: 0s - loss: 0.718 - 1s 11ms/step - loss: 0.7558 - val_loss: 0.5622\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.726 - ETA: 0s - loss: 0.598 - 0s 845us/step - loss: 0.5516 - val_loss: 0.8705\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.347 - 0s 1ms/step - loss: 0.4964 - val_loss: 0.5341\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.357 - 0s 1ms/step - loss: 0.4672 - val_loss: 0.4410\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.528 - ETA: 0s - loss: 0.403 - 0s 950us/step - loss: 0.4216 - val_loss: 0.7080\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.388 - 0s 342us/step - loss: 0.3684 - val_loss: 0.5319\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - 0s 323us/step - loss: 0.3670 - val_loss: 0.5480\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - 0s 342us/step - loss: 0.3414 - val_loss: 0.6588\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - 0s 370us/step - loss: 0.3230 - val_loss: 0.5169\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - 0s 361us/step - loss: 0.3196 - val_loss: 0.4683\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - 0s 389us/step - loss: 0.3115 - val_loss: 0.7888\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.308 - 0s 779us/step - loss: 0.3070 - val_loss: 0.4703\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.269 - 0s 1ms/step - loss: 0.2685 - val_loss: 0.4991\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.259 - 0s 2ms/step - loss: 0.2570 - val_loss: 0.4831\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.251 - 0s 1ms/step - loss: 0.2497 - val_loss: 0.4794\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - 0s 418us/step - loss: 0.2506 - val_loss: 0.5283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - 0s 371us/step - loss: 0.2453 - val_loss: 0.3903\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 475us/step - loss: 0.2149 - val_loss: 0.4855\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - 0s 570us/step - loss: 0.2133 - val_loss: 0.5299\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - 0s 380us/step - loss: 0.2103 - val_loss: 0.5187\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - 0s 370us/step - loss: 0.2133 - val_loss: 0.6813\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - 0s 333us/step - loss: 0.1979 - val_loss: 0.5806\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 342us/step - loss: 0.1867 - val_loss: 0.5285\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.234 - 0s 845us/step - loss: 0.1923 - val_loss: 0.5590\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - 0s 418us/step - loss: 0.1742 - val_loss: 0.6557\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.157 - 0s 1ms/step - loss: 0.1587 - val_loss: 0.7141\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.166 - 0s 893us/step - loss: 0.1653 - val_loss: 0.6873\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.194 - 0s 922us/step - loss: 0.1648 - val_loss: 0.7005\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 466us/step - loss: 0.1533 - val_loss: 0.6621\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.133 - 0s 2ms/step - loss: 0.1383 - val_loss: 0.6896\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.152 - 0s 912us/step - loss: 0.1485 - val_loss: 0.7461\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.142 - 0s 779us/step - loss: 0.1422 - val_loss: 0.6453\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.171 - 0s 883us/step - loss: 0.1425 - val_loss: 0.7437\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.134 - 0s 617us/step - loss: 0.1359 - val_loss: 0.7517\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - 0s 551us/step - loss: 0.1307 - val_loss: 0.7710\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - 0s 627us/step - loss: 0.1367 - val_loss: 0.8254\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.132 - 0s 845us/step - loss: 0.1324 - val_loss: 0.8177\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 598us/step - loss: 0.1259 - val_loss: 0.7731\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.109 - 0s 684us/step - loss: 0.1083 - val_loss: 0.8783\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.298 - 0s 608us/step - loss: 0.1095 - val_loss: 1.0062\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 475us/step - loss: 0.1166 - val_loss: 0.8353\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.108 - 0s 1ms/step - loss: 0.1038 - val_loss: 0.8444\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - 0s 361us/step - loss: 0.1052 - val_loss: 0.9272\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 399us/step - loss: 0.1040 - val_loss: 0.9483\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 513us/step - loss: 0.1096 - val_loss: 0.9494\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 361us/step - loss: 0.1034 - val_loss: 0.9470\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 370us/step - loss: 0.0976 - val_loss: 0.9075\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 361us/step - loss: 0.0885 - val_loss: 0.8943\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - 0s 390us/step - loss: 0.0885 - val_loss: 1.0874\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 380us/step - loss: 0.0988 - val_loss: 0.7767\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 370us/step - loss: 0.0937 - val_loss: 1.0825\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 380us/step - loss: 0.0961 - val_loss: 0.9486\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 370us/step - loss: 0.0914 - val_loss: 1.0602\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.081 - 0s 703us/step - loss: 0.0849 - val_loss: 0.9260\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 361us/step - loss: 0.0829 - val_loss: 0.8718\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 371us/step - loss: 0.0627 - val_loss: 1.0408\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - 0s 370us/step - loss: 0.0768 - val_loss: 0.9062\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 361us/step - loss: 0.0701 - val_loss: 0.9131\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 370us/step - loss: 0.0596 - val_loss: 0.9948\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 370us/step - loss: 0.0696 - val_loss: 0.9575\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - 0s 370us/step - loss: 0.0701 - val_loss: 0.8810\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 370us/step - loss: 0.0636 - val_loss: 0.9271\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 399us/step - loss: 0.0732 - val_loss: 0.8335\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 399us/step - loss: 0.0649 - val_loss: 1.0975\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 370us/step - loss: 0.0675 - val_loss: 0.8232\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.305 - 0s 352us/step - loss: 0.0799 - val_loss: 0.9485\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - 0s 342us/step - loss: 0.0806 - val_loss: 0.8639\n",
      "Epoch 00067: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 28\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 8s - loss: 1.034 - 1s 8ms/step - loss: 0.7137 - val_loss: 0.8539\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - 0s 437us/step - loss: 0.3957 - val_loss: 1.3362\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - 0s 399us/step - loss: 0.3722 - val_loss: 1.6675\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.581 - 0s 399us/step - loss: 0.3253 - val_loss: 1.3748\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - 0s 342us/step - loss: 0.3121 - val_loss: 1.2552\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 361us/step - loss: 0.3078 - val_loss: 1.8876\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.484 - 0s 380us/step - loss: 0.2762 - val_loss: 1.4659\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - 0s 380us/step - loss: 0.1931 - val_loss: 1.3497\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - 0s 380us/step - loss: 0.2085 - val_loss: 1.6285\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - 0s 494us/step - loss: 0.1944 - val_loss: 1.6331\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - 0s 399us/step - loss: 0.1927 - val_loss: 1.7615\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 351us/step - loss: 0.1896 - val_loss: 1.2516\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.424 - 0s 351us/step - loss: 0.1586 - val_loss: 1.8635\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 342us/step - loss: 0.1481 - val_loss: 1.9212\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - 0s 342us/step - loss: 0.1450 - val_loss: 1.5128\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 399us/step - loss: 0.1212 - val_loss: 1.7795\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - 0s 380us/step - loss: 0.1320 - val_loss: 1.8962\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 342us/step - loss: 0.1245 - val_loss: 1.7848\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - 0s 351us/step - loss: 0.1221 - val_loss: 1.7602\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - 0s 332us/step - loss: 0.1211 - val_loss: 1.8489\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 342us/step - loss: 0.1225 - val_loss: 1.4583\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 352us/step - loss: 0.1163 - val_loss: 1.4741\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 351us/step - loss: 0.1046 - val_loss: 1.7174\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - 0s 342us/step - loss: 0.0965 - val_loss: 1.9277\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 361us/step - loss: 0.0911 - val_loss: 1.8066\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 351us/step - loss: 0.0971 - val_loss: 1.6973\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 313us/step - loss: 0.0962 - val_loss: 1.7418\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 332us/step - loss: 0.0961 - val_loss: 1.5653\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 351us/step - loss: 0.0888 - val_loss: 2.0267\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 332us/step - loss: 0.0929 - val_loss: 1.5678\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - 0s 351us/step - loss: 0.0798 - val_loss: 1.7782\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 332us/step - loss: 0.0877 - val_loss: 1.7337\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 361us/step - loss: 0.0770 - val_loss: 1.5449\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - 0s 361us/step - loss: 0.0778 - val_loss: 1.7991\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - 0s 370us/step - loss: 0.0768 - val_loss: 1.8100\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 351us/step - loss: 0.0697 - val_loss: 1.7497\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - 0s 351us/step - loss: 0.0701 - val_loss: 1.8009\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 361us/step - loss: 0.0656 - val_loss: 1.7841\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - 0s 428us/step - loss: 0.0719 - val_loss: 1.7802\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 484us/step - loss: 0.0633 - val_loss: 1.6561\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - 0s 380us/step - loss: 0.0892 - val_loss: 1.8272\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 380us/step - loss: 0.0635 - val_loss: 1.6694\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 465us/step - loss: 0.0654 - val_loss: 1.6968\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - 0s 427us/step - loss: 0.0658 - val_loss: 1.8647\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - 0s 446us/step - loss: 0.0665 - val_loss: 1.6518\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 456us/step - loss: 0.0671 - val_loss: 1.7087\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 503us/step - loss: 0.0607 - val_loss: 1.5787\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 465us/step - loss: 0.0574 - val_loss: 1.7576\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 437us/step - loss: 0.0537 - val_loss: 1.7956\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 456us/step - loss: 0.0590 - val_loss: 1.6827\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 428us/step - loss: 0.0557 - val_loss: 1.6999\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 29\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 8s - loss: 0.951 - 1s 8ms/step - loss: 0.6502 - val_loss: 0.5424\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.473 - 0s 399us/step - loss: 0.4320 - val_loss: 1.2305\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.384 - 0s 389us/step - loss: 0.3952 - val_loss: 1.6201\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - 0s 399us/step - loss: 0.3325 - val_loss: 1.2797\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - 0s 370us/step - loss: 0.2927 - val_loss: 1.0160\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - 0s 361us/step - loss: 0.3069 - val_loss: 0.9564\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - 0s 361us/step - loss: 0.2789 - val_loss: 1.2072\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.249 - 0s 1ms/step - loss: 0.2484 - val_loss: 1.4155\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - 0s 665us/step - loss: 0.2397 - val_loss: 0.9652\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - 0s 475us/step - loss: 0.2432 - val_loss: 1.0587\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - 0s 408us/step - loss: 0.2104 - val_loss: 1.1438\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - 0s 484us/step - loss: 0.2050 - val_loss: 1.3198\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 589us/step - loss: 0.1895 - val_loss: 1.2920\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - 0s 551us/step - loss: 0.1972 - val_loss: 1.4877\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.184 - 0s 655us/step - loss: 0.1842 - val_loss: 1.4664\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.196 - 0s 788us/step - loss: 0.1761 - val_loss: 1.5483\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 408us/step - loss: 0.1607 - val_loss: 1.4058\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.145 - 0s 699us/step - loss: 0.1459 - val_loss: 1.4880\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - 0s 579us/step - loss: 0.1403 - val_loss: 1.6439\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 532us/step - loss: 0.1490 - val_loss: 1.7600\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.146 - 0s 684us/step - loss: 0.1475 - val_loss: 1.6866\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - 0s 468us/step - loss: 0.1539 - val_loss: 1.7744\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 437us/step - loss: 0.1276 - val_loss: 1.7249\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - 0s 399us/step - loss: 0.1216 - val_loss: 1.8182\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.416 - 0s 475us/step - loss: 0.1183 - val_loss: 1.7687\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 465us/step - loss: 0.1089 - val_loss: 1.9807\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 522us/step - loss: 0.1093 - val_loss: 1.9045\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 427us/step - loss: 0.1091 - val_loss: 1.7380\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - 0s 494us/step - loss: 0.1009 - val_loss: 1.8574\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 560us/step - loss: 0.1020 - val_loss: 2.0147\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 570us/step - loss: 0.0930 - val_loss: 1.9203\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - 0s 522us/step - loss: 0.0931 - val_loss: 1.8690\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.087 - 0s 712us/step - loss: 0.0871 - val_loss: 2.1013\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - 0s 465us/step - loss: 0.0878 - val_loss: 1.8848\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 446us/step - loss: 0.0815 - val_loss: 1.9182\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 484us/step - loss: 0.0921 - val_loss: 1.9931\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - 0s 551us/step - loss: 0.0821 - val_loss: 1.8210\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 541us/step - loss: 0.0833 - val_loss: 1.7040\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.080 - 0s 731us/step - loss: 0.0800 - val_loss: 1.6740\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.088 - 0s 665us/step - loss: 0.0888 - val_loss: 1.7880\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 503us/step - loss: 0.0767 - val_loss: 1.7698\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.073 - 0s 693us/step - loss: 0.0751 - val_loss: 1.8437\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 494us/step - loss: 0.0658 - val_loss: 1.6625\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.078 - 0s 632us/step - loss: 0.0784 - val_loss: 1.8973\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - 0s 604us/step - loss: 0.0638 - val_loss: 2.0051\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 522us/step - loss: 0.0704 - val_loss: 1.7549\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 480us/step - loss: 0.0676 - val_loss: 1.6721\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 560us/step - loss: 0.0606 - val_loss: 1.7569\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 465us/step - loss: 0.0630 - val_loss: 1.7855\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 522us/step - loss: 0.0625 - val_loss: 1.7140\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 560us/step - loss: 0.0597 - val_loss: 1.6690\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 30\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.827 - 1s 7ms/step - loss: 0.6965 - val_loss: 0.5478\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.628 - ETA: 0s - loss: 0.423 - 0s 665us/step - loss: 0.4197 - val_loss: 1.0854\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.433 - ETA: 0s - loss: 0.425 - 0s 969us/step - loss: 0.4097 - val_loss: 0.9625\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.780 - ETA: 0s - loss: 0.368 - 0s 722us/step - loss: 0.3653 - val_loss: 0.5219\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.380 - ETA: 0s - loss: 0.317 - 0s 1ms/step - loss: 0.3216 - val_loss: 0.6212\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - 0s 475us/step - loss: 0.3330 - val_loss: 0.7484\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.410 - 0s 399us/step - loss: 0.2844 - val_loss: 0.4915\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 465us/step - loss: 0.2616 - val_loss: 0.4807\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - 0s 399us/step - loss: 0.2395 - val_loss: 0.4461\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.815 - 0s 418us/step - loss: 0.2242 - val_loss: 0.3989\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 465us/step - loss: 0.2025 - val_loss: 0.4107\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.177 - 0s 798us/step - loss: 0.1952 - val_loss: 0.4464\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.212 - 0s 969us/step - loss: 0.1916 - val_loss: 0.3951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.554 - 0s 541us/step - loss: 0.1752 - val_loss: 0.4315\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 475us/step - loss: 0.1846 - val_loss: 0.4337\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.183 - 0s 902us/step - loss: 0.1769 - val_loss: 0.4251\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 484us/step - loss: 0.1591 - val_loss: 0.4913\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - 0s 471us/step - loss: 0.1555 - val_loss: 0.4470\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 494us/step - loss: 0.1547 - val_loss: 0.5372\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.377 - 0s 556us/step - loss: 0.1497 - val_loss: 0.5605\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.449 - 0s 437us/step - loss: 0.1472 - val_loss: 0.4960\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 475us/step - loss: 0.1352 - val_loss: 0.6020\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - 0s 446us/step - loss: 0.1374 - val_loss: 0.6224\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - 0s 446us/step - loss: 0.1296 - val_loss: 0.5819\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - 0s 418us/step - loss: 0.1281 - val_loss: 0.5771\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.124 - 0s 1ms/step - loss: 0.1236 - val_loss: 0.5868\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.131 - 0s 779us/step - loss: 0.1283 - val_loss: 0.6545\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - 0s 408us/step - loss: 0.1191 - val_loss: 0.6473\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.122 - 0s 836us/step - loss: 0.1201 - val_loss: 0.6956\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - 0s 465us/step - loss: 0.1076 - val_loss: 0.7066\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 475us/step - loss: 0.1122 - val_loss: 0.7151\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - 0s 418us/step - loss: 0.1204 - val_loss: 0.7186\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 399us/step - loss: 0.1090 - val_loss: 0.7051\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 475us/step - loss: 0.1070 - val_loss: 0.7793\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - 0s 408us/step - loss: 0.1014 - val_loss: 0.8042\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 503us/step - loss: 0.0925 - val_loss: 0.7850\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 437us/step - loss: 0.1053 - val_loss: 0.7508\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 484us/step - loss: 0.0880 - val_loss: 0.7136\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - 0s 532us/step - loss: 0.0924 - val_loss: 0.8358\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - 0s 380us/step - loss: 0.0918 - val_loss: 0.8090\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - 0s 437us/step - loss: 0.0912 - val_loss: 0.8461\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 437us/step - loss: 0.0854 - val_loss: 0.8455\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 380us/step - loss: 0.0981 - val_loss: 0.8535\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 493us/step - loss: 0.0856 - val_loss: 0.8331\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.095 - 0s 599us/step - loss: 0.0951 - val_loss: 0.8363\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.081 - 0s 1ms/step - loss: 0.0811 - val_loss: 0.8447\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.090 - 0s 1ms/step - loss: 0.0793 - val_loss: 0.8329\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.091 - 0s 1ms/step - loss: 0.0874 - val_loss: 0.8773\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.080 - 0s 1ms/step - loss: 0.0760 - val_loss: 0.9270\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.083 - 0s 750us/step - loss: 0.0823 - val_loss: 0.9072\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.085 - 0s 722us/step - loss: 0.0855 - val_loss: 0.9165\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.077 - 0s 921us/step - loss: 0.0736 - val_loss: 0.9322\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.069 - 0s 703us/step - loss: 0.0691 - val_loss: 0.9353\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 570us/step - loss: 0.0738 - val_loss: 0.9574\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.064 - 0s 807us/step - loss: 0.0687 - val_loss: 0.9688\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 596us/step - loss: 0.0681 - val_loss: 0.9048\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 513us/step - loss: 0.0706 - val_loss: 1.0168\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.065 - 0s 1ms/step - loss: 0.0639 - val_loss: 0.9375\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - 0s 396us/step - loss: 0.0714 - val_loss: 0.8870\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 554us/step - loss: 0.0571 - val_loss: 0.9714\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.068 - 0s 731us/step - loss: 0.0672 - val_loss: 0.9434\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 627us/step - loss: 0.0496 - val_loss: 0.9363\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.047 - 0s 646us/step - loss: 0.0563 - val_loss: 0.9948\n",
      "Epoch 00063: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 31\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 8s - loss: 0.991 - 1s 8ms/step - loss: 0.6701 - val_loss: 0.5417\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - 0s 427us/step - loss: 0.4935 - val_loss: 0.9273\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - 0s 389us/step - loss: 0.4385 - val_loss: 0.8195\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.786 - 0s 399us/step - loss: 0.4193 - val_loss: 0.9247\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - 0s 513us/step - loss: 0.3780 - val_loss: 1.0406\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - 0s 389us/step - loss: 0.3377 - val_loss: 1.0576\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.398 - 0s 399us/step - loss: 0.3348 - val_loss: 1.0674\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.524 - 0s 437us/step - loss: 0.2824 - val_loss: 0.8974\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - 0s 370us/step - loss: 0.2677 - val_loss: 0.8850\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - 0s 361us/step - loss: 0.2561 - val_loss: 0.9669\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - 0s 408us/step - loss: 0.2211 - val_loss: 0.6145\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - 0s 456us/step - loss: 0.2384 - val_loss: 0.7082\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - 0s 361us/step - loss: 0.2167 - val_loss: 0.8854\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - 0s 437us/step - loss: 0.1994 - val_loss: 1.2079\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - 0s 636us/step - loss: 0.1872 - val_loss: 1.0618\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - 0s 389us/step - loss: 0.1854 - val_loss: 1.0543\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - 0s 370us/step - loss: 0.1563 - val_loss: 0.9871\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - 0s 472us/step - loss: 0.1568 - val_loss: 0.9084\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - 0s 418us/step - loss: 0.1505 - val_loss: 1.0657\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - 0s 432us/step - loss: 0.1450 - val_loss: 0.8884\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 608us/step - loss: 0.1455 - val_loss: 0.8839\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - 0s 399us/step - loss: 0.1258 - val_loss: 1.0414\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.134 - 0s 674us/step - loss: 0.1305 - val_loss: 0.9875\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - 0s 408us/step - loss: 0.1201 - val_loss: 1.0049\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - 0s 380us/step - loss: 0.1193 - val_loss: 1.0529\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - 0s 408us/step - loss: 0.1220 - val_loss: 1.1490\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 503us/step - loss: 0.1043 - val_loss: 0.9190\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 446us/step - loss: 0.0943 - val_loss: 1.0228\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 541us/step - loss: 0.0881 - val_loss: 1.1740\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 418us/step - loss: 0.0861 - val_loss: 1.2704\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - 0s 399us/step - loss: 0.0910 - val_loss: 1.1918\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 408us/step - loss: 0.0883 - val_loss: 1.1440\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 627us/step - loss: 0.0744 - val_loss: 1.1945\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.071 - 0s 855us/step - loss: 0.0712 - val_loss: 1.1699\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.076 - 0s 2ms/step - loss: 0.0808 - val_loss: 1.0620\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.068 - 0s 703us/step - loss: 0.0781 - val_loss: 1.0052\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.063 - 0s 756us/step - loss: 0.0765 - val_loss: 1.2254\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - 0s 560us/step - loss: 0.0612 - val_loss: 1.0951\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.073 - 0s 1ms/step - loss: 0.0727 - val_loss: 1.3581\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 518us/step - loss: 0.0673 - val_loss: 1.1719\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.061 - 0s 2ms/step - loss: 0.0559 - val_loss: 1.2486\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 2ms/step - loss: 0.0615 - val_loss: 1.2488\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.051 - 0s 2ms/step - loss: 0.0537 - val_loss: 1.0262\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.050 - 0s 627us/step - loss: 0.0504 - val_loss: 1.3273\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.053 - 0s 608us/step - loss: 0.0532 - val_loss: 1.2900\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 541us/step - loss: 0.0441 - val_loss: 1.2446\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - 0s 456us/step - loss: 0.0496 - val_loss: 1.0931\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 475us/step - loss: 0.0535 - val_loss: 1.2888\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - 0s 408us/step - loss: 0.0506 - val_loss: 1.2015\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.050 - 0s 831us/step - loss: 0.0455 - val_loss: 1.1919\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 423us/step - loss: 0.0446 - val_loss: 1.2299\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 32\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 4.240 - 1s 8ms/step - loss: 0.7542 - val_loss: 0.7604\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.764 - 0s 361us/step - loss: 0.5235 - val_loss: 1.1938\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - 0s 437us/step - loss: 0.4466 - val_loss: 1.0784\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - 0s 390us/step - loss: 0.4073 - val_loss: 1.0042\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - 0s 361us/step - loss: 0.3731 - val_loss: 1.1299\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.381 - 0s 427us/step - loss: 0.3279 - val_loss: 0.9586\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - 0s 484us/step - loss: 0.3239 - val_loss: 0.6724\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 342us/step - loss: 0.2649 - val_loss: 0.9526\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 418us/step - loss: 0.2382 - val_loss: 0.8107\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - 0s 361us/step - loss: 0.2430 - val_loss: 0.6159\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 380us/step - loss: 0.2114 - val_loss: 0.8456\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.380 - 0s 446us/step - loss: 0.1910 - val_loss: 0.9012\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - 0s 390us/step - loss: 0.1775 - val_loss: 0.8453\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 389us/step - loss: 0.1667 - val_loss: 0.8283\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - 0s 361us/step - loss: 0.1702 - val_loss: 0.7087\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - 0s 433us/step - loss: 0.1434 - val_loss: 0.8992\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.142 - 0s 814us/step - loss: 0.1270 - val_loss: 0.9400\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 361us/step - loss: 0.1329 - val_loss: 1.0007\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - 0s 397us/step - loss: 0.1515 - val_loss: 1.0037\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 485us/step - loss: 0.1050 - val_loss: 0.7483\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.097 - 0s 693us/step - loss: 0.0967 - val_loss: 0.9275\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.094 - 0s 693us/step - loss: 0.0948 - val_loss: 0.7963\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - 0s 380us/step - loss: 0.1019 - val_loss: 0.7446\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.285 - 0s 399us/step - loss: 0.1279 - val_loss: 1.0224\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - 0s 541us/step - loss: 0.1056 - val_loss: 0.6576\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 380us/step - loss: 0.0785 - val_loss: 0.8040\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - 0s 371us/step - loss: 0.0734 - val_loss: 0.9118\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 380us/step - loss: 0.0719 - val_loss: 0.7803\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 484us/step - loss: 0.0598 - val_loss: 0.9026\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 390us/step - loss: 0.0671 - val_loss: 0.7956\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 399us/step - loss: 0.0597 - val_loss: 0.8851\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - 0s 370us/step - loss: 0.0689 - val_loss: 0.7438\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 399us/step - loss: 0.0619 - val_loss: 0.8033\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 361us/step - loss: 0.0585 - val_loss: 0.8248\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 380us/step - loss: 0.0548 - val_loss: 0.9462\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - 0s 447us/step - loss: 0.0573 - val_loss: 0.9169\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - 0s 513us/step - loss: 0.0475 - val_loss: 0.7959\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 408us/step - loss: 0.0509 - val_loss: 0.9654\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 389us/step - loss: 0.0495 - val_loss: 0.9247\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 361us/step - loss: 0.0448 - val_loss: 0.9556\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - 0s 494us/step - loss: 0.0486 - val_loss: 1.1886\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - 0s 370us/step - loss: 0.0521 - val_loss: 0.8688\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - 0s 380us/step - loss: 0.0487 - val_loss: 0.8256\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 361us/step - loss: 0.0416 - val_loss: 1.0350\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - 0s 361us/step - loss: 0.0370 - val_loss: 0.9770\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 351us/step - loss: 0.0364 - val_loss: 0.8651\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - 0s 446us/step - loss: 0.0413 - val_loss: 1.0111\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - 0s 399us/step - loss: 0.0393 - val_loss: 1.0015\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 380us/step - loss: 0.0420 - val_loss: 0.9209\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.038 - 0s 1ms/step - loss: 0.0385 - val_loss: 0.7973\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 522us/step - loss: 0.0417 - val_loss: 0.9187\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - 0s 408us/step - loss: 0.0351 - val_loss: 0.8824\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 380us/step - loss: 0.0391 - val_loss: 0.8426\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - 0s 399us/step - loss: 0.0305 - val_loss: 0.9285\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.032 - 0s 608us/step - loss: 0.0329 - val_loss: 0.9746\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 456us/step - loss: 0.0407 - val_loss: 0.7270\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 380us/step - loss: 0.0405 - val_loss: 1.0135\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - 0s 390us/step - loss: 0.0372 - val_loss: 1.0115\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - 0s 380us/step - loss: 0.0291 - val_loss: 0.9362\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 399us/step - loss: 0.0394 - val_loss: 0.7363\n",
      "Epoch 00060: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 33\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.212 - 1s 7ms/step - loss: 0.9438 - val_loss: 0.2439\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 332us/step - loss: 0.8646 - val_loss: 0.2518\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - 0s 370us/step - loss: 0.8088 - val_loss: 0.1951\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.611 - 0s 333us/step - loss: 0.7652 - val_loss: 0.2657\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.379 - 0s 370us/step - loss: 0.7259 - val_loss: 0.3063\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 352us/step - loss: 0.6843 - val_loss: 0.3730\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 361us/step - loss: 0.6475 - val_loss: 0.3965\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - 0s 370us/step - loss: 0.6147 - val_loss: 0.5341\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 646us/step - loss: 0.5595 - val_loss: 0.5535\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 361us/step - loss: 0.5019 - val_loss: 0.7819\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - 0s 447us/step - loss: 0.4829 - val_loss: 0.9376\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - 0s 380us/step - loss: 0.4988 - val_loss: 0.9269\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - 0s 627us/step - loss: 0.4078 - val_loss: 1.0975\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.347 - 0s 874us/step - loss: 0.3445 - val_loss: 1.3297\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 532us/step - loss: 0.3439 - val_loss: 1.4858\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - 0s 332us/step - loss: 0.3303 - val_loss: 1.4941\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 408us/step - loss: 0.3039 - val_loss: 1.8346\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 333us/step - loss: 0.2719 - val_loss: 1.9013\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - 0s 389us/step - loss: 0.2452 - val_loss: 2.0406\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - 0s 437us/step - loss: 0.2089 - val_loss: 2.1918\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 446us/step - loss: 0.1909 - val_loss: 2.3494\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - 0s 370us/step - loss: 0.1469 - val_loss: 2.3788\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 361us/step - loss: 0.1511 - val_loss: 2.7598\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - 0s 390us/step - loss: 0.1421 - val_loss: 2.6364\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - 0s 361us/step - loss: 0.1384 - val_loss: 2.8516\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - 0s 389us/step - loss: 0.1153 - val_loss: 2.9137\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - 0s 380us/step - loss: 0.1487 - val_loss: 3.1299\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 380us/step - loss: 0.1020 - val_loss: 3.3176\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - 0s 380us/step - loss: 0.1160 - val_loss: 3.1490\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - 0s 361us/step - loss: 0.0919 - val_loss: 3.4934\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - 0s 380us/step - loss: 0.0858 - val_loss: 3.3942\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 389us/step - loss: 0.0896 - val_loss: 3.4278\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - 0s 361us/step - loss: 0.0832 - val_loss: 3.4663\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 399us/step - loss: 0.0694 - val_loss: 3.6350\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 370us/step - loss: 0.0803 - val_loss: 3.7048\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - 0s 389us/step - loss: 0.0747 - val_loss: 3.7943\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - 0s 370us/step - loss: 0.0664 - val_loss: 3.7879\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 389us/step - loss: 0.0467 - val_loss: 3.8136\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 370us/step - loss: 0.0606 - val_loss: 3.9105\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - 0s 370us/step - loss: 0.0443 - val_loss: 4.1045\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 361us/step - loss: 0.0747 - val_loss: 3.7321\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 399us/step - loss: 0.1085 - val_loss: 4.1607\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 437us/step - loss: 0.0582 - val_loss: 4.1057\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - 0s 446us/step - loss: 0.0534 - val_loss: 3.8889\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - 0s 399us/step - loss: 0.0622 - val_loss: 3.8426\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - 0s 389us/step - loss: 0.0510 - val_loss: 4.0134\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 399us/step - loss: 0.0474 - val_loss: 3.9245\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - 0s 389us/step - loss: 0.0550 - val_loss: 4.1166\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 380us/step - loss: 0.0486 - val_loss: 4.2058\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 361us/step - loss: 0.0371 - val_loss: 4.4246\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 389us/step - loss: 0.0439 - val_loss: 4.2369\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - 0s 371us/step - loss: 0.0383 - val_loss: 4.0660\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 370us/step - loss: 0.0423 - val_loss: 4.2160\n",
      "Epoch 00053: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 34\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 8s - loss: 1.134 - 1s 8ms/step - loss: 0.7989 - val_loss: 0.4529\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.587 - 0s 513us/step - loss: 0.5507 - val_loss: 0.8519\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.732 - 0s 408us/step - loss: 0.5168 - val_loss: 1.0036\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.848 - 0s 418us/step - loss: 0.4942 - val_loss: 0.8986\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.377 - 0s 380us/step - loss: 0.4554 - val_loss: 1.0646\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.427 - 0s 409us/step - loss: 0.4158 - val_loss: 0.9693\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.380 - 0s 399us/step - loss: 0.4315 - val_loss: 1.2302\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.672 - 0s 399us/step - loss: 0.3751 - val_loss: 1.2268\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.530 - 0s 409us/step - loss: 0.3640 - val_loss: 1.4145\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.324 - 0s 456us/step - loss: 0.3585 - val_loss: 1.2066\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - 0s 465us/step - loss: 0.3283 - val_loss: 1.4952\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.788 - 0s 427us/step - loss: 0.3180 - val_loss: 1.4306\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - 0s 456us/step - loss: 0.3015 - val_loss: 1.5716\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.389 - 0s 399us/step - loss: 0.2873 - val_loss: 1.5031\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - 0s 361us/step - loss: 0.2782 - val_loss: 1.5896\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - 0s 361us/step - loss: 0.2698 - val_loss: 1.7679\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.503 - 0s 380us/step - loss: 0.2748 - val_loss: 1.7179\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.290 - 0s 370us/step - loss: 0.2678 - val_loss: 1.8104\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - 0s 361us/step - loss: 0.2519 - val_loss: 1.8996\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.267 - 0s 351us/step - loss: 0.2456 - val_loss: 1.7438\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - 0s 361us/step - loss: 0.2554 - val_loss: 1.8800\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - 0s 342us/step - loss: 0.2431 - val_loss: 2.0456\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - 0s 361us/step - loss: 0.2372 - val_loss: 1.7369\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.246 - 0s 674us/step - loss: 0.2467 - val_loss: 1.7436\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - 0s 351us/step - loss: 0.2178 - val_loss: 2.0309\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 380us/step - loss: 0.2164 - val_loss: 2.2055\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - 0s 380us/step - loss: 0.2193 - val_loss: 2.2556\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - 0s 342us/step - loss: 0.2102 - val_loss: 2.0292\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - 0s 380us/step - loss: 0.2090 - val_loss: 1.9707\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - 0s 465us/step - loss: 0.1943 - val_loss: 2.2349\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - 0s 475us/step - loss: 0.2077 - val_loss: 1.9163\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.581 - 0s 446us/step - loss: 0.1976 - val_loss: 1.9712\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - 0s 437us/step - loss: 0.1864 - val_loss: 1.8628\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.373 - 0s 418us/step - loss: 0.1799 - val_loss: 1.8063\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - 0s 361us/step - loss: 0.1676 - val_loss: 1.8114\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 361us/step - loss: 0.1813 - val_loss: 1.8498\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - 0s 408us/step - loss: 0.1766 - val_loss: 1.7019\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - 0s 532us/step - loss: 0.1806 - val_loss: 1.9690\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - 0s 418us/step - loss: 0.1558 - val_loss: 1.9244\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.434 - 0s 418us/step - loss: 0.1574 - val_loss: 1.8369\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - 0s 428us/step - loss: 0.1552 - val_loss: 2.0797\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 427us/step - loss: 0.1475 - val_loss: 1.6558\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - 0s 418us/step - loss: 0.1665 - val_loss: 1.9555\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.449 - 0s 389us/step - loss: 0.1556 - val_loss: 1.6611\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - 0s 418us/step - loss: 0.1646 - val_loss: 1.7823\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 418us/step - loss: 0.1451 - val_loss: 1.8421\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.372 - 0s 465us/step - loss: 0.1507 - val_loss: 1.7229\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - 0s 389us/step - loss: 0.1495 - val_loss: 1.7160\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - 0s 380us/step - loss: 0.1328 - val_loss: 1.5374\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - 0s 370us/step - loss: 0.1598 - val_loss: 1.8651\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.355 - 0s 399us/step - loss: 0.1676 - val_loss: 1.7802\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 35\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.427 - 1s 8ms/step - loss: 0.9098 - val_loss: 0.0895\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - 0s 541us/step - loss: 0.8107 - val_loss: 0.0605\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - 0s 418us/step - loss: 0.7758 - val_loss: 0.0490\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 390us/step - loss: 0.7594 - val_loss: 0.0599\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 389us/step - loss: 0.7177 - val_loss: 0.0743\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.415 - ETA: 0s - loss: 0.723 - 0s 855us/step - loss: 0.6805 - val_loss: 0.0869\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.680 - 0s 342us/step - loss: 0.6619 - val_loss: 0.0833\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.998 - 0s 693us/step - loss: 0.6245 - val_loss: 0.0765\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - 0s 361us/step - loss: 0.5853 - val_loss: 0.0911\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.456 - 0s 342us/step - loss: 0.5448 - val_loss: 0.1229\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 342us/step - loss: 0.5092 - val_loss: 0.1223\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 370us/step - loss: 0.5083 - val_loss: 0.1408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - 0s 342us/step - loss: 0.4789 - val_loss: 0.1525\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - 0s 494us/step - loss: 0.4372 - val_loss: 0.1793\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 342us/step - loss: 0.4430 - val_loss: 0.1752\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.419 - 0s 342us/step - loss: 0.4149 - val_loss: 0.1681\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.365 - 0s 370us/step - loss: 0.3920 - val_loss: 0.1742\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 342us/step - loss: 0.3622 - val_loss: 0.1763\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.641 - 0s 332us/step - loss: 0.3188 - val_loss: 0.2133\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - 0s 332us/step - loss: 0.2906 - val_loss: 0.2097\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - 0s 370us/step - loss: 0.3103 - val_loss: 0.2539\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - 0s 352us/step - loss: 0.2593 - val_loss: 0.2463\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - 0s 408us/step - loss: 0.2368 - val_loss: 0.2468\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - 0s 323us/step - loss: 0.2375 - val_loss: 0.2627\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 361us/step - loss: 0.1912 - val_loss: 0.2776\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - 0s 447us/step - loss: 0.2125 - val_loss: 0.2389\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - 0s 342us/step - loss: 0.2036 - val_loss: 0.3136\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 351us/step - loss: 0.1746 - val_loss: 0.2706\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - 0s 342us/step - loss: 0.1799 - val_loss: 0.2987\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 342us/step - loss: 0.1669 - val_loss: 0.2823\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - 0s 361us/step - loss: 0.1606 - val_loss: 0.2838\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 361us/step - loss: 0.1303 - val_loss: 0.2981\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - 0s 352us/step - loss: 0.1332 - val_loss: 0.2993\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 342us/step - loss: 0.1026 - val_loss: 0.3106\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - 0s 333us/step - loss: 0.1378 - val_loss: 0.3090\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 351us/step - loss: 0.0962 - val_loss: 0.3057\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.475 - 0s 361us/step - loss: 0.1167 - val_loss: 0.3253\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - 0s 370us/step - loss: 0.0882 - val_loss: 0.3187\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - 0s 342us/step - loss: 0.1077 - val_loss: 0.3343\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - 0s 361us/step - loss: 0.0735 - val_loss: 0.3208\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - 0s 361us/step - loss: 0.0733 - val_loss: 0.3537\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - 0s 342us/step - loss: 0.0647 - val_loss: 0.3033\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 361us/step - loss: 0.0659 - val_loss: 0.3023\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - 0s 361us/step - loss: 0.0706 - val_loss: 0.3344\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - 0s 370us/step - loss: 0.0567 - val_loss: 0.3258\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 389us/step - loss: 0.0860 - val_loss: 0.3788\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - 0s 484us/step - loss: 0.0757 - val_loss: 0.3445\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - 0s 380us/step - loss: 0.0589 - val_loss: 0.3092\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - 0s 342us/step - loss: 0.0494 - val_loss: 0.3045\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - 0s 332us/step - loss: 0.0555 - val_loss: 0.3000\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - 0s 351us/step - loss: 0.0534 - val_loss: 0.2830\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 351us/step - loss: 0.0500 - val_loss: 0.2731\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 427us/step - loss: 0.0540 - val_loss: 0.2625\n",
      "Epoch 00053: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 36\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 0.317 - 1s 7ms/step - loss: 0.9157 - val_loss: 0.2539\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - 0s 389us/step - loss: 0.7523 - val_loss: 0.3078\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.263 - 0s 342us/step - loss: 0.6949 - val_loss: 0.4079\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - 0s 361us/step - loss: 0.6618 - val_loss: 0.4253\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.224 - 0s 361us/step - loss: 0.6154 - val_loss: 0.4712\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 456us/step - loss: 0.5362 - val_loss: 0.5722\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - 0s 351us/step - loss: 0.4926 - val_loss: 0.5006\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 351us/step - loss: 0.4594 - val_loss: 0.7490\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - 0s 351us/step - loss: 0.3999 - val_loss: 0.5811\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 370us/step - loss: 0.3757 - val_loss: 0.4218\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - 0s 361us/step - loss: 0.3359 - val_loss: 0.7601\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.740 - 0s 370us/step - loss: 0.3194 - val_loss: 0.7239\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 361us/step - loss: 0.2621 - val_loss: 0.7209\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - 0s 361us/step - loss: 0.2758 - val_loss: 0.7383\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - 0s 361us/step - loss: 0.2027 - val_loss: 0.7916\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.261 - 0s 370us/step - loss: 0.2011 - val_loss: 0.8389\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - 0s 361us/step - loss: 0.1677 - val_loss: 0.7004\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - 0s 370us/step - loss: 0.1359 - val_loss: 0.7819\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 370us/step - loss: 0.1982 - val_loss: 0.6785\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 361us/step - loss: 0.1531 - val_loss: 0.9624\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - 0s 361us/step - loss: 0.1271 - val_loss: 0.7297\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - 0s 361us/step - loss: 0.1725 - val_loss: 1.0629\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 475us/step - loss: 0.1176 - val_loss: 1.0084\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - 0s 370us/step - loss: 0.1154 - val_loss: 1.0026\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 389us/step - loss: 0.1027 - val_loss: 1.0746\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - 0s 351us/step - loss: 0.1021 - val_loss: 1.0887\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 371us/step - loss: 0.0836 - val_loss: 0.8939\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - 0s 351us/step - loss: 0.0844 - val_loss: 1.0683\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - 0s 342us/step - loss: 0.0796 - val_loss: 0.8917\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - 0s 352us/step - loss: 0.0780 - val_loss: 1.1854\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - 0s 352us/step - loss: 0.0636 - val_loss: 0.9432\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - 0s 351us/step - loss: 0.0553 - val_loss: 0.9956\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - 0s 342us/step - loss: 0.0542 - val_loss: 1.0792\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 351us/step - loss: 0.0691 - val_loss: 1.1604\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - 0s 342us/step - loss: 0.0732 - val_loss: 0.8594\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 361us/step - loss: 0.0751 - val_loss: 1.2107\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 342us/step - loss: 0.0807 - val_loss: 1.2152\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - 0s 380us/step - loss: 0.0530 - val_loss: 1.0345\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - 0s 361us/step - loss: 0.0508 - val_loss: 1.3140\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 361us/step - loss: 0.0568 - val_loss: 1.1177\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - 0s 351us/step - loss: 0.0522 - val_loss: 1.0795\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 437us/step - loss: 0.0501 - val_loss: 1.2407\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - 0s 361us/step - loss: 0.0595 - val_loss: 0.8690\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - 0s 342us/step - loss: 0.0652 - val_loss: 1.3401\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - 0s 380us/step - loss: 0.0572 - val_loss: 0.9644\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - 0s 380us/step - loss: 0.0445 - val_loss: 1.1461\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - 0s 504us/step - loss: 0.0446 - val_loss: 1.0462\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - 0s 361us/step - loss: 0.0447 - val_loss: 1.2933\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - 0s 399us/step - loss: 0.0585 - val_loss: 1.0259\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - 0s 475us/step - loss: 0.0530 - val_loss: 1.1867\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - 0s 437us/step - loss: 0.0442 - val_loss: 1.3563\n",
      "Epoch 00051: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 37\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,301\n",
      "Trainable params: 55,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 7s - loss: 2.761 - 1s 7ms/step - loss: 0.7920 - val_loss: 0.4553\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.404 - 0s 332us/step - loss: 0.5364 - val_loss: 0.7628\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.447 - 0s 361us/step - loss: 0.5075 - val_loss: 0.8289\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.546 - 0s 361us/step - loss: 0.4848 - val_loss: 0.7226\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - 0s 361us/step - loss: 0.4321 - val_loss: 0.8290\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - 0s 342us/step - loss: 0.4045 - val_loss: 0.8070\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - 0s 370us/step - loss: 0.3863 - val_loss: 1.0691\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.100 - 0s 370us/step - loss: 0.3769 - val_loss: 1.2515\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.284 - 0s 370us/step - loss: 0.3305 - val_loss: 1.1489\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - 0s 371us/step - loss: 0.3224 - val_loss: 1.3114\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - 0s 351us/step - loss: 0.2961 - val_loss: 1.3088\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - 0s 389us/step - loss: 0.2970 - val_loss: 1.2534\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 370us/step - loss: 0.2739 - val_loss: 1.4545\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.408 - 0s 409us/step - loss: 0.2791 - val_loss: 1.4738\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - 0s 475us/step - loss: 0.2519 - val_loss: 1.7320\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - 0s 380us/step - loss: 0.2258 - val_loss: 1.6386\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - 0s 370us/step - loss: 0.2295 - val_loss: 1.5169\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - 0s 446us/step - loss: 0.2122 - val_loss: 1.6258\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - 0s 370us/step - loss: 0.2067 - val_loss: 1.7527\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - 0s 351us/step - loss: 0.2031 - val_loss: 1.8309\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - 0s 361us/step - loss: 0.1956 - val_loss: 2.0127\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.326 - 0s 352us/step - loss: 0.1750 - val_loss: 1.6214\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - 0s 351us/step - loss: 0.1740 - val_loss: 1.9939\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - 0s 351us/step - loss: 0.1772 - val_loss: 2.1646\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 361us/step - loss: 0.1642 - val_loss: 2.1573\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - 0s 370us/step - loss: 0.1556 - val_loss: 1.9756\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - 0s 342us/step - loss: 0.1424 - val_loss: 1.7310\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - 0s 370us/step - loss: 0.1413 - val_loss: 1.9888\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - 0s 361us/step - loss: 0.1370 - val_loss: 1.7658\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - 0s 351us/step - loss: 0.1313 - val_loss: 2.1489\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - 0s 370us/step - loss: 0.1302 - val_loss: 1.8422\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - 0s 361us/step - loss: 0.1395 - val_loss: 1.9521\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - 0s 351us/step - loss: 0.1198 - val_loss: 1.8030\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 361us/step - loss: 0.1205 - val_loss: 1.7561\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - 0s 361us/step - loss: 0.1143 - val_loss: 2.0086\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - 0s 361us/step - loss: 0.1035 - val_loss: 1.9381\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - 0s 352us/step - loss: 0.1078 - val_loss: 2.2563\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - 0s 380us/step - loss: 0.1144 - val_loss: 1.8529\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - 0s 380us/step - loss: 0.0990 - val_loss: 2.1387\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - 0s 389us/step - loss: 0.0926 - val_loss: 2.1321\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - 0s 351us/step - loss: 0.1047 - val_loss: 2.3520\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - 0s 323us/step - loss: 0.1035 - val_loss: 1.6889\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - 0s 370us/step - loss: 0.1106 - val_loss: 2.0631\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - 0s 351us/step - loss: 0.0939 - val_loss: 1.9523\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - 0s 370us/step - loss: 0.0845 - val_loss: 1.4508\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - 0s 351us/step - loss: 0.0942 - val_loss: 1.8173\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - 0s 342us/step - loss: 0.0800 - val_loss: 2.0273\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - 0s 351us/step - loss: 0.0827 - val_loss: 1.8429\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - 0s 351us/step - loss: 0.0871 - val_loss: 2.0596\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - 0s 351us/step - loss: 0.0824 - val_loss: 1.7059\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - 0s 351us/step - loss: 0.0775 - val_loss: 1.8116\n",
      "Epoch 00051: early stopping\n",
      "MSE: 931.682912\n",
      "RMSE: 30.523481\n",
      "MAE: 23.253304\n",
      "MAPE: 27.719433\n",
      "\n",
      "Quantile 1, between 39.99999999999999 and 77.5\n",
      "MSE: 741.595041\n",
      "RMSE: 27.232243\n",
      "MAE: 21.962900\n",
      "MAPE: 37.285987\n",
      "\n",
      "Quantile 2, between 77.5 and 87.5\n",
      "MSE: 1058.493178\n",
      "RMSE: 32.534492\n",
      "MAE: 21.507399\n",
      "MAPE: 25.958253\n",
      "\n",
      "Quantile 3, between 87.5 and 98.00000000000001\n",
      "MSE: 397.692462\n",
      "RMSE: 19.942228\n",
      "MAE: 16.840608\n",
      "MAPE: 18.038165\n",
      "\n",
      "Quantile 4, between 98.00000000000001 and 130.00000000000003\n",
      "MSE: 1488.232950\n",
      "RMSE: 38.577622\n",
      "MAE: 31.886449\n",
      "MAPE: 28.451081\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_test = 15\n",
    "name_error = 'LSTM predictions - 15 Days - 4q'\n",
    "path_error = \"imgs/ips_predictions/prediction_lstm_15_days_4q.png\"\n",
    "name_map = '15 days'\n",
    "path_map = \"imgs/ips_predictions/maps_LSTM_prediction_15_4q.png\"\n",
    "\n",
    "dict_pred,pred_lstm,pred_lstm_cases = lstm_feature_importance(data_days,n_test,name_error,path_error,name_map,path_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pred_sel_hexa = dict_pred.copy()\n",
    "\n",
    "with open('data/LSTM_features_importance_1d15d_4q.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_pred_sel_hexa, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('data/LSTM_features_importance_1d15d_4q.pickle', 'rb') as handle:\n",
    "    dict_pred_sel_hexa = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo  se comportan los hexagonos mas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hexa0_best_4q.png\n",
      "hexa10_best_4q.png\n",
      "hexa11_best_4q.png\n",
      "hexa12_best_4q.png\n",
      "hexa13_best_4q.png\n",
      "hexa14_best_4q.png\n",
      "hexa15_best_4q.png\n",
      "hexa16_best_4q.png\n",
      "hexa17_best_4q.png\n",
      "hexa18_best_4q.png\n",
      "hexa19_best_4q.png\n",
      "hexa1_best_4q.png\n",
      "hexa20_best_4q.png\n",
      "hexa21_best_4q.png\n",
      "hexa22_best_4q.png\n",
      "hexa23_best_4q.png\n",
      "hexa24_best_4q.png\n",
      "hexa25_best_4q.png\n",
      "hexa26_best_4q.png\n",
      "hexa27_best_4q.png\n",
      "hexa28_best_4q.png\n",
      "hexa29_best_4q.png\n",
      "hexa2_best_4q.png\n",
      "hexa30_best_4q.png\n",
      "hexa31_best_4q.png\n",
      "hexa32_best_4q.png\n",
      "hexa33_best_4q.png\n",
      "hexa34_best_4q.png\n",
      "hexa35_best_4q.png\n",
      "hexa36_best_4q.png\n",
      "hexa37_best_4q.png\n",
      "hexa3_best_4q.png\n",
      "hexa4_best_4q.png\n",
      "hexa5_best_4q.png\n",
      "hexa6_best_4q.png\n",
      "hexa7_best_4q.png\n",
      "hexa8_best_4q.png\n",
      "hexa9_best_4q.png\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "# Created a gif using the plots generated\n",
    "images = []\n",
    "for file_name in sorted(os.listdir('imgs/ips_predictions/best_hex')):\n",
    "    if file_name.endswith('4q.png'):\n",
    "        print(file_name)\n",
    "        file_path = os.path.join('imgs/ips_predictions/best_hex', file_name)\n",
    "        images.append(imageio.imread(file_path))\n",
    "        \n",
    "imageio.mimsave('imgs/ips_predictions/best_hex' + '/movie.gif', images, fps=20, duration=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis con diferentes número de hexágonos (SHAP feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,) (15,)\n",
      "model compiled 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.21 - ETA: 0s - loss: 0.8171 - 1s 8ms/step - loss: 0.6941 - val_loss: 0.7968\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.170 - ETA: 0s - loss: 0.661 - ETA: 0s - loss: 0.557 - 0s 1ms/step - loss: 0.5533 - val_loss: 0.7390\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.560 - 0s 998us/step - loss: 0.5322 - val_loss: 0.9098\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.498 - 0s 1ms/step - loss: 0.4701 - val_loss: 0.9359\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.461 - 0s 883us/step - loss: 0.4467 - val_loss: 1.0176\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.515 - ETA: 0s - loss: 0.430 - 0s 884us/step - loss: 0.4326 - val_loss: 1.0267\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.065 - ETA: 0s - loss: 0.491 - 0s 978us/step - loss: 0.4032 - val_loss: 1.0846\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.387 - 0s 1ms/step - loss: 0.3904 - val_loss: 1.0323\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.356 - 0s 978us/step - loss: 0.3659 - val_loss: 1.3711\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.521 - ETA: 0s - loss: 0.425 - 0s 1ms/step - loss: 0.3635 - val_loss: 1.1882\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.332 - ETA: 0s - loss: 0.336 - 0s 1ms/step - loss: 0.3207 - val_loss: 1.2840\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.215 - 0s 1ms/step - loss: 0.3220 - val_loss: 1.2294\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.304 - 0s 874us/step - loss: 0.2870 - val_loss: 1.5833\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.200 - 0s 960us/step - loss: 0.3005 - val_loss: 1.5066\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.257 - 0s 846us/step - loss: 0.2653 - val_loss: 1.8034\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.303 - 0s 883us/step - loss: 0.2759 - val_loss: 1.6513\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.401 - ETA: 0s - loss: 0.268 - 0s 940us/step - loss: 0.2541 - val_loss: 1.9034\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.253 - 0s 959us/step - loss: 0.2435 - val_loss: 1.4617\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.226 - 0s 1ms/step - loss: 0.2307 - val_loss: 1.5703\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.233 - 0s 921us/step - loss: 0.2235 - val_loss: 1.9240\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.183 - 0s 950us/step - loss: 0.2138 - val_loss: 1.9822\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.215 - 0s 1ms/step - loss: 0.1975 - val_loss: 2.1273\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.175 - 0s 1ms/step - loss: 0.2094 - val_loss: 1.8436\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.165 - 0s 978us/step - loss: 0.1877 - val_loss: 1.9465\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.221 - 0s 1ms/step - loss: 0.1896 - val_loss: 1.9962\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.180 - 0s 1ms/step - loss: 0.1726 - val_loss: 2.1520\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.170 - 0s 1ms/step - loss: 0.1725 - val_loss: 2.1717\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.144 - 0s 1ms/step - loss: 0.1682 - val_loss: 1.9857\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.164 - 0s 2ms/step - loss: 0.1627 - val_loss: 2.5199\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.166 - 0s 3ms/step - loss: 0.1648 - val_loss: 2.2576\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.162 - 0s 1ms/step - loss: 0.1585 - val_loss: 2.2259\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.156 - 0s 2ms/step - loss: 0.1448 - val_loss: 2.6301\n",
      "Epoch 00032: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.66 - ETA: 0s - loss: 0.7622 - 1s 8ms/step - loss: 0.6453 - val_loss: 0.7748\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.906 - ETA: 0s - loss: 0.525 - 0s 988us/step - loss: 0.4767 - val_loss: 0.9568\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.391 - 0s 902us/step - loss: 0.4126 - val_loss: 0.8124\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.805 - ETA: 0s - loss: 0.452 - 0s 1ms/step - loss: 0.3804 - val_loss: 0.9436\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.361 - 0s 855us/step - loss: 0.3266 - val_loss: 0.9019\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.519 - ETA: 0s - loss: 0.303 - 0s 1ms/step - loss: 0.2928 - val_loss: 0.9962\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.350 - 0s 969us/step - loss: 0.2991 - val_loss: 0.8486\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.277 - 0s 826us/step - loss: 0.2767 - val_loss: 0.8914\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.272 - 0s 808us/step - loss: 0.2575 - val_loss: 0.8922\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.383 - ETA: 0s - loss: 0.200 - 0s 874us/step - loss: 0.2385 - val_loss: 0.7210\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.517 - ETA: 0s - loss: 0.220 - 0s 817us/step - loss: 0.2209 - val_loss: 0.8870\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.229 - 0s 760us/step - loss: 0.2150 - val_loss: 0.8005\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.172 - 0s 760us/step - loss: 0.2106 - val_loss: 0.7679\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.749 - ETA: 0s - loss: 0.208 - 0s 855us/step - loss: 0.1990 - val_loss: 0.8744\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.196 - 0s 902us/step - loss: 0.2029 - val_loss: 0.9020\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.208 - 0s 826us/step - loss: 0.1816 - val_loss: 0.9685\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.165 - 0s 788us/step - loss: 0.1775 - val_loss: 0.8890\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.146 - 0s 807us/step - loss: 0.1698 - val_loss: 0.8271\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.185 - 0s 788us/step - loss: 0.1694 - val_loss: 0.9403\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.145 - 0s 826us/step - loss: 0.1637 - val_loss: 0.7758\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.165 - 0s 836us/step - loss: 0.1506 - val_loss: 1.0262\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.179 - 0s 855us/step - loss: 0.1498 - val_loss: 1.1086\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.142 - 0s 779us/step - loss: 0.1358 - val_loss: 0.9083\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.142 - 0s 798us/step - loss: 0.1369 - val_loss: 0.8651\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.154 - 0s 893us/step - loss: 0.1395 - val_loss: 1.1481\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.117 - 0s 769us/step - loss: 0.1315 - val_loss: 0.7880\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.136 - 0s 883us/step - loss: 0.1298 - val_loss: 1.0592\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.130 - 0s 798us/step - loss: 0.1201 - val_loss: 1.0112\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.106 - 0s 817us/step - loss: 0.1233 - val_loss: 0.9788\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.489 - ETA: 0s - loss: 0.118 - 0s 817us/step - loss: 0.1166 - val_loss: 1.1150\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.117 - 0s 798us/step - loss: 0.1086 - val_loss: 1.1948\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.103 - 0s 826us/step - loss: 0.1091 - val_loss: 1.1218\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.103 - 0s 769us/step - loss: 0.0992 - val_loss: 1.0703\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.082 - 0s 769us/step - loss: 0.1037 - val_loss: 0.8003\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.122 - 0s 826us/step - loss: 0.1185 - val_loss: 1.0783\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.093 - 0s 855us/step - loss: 0.1067 - val_loss: 1.0463\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.109 - 0s 893us/step - loss: 0.1035 - val_loss: 0.9836\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.099 - 0s 893us/step - loss: 0.0960 - val_loss: 0.9731\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.106 - 0s 855us/step - loss: 0.0899 - val_loss: 1.0051\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.104 - 0s 836us/step - loss: 0.0960 - val_loss: 1.0780\n",
      "Epoch 00040: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.24 - ETA: 0s - loss: 0.7653 - 1s 8ms/step - loss: 0.6471 - val_loss: 0.6125\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.618 - 0s 798us/step - loss: 0.5380 - val_loss: 0.7226\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.443 - ETA: 0s - loss: 0.524 - 0s 789us/step - loss: 0.4613 - val_loss: 0.6871\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.547 - ETA: 0s - loss: 0.444 - 0s 807us/step - loss: 0.4193 - val_loss: 0.7182\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.522 - ETA: 0s - loss: 0.409 - 0s 817us/step - loss: 0.3768 - val_loss: 0.7129\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.189 - 0s 855us/step - loss: 0.3291 - val_loss: 0.7825\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.187 - 0s 855us/step - loss: 0.3003 - val_loss: 0.7945\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.283 - 0s 902us/step - loss: 0.2657 - val_loss: 0.6091\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.222 - 0s 789us/step - loss: 0.2636 - val_loss: 0.6379\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.232 - 0s 779us/step - loss: 0.2588 - val_loss: 0.6764\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.153 - 0s 836us/step - loss: 0.2265 - val_loss: 0.6862\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.234 - 0s 817us/step - loss: 0.2062 - val_loss: 0.7297\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.142 - 0s 865us/step - loss: 0.1857 - val_loss: 0.8525\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.199 - 0s 798us/step - loss: 0.1992 - val_loss: 0.7728\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.172 - 0s 855us/step - loss: 0.1884 - val_loss: 1.0420\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.212 - 0s 960us/step - loss: 0.1813 - val_loss: 0.6944\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.169 - 0s 912us/step - loss: 0.1609 - val_loss: 0.8215\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.084 - 0s 912us/step - loss: 0.1531 - val_loss: 0.9907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.158 - 0s 865us/step - loss: 0.1779 - val_loss: 0.7069\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.154 - 0s 845us/step - loss: 0.1417 - val_loss: 0.8022\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.141 - 0s 798us/step - loss: 0.1324 - val_loss: 0.8971\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.109 - 0s 826us/step - loss: 0.1361 - val_loss: 0.6574\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.118 - 0s 779us/step - loss: 0.1117 - val_loss: 0.9497\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.095 - 0s 750us/step - loss: 0.1223 - val_loss: 0.7258\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.079 - 0s 779us/step - loss: 0.1052 - val_loss: 0.7611\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.098 - 0s 874us/step - loss: 0.0935 - val_loss: 0.8147\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.088 - 0s 808us/step - loss: 0.0961 - val_loss: 0.7737\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.088 - 0s 874us/step - loss: 0.0844 - val_loss: 0.7721\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.102 - 0s 912us/step - loss: 0.0895 - val_loss: 0.5527\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.066 - 0s 883us/step - loss: 0.0812 - val_loss: 0.7627\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.099 - 0s 855us/step - loss: 0.0901 - val_loss: 0.8241\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.077 - 0s 826us/step - loss: 0.0712 - val_loss: 0.6477\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.067 - 0s 845us/step - loss: 0.0698 - val_loss: 0.7858\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.066 - 0s 826us/step - loss: 0.0667 - val_loss: 0.6239\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.054 - 0s 826us/step - loss: 0.0682 - val_loss: 0.6296\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.072 - 0s 874us/step - loss: 0.0661 - val_loss: 0.8439\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.065 - 0s 864us/step - loss: 0.0853 - val_loss: 0.7895\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.044 - 0s 940us/step - loss: 0.0593 - val_loss: 0.6881\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.051 - 0s 883us/step - loss: 0.0521 - val_loss: 0.6398\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.048 - 0s 836us/step - loss: 0.0544 - val_loss: 0.7305\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.063 - 0s 921us/step - loss: 0.0534 - val_loss: 0.7847\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.057 - 0s 893us/step - loss: 0.0631 - val_loss: 0.7980\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.051 - 0s 865us/step - loss: 0.0590 - val_loss: 0.7601\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.055 - 0s 855us/step - loss: 0.0538 - val_loss: 0.5493\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.054 - 0s 788us/step - loss: 0.0497 - val_loss: 0.8684\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.051 - 0s 817us/step - loss: 0.0508 - val_loss: 0.7704\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.037 - 0s 950us/step - loss: 0.0367 - val_loss: 0.6600\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.038 - 0s 940us/step - loss: 0.0425 - val_loss: 0.9916\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.053 - 0s 931us/step - loss: 0.0490 - val_loss: 0.7087\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.040 - 0s 912us/step - loss: 0.0404 - val_loss: 0.6926\n",
      "(105,) (15,)\n",
      "model compiled 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.50 - ETA: 0s - loss: 0.6480 - 1s 8ms/step - loss: 0.5076 - val_loss: 1.2367\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.326 - 0s 864us/step - loss: 0.3210 - val_loss: 1.3935\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.299 - 0s 845us/step - loss: 0.2735 - val_loss: 1.2394\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.266 - 0s 874us/step - loss: 0.2502 - val_loss: 1.5469\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.195 - 0s 817us/step - loss: 0.2064 - val_loss: 1.5535\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.230 - 0s 846us/step - loss: 0.2200 - val_loss: 1.5490\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.192 - 0s 902us/step - loss: 0.1838 - val_loss: 2.1993\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.127 - 0s 874us/step - loss: 0.1493 - val_loss: 2.1037\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.148 - 0s 836us/step - loss: 0.1515 - val_loss: 2.4002\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.141 - 0s 817us/step - loss: 0.1275 - val_loss: 2.1258\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.134 - 0s 864us/step - loss: 0.1245 - val_loss: 2.2765\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.089 - 0s 912us/step - loss: 0.1169 - val_loss: 2.6817\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.123 - 0s 855us/step - loss: 0.1122 - val_loss: 1.8917\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.126 - 0s 826us/step - loss: 0.1099 - val_loss: 2.6943\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.507 - ETA: 0s - loss: 0.105 - 0s 883us/step - loss: 0.1050 - val_loss: 2.2495\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.096 - 0s 883us/step - loss: 0.0950 - val_loss: 2.6938\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.094 - 0s 1ms/step - loss: 0.0932 - val_loss: 2.2423\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.080 - 0s 712us/step - loss: 0.0886 - val_loss: 2.3725\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.085 - 0s 731us/step - loss: 0.0801 - val_loss: 2.7987\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.080 - 0s 741us/step - loss: 0.0795 - val_loss: 2.2964\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.077 - 0s 731us/step - loss: 0.0718 - val_loss: 2.6451\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.049 - 0s 779us/step - loss: 0.0659 - val_loss: 2.6527\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.058 - 0s 874us/step - loss: 0.0585 - val_loss: 2.4101\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.060 - 0s 988us/step - loss: 0.0635 - val_loss: 3.0873\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.079 - 0s 864us/step - loss: 0.0680 - val_loss: 2.4979\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.063 - 0s 770us/step - loss: 0.0589 - val_loss: 2.6868\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.050 - 0s 798us/step - loss: 0.0506 - val_loss: 2.4534\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.050 - 0s 798us/step - loss: 0.0513 - val_loss: 2.7715\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.045 - 0s 807us/step - loss: 0.0465 - val_loss: 2.6983\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.059 - 0s 779us/step - loss: 0.0566 - val_loss: 3.1887\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.048 - 0s 769us/step - loss: 0.0538 - val_loss: 3.0471\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.28 - ETA: 0s - loss: 0.9427 - 1s 8ms/step - loss: 0.8390 - val_loss: 0.3157\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.560 - ETA: 0s - loss: 0.687 - 0s 893us/step - loss: 0.5857 - val_loss: 0.4897\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.564 - ETA: 0s - loss: 0.666 - 0s 798us/step - loss: 0.5507 - val_loss: 0.4142\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.456 - 0s 893us/step - loss: 0.5155 - val_loss: 0.4572\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.465 - 0s 855us/step - loss: 0.4673 - val_loss: 0.5215\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.394 - ETA: 0s - loss: 0.413 - 0s 845us/step - loss: 0.4269 - val_loss: 0.5207\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.333 - 0s 883us/step - loss: 0.3992 - val_loss: 0.5358\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.320 - 0s 836us/step - loss: 0.3635 - val_loss: 0.5424\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.367 - 0s 817us/step - loss: 0.3680 - val_loss: 0.6118\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.319 - 0s 817us/step - loss: 0.3345 - val_loss: 0.6928\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.556 - ETA: 0s - loss: 0.302 - 0s 817us/step - loss: 0.3333 - val_loss: 0.6325\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.323 - 0s 855us/step - loss: 0.3324 - val_loss: 0.7322\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.320 - 0s 874us/step - loss: 0.2863 - val_loss: 0.6592\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.981 - ETA: 0s - loss: 0.297 - 0s 779us/step - loss: 0.2858 - val_loss: 0.7330\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.289 - 0s 969us/step - loss: 0.2699 - val_loss: 0.8508\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.324 - 0s 959us/step - loss: 0.2657 - val_loss: 0.7344\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.250 - 0s 817us/step - loss: 0.2641 - val_loss: 0.7452\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.201 - 0s 836us/step - loss: 0.2290 - val_loss: 0.7167\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - ETA: 0s - loss: 0.234 - 0s 855us/step - loss: 0.2376 - val_loss: 0.7722\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.219 - 0s 845us/step - loss: 0.2067 - val_loss: 0.8023\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.228 - 0s 826us/step - loss: 0.2200 - val_loss: 0.8517\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.214 - 0s 883us/step - loss: 0.1980 - val_loss: 0.9654\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.232 - 0s 893us/step - loss: 0.2041 - val_loss: 0.8023\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.189 - 0s 864us/step - loss: 0.1848 - val_loss: 0.8757\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.149 - 0s 940us/step - loss: 0.1803 - val_loss: 0.7681\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.114 - 0s 902us/step - loss: 0.1698 - val_loss: 0.8836\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.176 - 0s 826us/step - loss: 0.1648 - val_loss: 0.8951\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.183 - 0s 808us/step - loss: 0.1698 - val_loss: 0.8686\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.183 - 0s 874us/step - loss: 0.1743 - val_loss: 0.8983\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.157 - 0s 817us/step - loss: 0.1602 - val_loss: 1.0638\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.161 - 0s 902us/step - loss: 0.1488 - val_loss: 0.9474\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 5.06 - ETA: 0s - loss: 1.0425 - 1s 8ms/step - loss: 0.8217 - val_loss: 0.4529\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.455 - 0s 807us/step - loss: 0.6104 - val_loss: 0.5266\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.973 - ETA: 0s - loss: 0.597 - 0s 769us/step - loss: 0.5515 - val_loss: 0.6683\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.426 - 0s 874us/step - loss: 0.4885 - val_loss: 0.7770\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.394 - 0s 836us/step - loss: 0.4359 - val_loss: 0.8190\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.407 - 0s 854us/step - loss: 0.3757 - val_loss: 0.8466\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.391 - 0s 864us/step - loss: 0.3427 - val_loss: 0.9693\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.481 - ETA: 0s - loss: 0.327 - 0s 874us/step - loss: 0.3072 - val_loss: 1.1118\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.291 - 0s 883us/step - loss: 0.2964 - val_loss: 0.9496\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.266 - 0s 807us/step - loss: 0.2634 - val_loss: 1.1865\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.683 - ETA: 0s - loss: 0.292 - 0s 893us/step - loss: 0.2704 - val_loss: 1.7149\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.226 - 0s 874us/step - loss: 0.2191 - val_loss: 1.4000\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.214 - 0s 874us/step - loss: 0.2107 - val_loss: 1.9165\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.227 - 0s 817us/step - loss: 0.1902 - val_loss: 1.6231\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.198 - 0s 864us/step - loss: 0.1951 - val_loss: 1.4697\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.154 - 0s 883us/step - loss: 0.1849 - val_loss: 1.5193\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.153 - 0s 817us/step - loss: 0.1754 - val_loss: 1.8192\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.130 - 0s 845us/step - loss: 0.1640 - val_loss: 1.6484\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.135 - 0s 807us/step - loss: 0.1599 - val_loss: 1.8783\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.138 - 0s 845us/step - loss: 0.1469 - val_loss: 1.7182\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.614 - ETA: 0s - loss: 0.136 - 0s 826us/step - loss: 0.1456 - val_loss: 1.6140\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.142 - 0s 779us/step - loss: 0.1545 - val_loss: 1.8231\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.111 - 0s 807us/step - loss: 0.1384 - val_loss: 1.2601\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.128 - 0s 874us/step - loss: 0.1350 - val_loss: 1.8478\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.127 - 0s 769us/step - loss: 0.1353 - val_loss: 1.5901\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.136 - 0s 826us/step - loss: 0.1365 - val_loss: 1.8743\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.149 - 0s 826us/step - loss: 0.1331 - val_loss: 1.3177\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.143 - 0s 893us/step - loss: 0.1277 - val_loss: 1.8035\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.146 - 0s 1ms/step - loss: 0.1278 - val_loss: 1.7596\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.073 - 0s 855us/step - loss: 0.1099 - val_loss: 1.4832\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.135 - 0s 817us/step - loss: 0.1234 - val_loss: 2.1416\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.05 - ETA: 1s - loss: 0.8847 - ETA: 0s - loss: 0.887 - 1s 8ms/step - loss: 0.7533 - val_loss: 0.5165\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.756 - ETA: 0s - loss: 0.585 - 0s 836us/step - loss: 0.5801 - val_loss: 0.6548\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.588 - 0s 922us/step - loss: 0.5208 - val_loss: 0.6390\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.591 - ETA: 0s - loss: 0.424 - 0s 807us/step - loss: 0.4809 - val_loss: 0.7411\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.496 - 0s 912us/step - loss: 0.4161 - val_loss: 0.7704\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.315 - 0s 845us/step - loss: 0.4033 - val_loss: 0.8621\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.267 - 0s 845us/step - loss: 0.3637 - val_loss: 0.9491\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.285 - 0s 902us/step - loss: 0.3461 - val_loss: 0.8562\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.230 - 0s 978us/step - loss: 0.3422 - val_loss: 1.0061\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.278 - 0s 940us/step - loss: 0.3340 - val_loss: 1.0782\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.877 - ETA: 0s - loss: 0.320 - 0s 912us/step - loss: 0.2934 - val_loss: 1.0206\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.299 - 0s 826us/step - loss: 0.2685 - val_loss: 0.9621\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - ETA: 0s - loss: 0.218 - 0s 902us/step - loss: 0.2474 - val_loss: 0.8904\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.161 - 0s 912us/step - loss: 0.2342 - val_loss: 1.1192\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.196 - 0s 874us/step - loss: 0.2224 - val_loss: 0.9350\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.228 - 0s 826us/step - loss: 0.2055 - val_loss: 1.1044\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.208 - 0s 789us/step - loss: 0.2381 - val_loss: 1.3278\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.242 - 0s 798us/step - loss: 0.2153 - val_loss: 0.9543\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.179 - 0s 902us/step - loss: 0.2071 - val_loss: 1.4468\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.596 - ETA: 0s - loss: 0.229 - 0s 931us/step - loss: 0.1872 - val_loss: 1.2062\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.160 - 0s 836us/step - loss: 0.1642 - val_loss: 1.1926\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.163 - 0s 798us/step - loss: 0.1603 - val_loss: 1.1801\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.151 - 0s 836us/step - loss: 0.1592 - val_loss: 1.3107\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.129 - 0s 808us/step - loss: 0.1459 - val_loss: 1.2767\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.144 - 0s 845us/step - loss: 0.1573 - val_loss: 1.3888\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.131 - 0s 836us/step - loss: 0.1446 - val_loss: 1.1510\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.139 - 0s 817us/step - loss: 0.1377 - val_loss: 1.3305\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.133 - 0s 836us/step - loss: 0.1242 - val_loss: 1.0267\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.125 - 0s 817us/step - loss: 0.1126 - val_loss: 1.3444\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.130 - 0s 902us/step - loss: 0.1266 - val_loss: 1.1125\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.085 - 0s 902us/step - loss: 0.1125 - val_loss: 1.7135\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 7\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.66 - ETA: 0s - loss: 0.6434 - 1s 8ms/step - loss: 0.7463 - val_loss: 0.5297\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.813 - ETA: 0s - loss: 0.583 - 0s 798us/step - loss: 0.5319 - val_loss: 0.9482\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.505 - 0s 845us/step - loss: 0.4720 - val_loss: 0.8367\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.115 - ETA: 0s - loss: 0.494 - 0s 750us/step - loss: 0.4500 - val_loss: 0.9233\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.359 - 0s 808us/step - loss: 0.4109 - val_loss: 0.7878\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.358 - 0s 798us/step - loss: 0.3807 - val_loss: 1.0686\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.453 - 0s 883us/step - loss: 0.3645 - val_loss: 1.0234\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.339 - 0s 893us/step - loss: 0.3205 - val_loss: 1.2352\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.297 - 0s 836us/step - loss: 0.2975 - val_loss: 1.2383\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.319 - 0s 855us/step - loss: 0.2956 - val_loss: 1.1317\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.224 - 0s 779us/step - loss: 0.2938 - val_loss: 1.0759\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.307 - 0s 1ms/step - loss: 0.2650 - val_loss: 1.0402\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.289 - 0s 931us/step - loss: 0.2633 - val_loss: 1.3045\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.254 - 0s 769us/step - loss: 0.2477 - val_loss: 1.2504\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.665 - ETA: 0s - loss: 0.222 - 0s 817us/step - loss: 0.2259 - val_loss: 1.4077\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.227 - 0s 874us/step - loss: 0.2255 - val_loss: 0.9895\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.453 - ETA: 0s - loss: 0.189 - 0s 902us/step - loss: 0.2138 - val_loss: 1.4466\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.551 - ETA: 0s - loss: 0.204 - 0s 912us/step - loss: 0.2083 - val_loss: 1.2923\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.179 - 0s 912us/step - loss: 0.2001 - val_loss: 1.1651\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.206 - 0s 798us/step - loss: 0.1952 - val_loss: 1.4091\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.185 - 0s 807us/step - loss: 0.1928 - val_loss: 1.1339\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.173 - 0s 931us/step - loss: 0.1688 - val_loss: 1.5559\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.155 - 0s 883us/step - loss: 0.1642 - val_loss: 1.3106\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.176 - 0s 893us/step - loss: 0.1592 - val_loss: 1.3097\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.142 - 0s 969us/step - loss: 0.1580 - val_loss: 1.6444\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.141 - 0s 883us/step - loss: 0.1489 - val_loss: 1.3415\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.143 - 0s 921us/step - loss: 0.1438 - val_loss: 1.6421\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.151 - 0s 893us/step - loss: 0.1446 - val_loss: 1.9340\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.158 - 0s 874us/step - loss: 0.1431 - val_loss: 1.5703\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.134 - 0s 902us/step - loss: 0.1418 - val_loss: 1.6081\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.125 - 0s 931us/step - loss: 0.1358 - val_loss: 1.5537\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 8\n",
      "Model: \"sequential_1\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.17 - ETA: 0s - loss: 0.6181 - 1s 8ms/step - loss: 0.5581 - val_loss: 0.8812\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.627 - ETA: 0s - loss: 0.468 - 0s 760us/step - loss: 0.4310 - val_loss: 0.9114\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.366 - 0s 731us/step - loss: 0.3661 - val_loss: 0.7564\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.198 - 0s 893us/step - loss: 0.2897 - val_loss: 0.5259\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.252 - 0s 1ms/step - loss: 0.2513 - val_loss: 0.5483\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.272 - 0s 769us/step - loss: 0.2400 - val_loss: 0.3573\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.164 - 0s 788us/step - loss: 0.1773 - val_loss: 0.4056\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.166 - 0s 798us/step - loss: 0.1777 - val_loss: 0.4642\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.115 - 0s 874us/step - loss: 0.1663 - val_loss: 0.3530\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.108 - 0s 798us/step - loss: 0.1338 - val_loss: 0.3348\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.125 - 0s 836us/step - loss: 0.1387 - val_loss: 0.3758\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.088 - 0s 807us/step - loss: 0.1183 - val_loss: 0.4254\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.118 - 0s 779us/step - loss: 0.1196 - val_loss: 0.4017\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.089 - 0s 826us/step - loss: 0.0914 - val_loss: 0.3812\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.084 - 0s 893us/step - loss: 0.0965 - val_loss: 0.4339\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.081 - 0s 845us/step - loss: 0.0827 - val_loss: 0.3964\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.086 - 0s 826us/step - loss: 0.0963 - val_loss: 0.4286\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.058 - 0s 817us/step - loss: 0.0664 - val_loss: 0.4735\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.077 - 0s 836us/step - loss: 0.0764 - val_loss: 0.4507\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.076 - 0s 836us/step - loss: 0.0712 - val_loss: 0.5060\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.068 - 0s 883us/step - loss: 0.0662 - val_loss: 0.4005\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.066 - 0s 779us/step - loss: 0.0626 - val_loss: 0.4671\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.059 - 0s 893us/step - loss: 0.0655 - val_loss: 0.5211\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.062 - 0s 817us/step - loss: 0.0706 - val_loss: 0.3681\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.051 - 0s 807us/step - loss: 0.0657 - val_loss: 0.4511\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.062 - 0s 903us/step - loss: 0.0561 - val_loss: 0.4095\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.066 - 0s 884us/step - loss: 0.0640 - val_loss: 0.4938\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - 0s 845us/step - loss: 0.0519 - val_loss: 0.4320\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.046 - 0s 921us/step - loss: 0.0526 - val_loss: 0.4672\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.051 - 0s 845us/step - loss: 0.0579 - val_loss: 0.4277\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.037 - 0s 893us/step - loss: 0.0488 - val_loss: 0.4760\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.049 - 0s 912us/step - loss: 0.0481 - val_loss: 0.4112\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.040 - 0s 959us/step - loss: 0.0436 - val_loss: 0.5562\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.043 - 0s 902us/step - loss: 0.0483 - val_loss: 0.5447\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.062 - 0s 836us/step - loss: 0.0591 - val_loss: 0.4095\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.052 - 0s 912us/step - loss: 0.0455 - val_loss: 0.4630\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.046 - 0s 921us/step - loss: 0.0439 - val_loss: 0.4554\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.038 - 0s 836us/step - loss: 0.0446 - val_loss: 0.4382\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.052 - 0s 855us/step - loss: 0.0529 - val_loss: 0.5570\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.046 - 0s 807us/step - loss: 0.0461 - val_loss: 0.4456\n",
      "Epoch 00040: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 9\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.23 - ETA: 0s - loss: 0.8929 - 1s 8ms/step - loss: 0.7340 - val_loss: 0.4636\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.410 - 0s 807us/step - loss: 0.5062 - val_loss: 0.5250\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.459 - 0s 855us/step - loss: 0.4223 - val_loss: 0.7286\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.368 - 0s 807us/step - loss: 0.3660 - val_loss: 0.5894\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.294 - 0s 855us/step - loss: 0.2795 - val_loss: 0.3317\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.251 - 0s 798us/step - loss: 0.2351 - val_loss: 0.5082\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.182 - 0s 864us/step - loss: 0.1884 - val_loss: 0.4395\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.169 - 0s 883us/step - loss: 0.1736 - val_loss: 0.4090\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.129 - 0s 874us/step - loss: 0.1596 - val_loss: 0.5699\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.830 - ETA: 0s - loss: 0.181 - 0s 836us/step - loss: 0.1454 - val_loss: 0.4063\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.140 - 0s 817us/step - loss: 0.1360 - val_loss: 0.4076\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.150 - 0s 845us/step - loss: 0.1339 - val_loss: 0.5134\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.104 - 0s 845us/step - loss: 0.1056 - val_loss: 0.3803\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.061 - 0s 902us/step - loss: 0.1058 - val_loss: 0.5992\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.086 - 0s 788us/step - loss: 0.0873 - val_loss: 0.6535\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.120 - 0s 798us/step - loss: 0.1041 - val_loss: 0.6827\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.089 - 0s 817us/step - loss: 0.0750 - val_loss: 0.7375\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.085 - 0s 779us/step - loss: 0.0800 - val_loss: 0.7301\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.070 - 0s 798us/step - loss: 0.0711 - val_loss: 0.8809\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.076 - 0s 836us/step - loss: 0.0738 - val_loss: 0.7999\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.050 - 0s 855us/step - loss: 0.0866 - val_loss: 0.6601\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.067 - 0s 760us/step - loss: 0.0647 - val_loss: 1.0263\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.087 - 0s 750us/step - loss: 0.0780 - val_loss: 0.7829\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.069 - 0s 807us/step - loss: 0.0666 - val_loss: 0.9102\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.064 - 0s 855us/step - loss: 0.0624 - val_loss: 1.0340\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 8.1477e-0 - ETA: 0s - loss: 0.0674    - 0s 798us/step - loss: 0.0555 - val_loss: 0.7569\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.059 - 0s 845us/step - loss: 0.0589 - val_loss: 1.4380\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.263 - ETA: 0s - loss: 0.084 - 0s 845us/step - loss: 0.0724 - val_loss: 0.9833\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.073 - 0s 959us/step - loss: 0.0697 - val_loss: 0.9926\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.032 - 0s 902us/step - loss: 0.0601 - val_loss: 0.8549\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.053 - 0s 836us/step - loss: 0.0452 - val_loss: 0.8841\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.052 - 0s 855us/step - loss: 0.0509 - val_loss: 1.0744\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.036 - 0s 826us/step - loss: 0.0428 - val_loss: 0.9147\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.046 - 0s 921us/step - loss: 0.0436 - val_loss: 0.8856\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.046 - 0s 807us/step - loss: 0.0426 - val_loss: 0.7849\n",
      "Epoch 00035: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.44 - ETA: 0s - loss: 1.0161 - 1s 8ms/step - loss: 0.7651 - val_loss: 0.4937\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.146 - 0s 1ms/step - loss: 0.5472 - val_loss: 0.6642\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.422 - 0s 855us/step - loss: 0.4436 - val_loss: 0.7446\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.909 - ETA: 0s - loss: 0.585 - 0s 978us/step - loss: 0.4389 - val_loss: 0.7830\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.225 - 0s 845us/step - loss: 0.3015 - val_loss: 0.7638\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.345 - 0s 826us/step - loss: 0.2990 - val_loss: 0.8316\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.139 - 0s 1ms/step - loss: 0.3351 - val_loss: 0.6715\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.173 - 0s 931us/step - loss: 0.2496 - val_loss: 0.7350\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.223 - 0s 817us/step - loss: 0.2053 - val_loss: 0.6620\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.158 - 0s 931us/step - loss: 0.1911 - val_loss: 0.7669\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.522 - ETA: 0s - loss: 0.150 - 0s 950us/step - loss: 0.1673 - val_loss: 0.6774\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.214 - 0s 874us/step - loss: 0.1787 - val_loss: 0.7737\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.680 - ETA: 0s - loss: 0.149 - 0s 865us/step - loss: 0.1405 - val_loss: 0.7152\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.076 - 0s 912us/step - loss: 0.0985 - val_loss: 0.7237\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.067 - 0s 855us/step - loss: 0.1134 - val_loss: 0.8329\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.099 - 0s 997us/step - loss: 0.1003 - val_loss: 0.8196\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.097 - 0s 959us/step - loss: 0.0873 - val_loss: 0.9199\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.073 - 0s 845us/step - loss: 0.0626 - val_loss: 0.8971\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.046 - 0s 817us/step - loss: 0.0607 - val_loss: 0.8485\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.078 - 0s 912us/step - loss: 0.0756 - val_loss: 0.8999\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.057 - 0s 826us/step - loss: 0.0609 - val_loss: 0.9501\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.067 - 0s 750us/step - loss: 0.0619 - val_loss: 1.0992\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.051 - 0s 826us/step - loss: 0.0477 - val_loss: 0.7447\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.040 - 0s 779us/step - loss: 0.0488 - val_loss: 1.1362\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.069 - 0s 789us/step - loss: 0.0606 - val_loss: 0.7059\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.040 - 0s 788us/step - loss: 0.0445 - val_loss: 0.9316\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.043 - 0s 883us/step - loss: 0.0424 - val_loss: 0.5680\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.045 - 0s 798us/step - loss: 0.0423 - val_loss: 0.8653\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.034 - 0s 731us/step - loss: 0.0346 - val_loss: 0.6689\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.041 - 0s 846us/step - loss: 0.0420 - val_loss: 0.8035\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.030 - 0s 845us/step - loss: 0.0315 - val_loss: 0.8276\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 11\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 2.57 - ETA: 0s - loss: 0.7673 - 1s 8ms/step - loss: 0.6408 - val_loss: 0.8578\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.371 - 0s 788us/step - loss: 0.3826 - val_loss: 1.1306\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.299 - 0s 817us/step - loss: 0.3231 - val_loss: 0.9381\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.295 - 0s 779us/step - loss: 0.2755 - val_loss: 0.9092\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.227 - 0s 1ms/step - loss: 0.2594 - val_loss: 0.8543\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.193 - 0s 798us/step - loss: 0.2272 - val_loss: 0.6675\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.185 - 0s 788us/step - loss: 0.1848 - val_loss: 0.8371\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.203 - 0s 864us/step - loss: 0.1900 - val_loss: 0.7413\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.169 - 0s 788us/step - loss: 0.1623 - val_loss: 0.7255\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.125 - 0s 978us/step - loss: 0.1675 - val_loss: 0.6291\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.164 - 0s 864us/step - loss: 0.1544 - val_loss: 0.6092\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.431 - ETA: 0s - loss: 0.130 - 0s 893us/step - loss: 0.1370 - val_loss: 0.7243\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.133 - 0s 940us/step - loss: 0.1367 - val_loss: 0.6958\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.104 - 0s 912us/step - loss: 0.1178 - val_loss: 0.5939\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.128 - 0s 836us/step - loss: 0.1223 - val_loss: 0.5653\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.105 - 0s 912us/step - loss: 0.1044 - val_loss: 0.6555\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.101 - 0s 883us/step - loss: 0.1039 - val_loss: 0.5988\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.123 - 0s 903us/step - loss: 0.1164 - val_loss: 0.5541\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.123 - 0s 893us/step - loss: 0.1096 - val_loss: 0.5613\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.088 - 0s 2ms/step - loss: 0.0895 - val_loss: 0.5815\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.080 - 0s 940us/step - loss: 0.0852 - val_loss: 0.6045\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.081 - 0s 845us/step - loss: 0.0799 - val_loss: 0.6161\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.094 - 0s 826us/step - loss: 0.0820 - val_loss: 0.4595\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.088 - 0s 912us/step - loss: 0.0829 - val_loss: 0.5725\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.058 - 0s 940us/step - loss: 0.0703 - val_loss: 0.6473\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.060 - 0s 836us/step - loss: 0.0640 - val_loss: 0.4442\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.058 - 0s 817us/step - loss: 0.0643 - val_loss: 0.5783\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.069 - 0s 874us/step - loss: 0.0691 - val_loss: 0.5449\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.060 - 0s 950us/step - loss: 0.0585 - val_loss: 0.4628\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.048 - 0s 845us/step - loss: 0.0556 - val_loss: 0.5862\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.063 - 0s 931us/step - loss: 0.0561 - val_loss: 0.7411\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.050 - 0s 779us/step - loss: 0.0530 - val_loss: 0.5111\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.053 - 0s 883us/step - loss: 0.0560 - val_loss: 0.6128\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.051 - 0s 893us/step - loss: 0.0505 - val_loss: 0.4842\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.052 - 0s 779us/step - loss: 0.0561 - val_loss: 0.5060\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.048 - 0s 788us/step - loss: 0.0470 - val_loss: 0.5953\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.037 - 0s 845us/step - loss: 0.0348 - val_loss: 0.4784\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.051 - 0s 788us/step - loss: 0.0493 - val_loss: 0.4365\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.029 - 0s 836us/step - loss: 0.0368 - val_loss: 0.5123\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.045 - 0s 864us/step - loss: 0.0445 - val_loss: 0.3469\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.050 - 0s 826us/step - loss: 0.0472 - val_loss: 0.6331\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.037 - 0s 855us/step - loss: 0.0394 - val_loss: 0.5531\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.027 - 0s 836us/step - loss: 0.0292 - val_loss: 0.4866\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.045 - 0s 788us/step - loss: 0.0417 - val_loss: 0.5276\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.032 - 0s 826us/step - loss: 0.0339 - val_loss: 0.6576\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.035 - 0s 864us/step - loss: 0.0350 - val_loss: 0.6665\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.030 - 0s 760us/step - loss: 0.0291 - val_loss: 0.5211\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.039 - 0s 893us/step - loss: 0.0377 - val_loss: 0.6021\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.045 - 0s 817us/step - loss: 0.0421 - val_loss: 0.4381\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.034 - 0s 750us/step - loss: 0.0330 - val_loss: 0.5992\n",
      "(105,) (15,)\n",
      "model compiled 12\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 2.00 - ETA: 0s - loss: 0.6909 - 1s 8ms/step - loss: 0.5775 - val_loss: 1.1895\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 0.292 - 0s 827us/step - loss: 0.3181 - val_loss: 1.3693\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.507 - ETA: 0s - loss: 0.253 - 0s 883us/step - loss: 0.2848 - val_loss: 1.3940\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.271 - 0s 1ms/step - loss: 0.2653 - val_loss: 1.6749\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.174 - 0s 855us/step - loss: 0.2183 - val_loss: 1.8098\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.208 - 0s 855us/step - loss: 0.1822 - val_loss: 1.7206\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.176 - 0s 865us/step - loss: 0.1667 - val_loss: 2.4270\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.158 - 0s 836us/step - loss: 0.1665 - val_loss: 2.1082\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.127 - 0s 903us/step - loss: 0.1426 - val_loss: 2.1135\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.136 - 0s 845us/step - loss: 0.1402 - val_loss: 2.3253\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.126 - 0s 836us/step - loss: 0.1262 - val_loss: 2.1845\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.127 - 0s 827us/step - loss: 0.1219 - val_loss: 2.1512\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.119 - 0s 807us/step - loss: 0.1152 - val_loss: 2.9841\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.111 - 0s 864us/step - loss: 0.1175 - val_loss: 2.1890\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.089 - 0s 826us/step - loss: 0.1073 - val_loss: 2.5522\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.103 - 0s 788us/step - loss: 0.1001 - val_loss: 2.6200\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.090 - 0s 826us/step - loss: 0.0857 - val_loss: 2.5033\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.073 - 0s 788us/step - loss: 0.0887 - val_loss: 3.0331\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.118 - 0s 817us/step - loss: 0.0981 - val_loss: 2.2321\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.082 - 0s 817us/step - loss: 0.0794 - val_loss: 2.6741\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.076 - 0s 855us/step - loss: 0.0773 - val_loss: 2.5608\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.068 - 0s 864us/step - loss: 0.0656 - val_loss: 2.4309\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.068 - 0s 817us/step - loss: 0.0677 - val_loss: 2.6948\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.063 - 0s 950us/step - loss: 0.0695 - val_loss: 2.7674\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.058 - 0s 940us/step - loss: 0.0621 - val_loss: 2.9606\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.059 - 0s 997us/step - loss: 0.0595 - val_loss: 2.5289\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.051 - 0s 864us/step - loss: 0.0638 - val_loss: 2.6993\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.061 - 0s 883us/step - loss: 0.0615 - val_loss: 2.0776\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.064 - 0s 874us/step - loss: 0.0641 - val_loss: 2.7850\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.069 - 0s 864us/step - loss: 0.0558 - val_loss: 3.0623\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.067 - 0s 826us/step - loss: 0.0612 - val_loss: 2.2547\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 13\n",
      "Model: \"sequential_1\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.36 - ETA: 2s - loss: 1.0387 - ETA: 1s - loss: 0.849 - ETA: 1s - loss: 0.740 - ETA: 0s - loss: 0.823 - ETA: 0s - loss: 0.733 - ETA: 0s - loss: 0.680 - 1s 11ms/step - loss: 0.6744 - val_loss: 0.5975\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.547 - ETA: 0s - loss: 0.670 - ETA: 0s - loss: 0.594 - ETA: 0s - loss: 0.520 - ETA: 0s - loss: 0.518 - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.501 - 1s 6ms/step - loss: 0.5004 - val_loss: 0.7239\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.389 - ETA: 0s - loss: 0.538 - ETA: 0s - loss: 0.546 - ETA: 0s - loss: 0.519 - ETA: 0s - loss: 0.491 - 0s 5ms/step - loss: 0.4549 - val_loss: 0.8660\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.462 - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.397 - ETA: 0s - loss: 0.516 - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.430 - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.409 - 1s 8ms/step - loss: 0.4005 - val_loss: 0.8831\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.355 - 1s 6ms/step - loss: 0.3646 - val_loss: 0.9247\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.310 - 0s 3ms/step - loss: 0.3362 - val_loss: 1.1193\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.337 - 0s 912us/step - loss: 0.3242 - val_loss: 1.4799\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.310 - 0s 864us/step - loss: 0.2925 - val_loss: 1.2317\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.266 - 0s 864us/step - loss: 0.2887 - val_loss: 1.3328\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.245 - 0s 855us/step - loss: 0.2669 - val_loss: 1.3634\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.541 - ETA: 0s - loss: 0.250 - 0s 760us/step - loss: 0.2397 - val_loss: 1.4508\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.200 - 0s 912us/step - loss: 0.2045 - val_loss: 1.4628\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.210 - 0s 864us/step - loss: 0.2155 - val_loss: 1.7333\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.149 - 0s 788us/step - loss: 0.1640 - val_loss: 1.5504\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.215 - 0s 864us/step - loss: 0.1876 - val_loss: 1.3915\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.134 - 0s 807us/step - loss: 0.1547 - val_loss: 1.4817\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.163 - 0s 931us/step - loss: 0.1492 - val_loss: 1.5531\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.153 - 0s 959us/step - loss: 0.1493 - val_loss: 1.4442\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.117 - 0s 855us/step - loss: 0.1275 - val_loss: 1.6917\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.083 - 0s 855us/step - loss: 0.1220 - val_loss: 1.5362\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.114 - 0s 855us/step - loss: 0.1223 - val_loss: 1.5088\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.120 - 0s 807us/step - loss: 0.1136 - val_loss: 1.9008\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.128 - 0s 845us/step - loss: 0.1268 - val_loss: 1.4926\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.114 - 0s 836us/step - loss: 0.1057 - val_loss: 1.5022\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.103 - 0s 826us/step - loss: 0.1015 - val_loss: 1.6932\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.072 - 0s 798us/step - loss: 0.0896 - val_loss: 1.6930\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.099 - 0s 931us/step - loss: 0.0899 - val_loss: 1.5739\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.081 - 0s 912us/step - loss: 0.0822 - val_loss: 1.8179\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.075 - 0s 959us/step - loss: 0.0720 - val_loss: 1.7519\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.059 - 0s 836us/step - loss: 0.0705 - val_loss: 1.5670\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.083 - 0s 836us/step - loss: 0.0769 - val_loss: 1.7905\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 14\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 1.90 - ETA: 0s - loss: 0.6830 - 1s 8ms/step - loss: 0.7725 - val_loss: 0.6070\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 0.412 - 0s 817us/step - loss: 0.5630 - val_loss: 0.7394\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.795 - ETA: 0s - loss: 0.489 - 0s 845us/step - loss: 0.4905 - val_loss: 0.7136\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.907 - ETA: 0s - loss: 0.318 - 0s 1ms/step - loss: 0.4291 - val_loss: 0.9333\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.392 - 0s 912us/step - loss: 0.4005 - val_loss: 1.3453\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.674 - ETA: 0s - loss: 0.234 - 0s 893us/step - loss: 0.3275 - val_loss: 1.4050\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.249 - 0s 912us/step - loss: 0.3198 - val_loss: 1.8554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.662 - ETA: 0s - loss: 0.265 - 0s 817us/step - loss: 0.2866 - val_loss: 1.8940\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.256 - 0s 750us/step - loss: 0.2440 - val_loss: 2.0831\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.204 - 0s 864us/step - loss: 0.2436 - val_loss: 2.4123\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.253 - 0s 703us/step - loss: 0.2518 - val_loss: 2.4463\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.795 - ETA: 0s - loss: 0.212 - 0s 760us/step - loss: 0.2210 - val_loss: 2.6001\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.227 - 0s 769us/step - loss: 0.2075 - val_loss: 3.3424\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.069 - ETA: 0s - loss: 0.205 - 0s 817us/step - loss: 0.2004 - val_loss: 2.8172\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.182 - 0s 845us/step - loss: 0.1876 - val_loss: 2.9397\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.198 - 0s 817us/step - loss: 0.1916 - val_loss: 3.3643\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.200 - 0s 836us/step - loss: 0.1864 - val_loss: 3.6560\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.196 - 0s 836us/step - loss: 0.1664 - val_loss: 3.1276\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.176 - 0s 798us/step - loss: 0.1753 - val_loss: 3.7044\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.143 - 0s 845us/step - loss: 0.1712 - val_loss: 3.4580\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.159 - 0s 893us/step - loss: 0.1435 - val_loss: 3.5044\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.161 - 0s 779us/step - loss: 0.1392 - val_loss: 3.5008\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.134 - 0s 779us/step - loss: 0.1396 - val_loss: 3.8342\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.136 - 0s 902us/step - loss: 0.1290 - val_loss: 3.7517\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.105 - 0s 2ms/step - loss: 0.1233 - val_loss: 3.5584\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.129 - 0s 855us/step - loss: 0.1337 - val_loss: 3.8752\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.142 - 0s 874us/step - loss: 0.1265 - val_loss: 4.2751\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.105 - 0s 1ms/step - loss: 0.1213 - val_loss: 3.4401\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.140 - 0s 798us/step - loss: 0.1255 - val_loss: 3.9633\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.122 - 0s 788us/step - loss: 0.1225 - val_loss: 3.3349\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.113 - 0s 722us/step - loss: 0.1052 - val_loss: 3.9013\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 15\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.23 - ETA: 0s - loss: 0.8374 - 1s 8ms/step - loss: 0.6861 - val_loss: 0.6880\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.641 - 0s 845us/step - loss: 0.5351 - val_loss: 0.5414\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.456 - 0s 788us/step - loss: 0.4722 - val_loss: 0.5668\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.381 - 0s 779us/step - loss: 0.4347 - val_loss: 0.6808\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.446 - 0s 874us/step - loss: 0.3923 - val_loss: 0.6365\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.361 - 0s 874us/step - loss: 0.3657 - val_loss: 0.6579\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.539 - ETA: 0s - loss: 0.369 - 0s 855us/step - loss: 0.3345 - val_loss: 0.6729\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.231 - 0s 902us/step - loss: 0.2831 - val_loss: 0.6067\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.254 - 0s 826us/step - loss: 0.2701 - val_loss: 0.6008\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.260 - 0s 817us/step - loss: 0.2749 - val_loss: 0.6993\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.278 - 0s 855us/step - loss: 0.2517 - val_loss: 0.6541\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.246 - 0s 931us/step - loss: 0.2214 - val_loss: 0.8159\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.161 - 0s 826us/step - loss: 0.2016 - val_loss: 0.5998\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.893 - ETA: 0s - loss: 0.174 - 0s 903us/step - loss: 0.2016 - val_loss: 0.6673\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.168 - 0s 741us/step - loss: 0.1706 - val_loss: 0.6760\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.198 - 0s 903us/step - loss: 0.1866 - val_loss: 0.6575\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.149 - 0s 921us/step - loss: 0.1576 - val_loss: 0.5860\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.134 - 0s 1ms/step - loss: 0.1339 - val_loss: 0.6557\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.154 - 0s 912us/step - loss: 0.1420 - val_loss: 0.7114\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.422 - ETA: 0s - loss: 0.147 - 0s 798us/step - loss: 0.1544 - val_loss: 0.6972\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.138 - 0s 883us/step - loss: 0.1327 - val_loss: 0.6402\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.128 - 0s 988us/step - loss: 0.1269 - val_loss: 0.6255\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.130 - 0s 817us/step - loss: 0.1222 - val_loss: 0.6268\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.109 - 0s 874us/step - loss: 0.1087 - val_loss: 0.6369\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.097 - 0s 874us/step - loss: 0.1050 - val_loss: 0.6889\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.107 - 0s 798us/step - loss: 0.1007 - val_loss: 0.7065\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.118 - 0s 827us/step - loss: 0.1186 - val_loss: 0.5736\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.097 - 0s 817us/step - loss: 0.1085 - val_loss: 0.6558\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.093 - 0s 788us/step - loss: 0.1029 - val_loss: 0.5933\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.097 - 0s 760us/step - loss: 0.0929 - val_loss: 0.5941\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.086 - 0s 808us/step - loss: 0.0869 - val_loss: 0.5278\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.102 - 0s 817us/step - loss: 0.1021 - val_loss: 0.6664\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.090 - 0s 855us/step - loss: 0.0848 - val_loss: 0.5626\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.086 - 0s 855us/step - loss: 0.0926 - val_loss: 0.6410\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.077 - 0s 750us/step - loss: 0.0819 - val_loss: 0.4622\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.087 - 0s 779us/step - loss: 0.0890 - val_loss: 0.5597\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.064 - 0s 836us/step - loss: 0.0665 - val_loss: 0.6431\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - 0s 731us/step - loss: 0.0707 - val_loss: 0.5713\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.062 - 0s 836us/step - loss: 0.0647 - val_loss: 0.6447\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.087 - 0s 798us/step - loss: 0.0821 - val_loss: 0.5626\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.072 - 0s 798us/step - loss: 0.0715 - val_loss: 0.5205\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.055 - 0s 769us/step - loss: 0.0573 - val_loss: 0.5242\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.066 - 0s 769us/step - loss: 0.0657 - val_loss: 0.4515\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - 0s 1ms/step - loss: 0.0641 - val_loss: 0.4707\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.053 - 0s 855us/step - loss: 0.0579 - val_loss: 0.4997\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.064 - 0s 836us/step - loss: 0.0630 - val_loss: 0.5521\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.047 - 0s 760us/step - loss: 0.0508 - val_loss: 0.5106\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.050 - 0s 883us/step - loss: 0.0480 - val_loss: 0.4941\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.048 - 0s 846us/step - loss: 0.0517 - val_loss: 0.4968\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.047 - 0s 855us/step - loss: 0.0446 - val_loss: 0.5375\n",
      "(105,) (15,)\n",
      "model compiled 16\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.26 - ETA: 0s - loss: 0.6074 - 1s 8ms/step - loss: 0.7167 - val_loss: 0.4994\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.594 - 0s 931us/step - loss: 0.5219 - val_loss: 0.8903\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.547 - 0s 788us/step - loss: 0.4938 - val_loss: 0.6931\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.496 - 0s 855us/step - loss: 0.4485 - val_loss: 0.6571\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.640 - ETA: 0s - loss: 0.451 - 0s 855us/step - loss: 0.3871 - val_loss: 0.7220\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.369 - 0s 902us/step - loss: 0.3586 - val_loss: 0.7688\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.341 - 0s 940us/step - loss: 0.3409 - val_loss: 0.5801\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.286 - 0s 836us/step - loss: 0.3200 - val_loss: 0.6054\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.307 - 0s 902us/step - loss: 0.2933 - val_loss: 0.7011\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.261 - 0s 893us/step - loss: 0.2597 - val_loss: 0.6596\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.235 - 0s 931us/step - loss: 0.2490 - val_loss: 0.5967\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.188 - 0s 864us/step - loss: 0.2477 - val_loss: 0.6490\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.265 - 0s 959us/step - loss: 0.2403 - val_loss: 0.6180\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.448 - ETA: 0s - loss: 0.224 - 0s 826us/step - loss: 0.2212 - val_loss: 0.6886\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.204 - 0s 883us/step - loss: 0.2036 - val_loss: 0.6808\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.465 - ETA: 0s - loss: 0.196 - 0s 893us/step - loss: 0.1891 - val_loss: 0.7480\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.241 - 0s 978us/step - loss: 0.1922 - val_loss: 0.5042\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.190 - 0s 902us/step - loss: 0.1860 - val_loss: 0.6502\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.189 - 0s 883us/step - loss: 0.1788 - val_loss: 0.6215\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.174 - 0s 855us/step - loss: 0.1680 - val_loss: 0.5632\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.167 - 0s 950us/step - loss: 0.1563 - val_loss: 0.5882\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.141 - 0s 817us/step - loss: 0.1506 - val_loss: 0.7823\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.139 - 0s 912us/step - loss: 0.1435 - val_loss: 0.5825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.144 - 0s 893us/step - loss: 0.1423 - val_loss: 0.6724\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.132 - 0s 817us/step - loss: 0.1296 - val_loss: 0.4633\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.131 - 0s 893us/step - loss: 0.1346 - val_loss: 0.5873\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.120 - 0s 1ms/step - loss: 0.1174 - val_loss: 0.5231\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.118 - 0s 779us/step - loss: 0.1154 - val_loss: 0.5460\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.117 - 0s 788us/step - loss: 0.1141 - val_loss: 0.4830\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.105 - 0s 760us/step - loss: 0.1085 - val_loss: 0.6859\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.107 - 0s 741us/step - loss: 0.1231 - val_loss: 0.6571\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.102 - 0s 779us/step - loss: 0.0986 - val_loss: 0.5367\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.104 - 0s 817us/step - loss: 0.0941 - val_loss: 0.4493\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.096 - 0s 769us/step - loss: 0.0965 - val_loss: 0.5204\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.090 - 0s 807us/step - loss: 0.0964 - val_loss: 0.3989\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.089 - 0s 836us/step - loss: 0.0836 - val_loss: 0.5265\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.093 - 0s 826us/step - loss: 0.0977 - val_loss: 0.5406\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.089 - 0s 826us/step - loss: 0.0878 - val_loss: 0.3726\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.070 - 0s 798us/step - loss: 0.0775 - val_loss: 0.3946\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.070 - 0s 959us/step - loss: 0.0810 - val_loss: 0.3991\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.100 - 0s 769us/step - loss: 0.0920 - val_loss: 0.3361\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.073 - 0s 798us/step - loss: 0.0712 - val_loss: 0.4051\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.072 - 0s 779us/step - loss: 0.0777 - val_loss: 0.4670\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.079 - 0s 798us/step - loss: 0.0707 - val_loss: 0.4020\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.076 - 0s 826us/step - loss: 0.0716 - val_loss: 0.4323\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.070 - 0s 779us/step - loss: 0.0658 - val_loss: 0.4047\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.055 - 0s 836us/step - loss: 0.0640 - val_loss: 0.3726\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.067 - 0s 779us/step - loss: 0.0605 - val_loss: 0.4072\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.058 - 0s 883us/step - loss: 0.0556 - val_loss: 0.3173\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.083 - 0s 788us/step - loss: 0.0707 - val_loss: 0.3955\n",
      "(105,) (15,)\n",
      "model compiled 17\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.28 - ETA: 0s - loss: 0.7994 - 1s 8ms/step - loss: 0.7171 - val_loss: 0.4552\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.568 - 0s 959us/step - loss: 0.6318 - val_loss: 0.3692\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.562 - 0s 817us/step - loss: 0.5716 - val_loss: 0.4835\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.369 - ETA: 0s - loss: 0.430 - 0s 836us/step - loss: 0.5408 - val_loss: 0.5651\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.385 - 0s 874us/step - loss: 0.5025 - val_loss: 0.5262\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.506 - 0s 845us/step - loss: 0.4808 - val_loss: 0.5477\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.779 - ETA: 0s - loss: 0.516 - 0s 921us/step - loss: 0.4212 - val_loss: 0.5427\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.947 - ETA: 0s - loss: 0.431 - 0s 826us/step - loss: 0.3929 - val_loss: 0.8460\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.337 - 0s 826us/step - loss: 0.3614 - val_loss: 0.7217\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.321 - 0s 902us/step - loss: 0.3485 - val_loss: 0.8505\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.450 - ETA: 0s - loss: 0.329 - 0s 874us/step - loss: 0.3356 - val_loss: 1.1426\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.315 - 0s 864us/step - loss: 0.3186 - val_loss: 1.0953\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.507 - ETA: 0s - loss: 0.289 - 0s 931us/step - loss: 0.2825 - val_loss: 1.0877\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.274 - ETA: 0s - loss: 0.344 - 0s 836us/step - loss: 0.2944 - val_loss: 1.3469\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.299 - 0s 845us/step - loss: 0.2836 - val_loss: 0.9819\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.294 - 0s 826us/step - loss: 0.2655 - val_loss: 1.2706\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.305 - 0s 836us/step - loss: 0.2805 - val_loss: 1.2290\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.295 - 0s 921us/step - loss: 0.2462 - val_loss: 1.1971\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.315 - 0s 864us/step - loss: 0.2488 - val_loss: 1.7975\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.302 - 0s 979us/step - loss: 0.2567 - val_loss: 0.9328\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.287 - 0s 1ms/step - loss: 0.2436 - val_loss: 1.3469\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.234 - 0s 921us/step - loss: 0.2112 - val_loss: 1.2689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.197 - 0s 864us/step - loss: 0.2396 - val_loss: 1.2171\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.222 - 0s 769us/step - loss: 0.2200 - val_loss: 1.7393\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.203 - 0s 788us/step - loss: 0.2036 - val_loss: 1.5867\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.141 - 0s 836us/step - loss: 0.1826 - val_loss: 1.2951\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.205 - 0s 769us/step - loss: 0.2021 - val_loss: 1.7828\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.159 - 0s 836us/step - loss: 0.1963 - val_loss: 1.3252\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.143 - 0s 826us/step - loss: 0.1823 - val_loss: 1.5268\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.199 - 0s 807us/step - loss: 0.1798 - val_loss: 1.5650\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.147 - 0s 836us/step - loss: 0.1630 - val_loss: 1.5917\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.121 - 0s 902us/step - loss: 0.1756 - val_loss: 1.5309\n",
      "Epoch 00032: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 18\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.69 - ETA: 0s - loss: 0.7236 - 1s 8ms/step - loss: 0.6472 - val_loss: 0.7868\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.625 - 0s 826us/step - loss: 0.5327 - val_loss: 0.9295\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.541 - 0s 912us/step - loss: 0.4632 - val_loss: 0.7923\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.632 - ETA: 0s - loss: 0.316 - 0s 846us/step - loss: 0.4012 - val_loss: 0.8957\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.328 - 0s 874us/step - loss: 0.3695 - val_loss: 1.2857\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.683 - ETA: 0s - loss: 0.300 - 0s 808us/step - loss: 0.3428 - val_loss: 1.0615\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.296 - 0s 912us/step - loss: 0.3121 - val_loss: 0.7929\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.293 - 0s 827us/step - loss: 0.2914 - val_loss: 0.8682\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.260 - 0s 883us/step - loss: 0.2786 - val_loss: 1.0943\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.292 - 0s 845us/step - loss: 0.2665 - val_loss: 0.8196\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.247 - 0s 789us/step - loss: 0.2495 - val_loss: 0.8787\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.292 - 0s 845us/step - loss: 0.2411 - val_loss: 0.8525\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.219 - 0s 855us/step - loss: 0.2392 - val_loss: 0.7277\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.249 - 0s 807us/step - loss: 0.2120 - val_loss: 0.6732\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.225 - 0s 798us/step - loss: 0.2085 - val_loss: 1.0196\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.166 - 0s 931us/step - loss: 0.1768 - val_loss: 0.7780\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.218 - 0s 846us/step - loss: 0.1931 - val_loss: 0.7177\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.175 - 0s 789us/step - loss: 0.1875 - val_loss: 0.8576\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.186 - 0s 836us/step - loss: 0.1826 - val_loss: 0.7526\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.140 - 0s 845us/step - loss: 0.1497 - val_loss: 0.8756\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.170 - 0s 874us/step - loss: 0.1552 - val_loss: 0.8092\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.174 - 0s 845us/step - loss: 0.1692 - val_loss: 0.8656\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.495 - ETA: 0s - loss: 0.155 - 0s 893us/step - loss: 0.1392 - val_loss: 0.9656\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.122 - 0s 864us/step - loss: 0.1420 - val_loss: 0.7514\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.105 - 0s 884us/step - loss: 0.1365 - val_loss: 0.6367\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.128 - 0s 893us/step - loss: 0.1219 - val_loss: 0.9130\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.082 - 0s 845us/step - loss: 0.1115 - val_loss: 0.6119\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.121 - 0s 864us/step - loss: 0.1249 - val_loss: 0.8829\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.095 - 0s 817us/step - loss: 0.1064 - val_loss: 0.7828\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.110 - 0s 807us/step - loss: 0.1072 - val_loss: 0.7356\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.108 - 0s 817us/step - loss: 0.1128 - val_loss: 0.8640\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.459 - ETA: 0s - loss: 0.109 - 0s 884us/step - loss: 0.0978 - val_loss: 0.8218\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.113 - 0s 893us/step - loss: 0.1140 - val_loss: 0.8513\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.109 - 0s 940us/step - loss: 0.1132 - val_loss: 0.8613\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.094 - 0s 788us/step - loss: 0.0912 - val_loss: 0.8816\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.088 - 0s 902us/step - loss: 0.0866 - val_loss: 0.9037\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.085 - 0s 893us/step - loss: 0.0866 - val_loss: 0.8921\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.086 - 0s 902us/step - loss: 0.0823 - val_loss: 0.7891\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.073 - 0s 826us/step - loss: 0.0853 - val_loss: 0.9804\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.078 - 0s 827us/step - loss: 0.0737 - val_loss: 0.7861\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.083 - 0s 769us/step - loss: 0.0790 - val_loss: 0.9112\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.064 - 0s 779us/step - loss: 0.0669 - val_loss: 0.8188\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.087 - 0s 741us/step - loss: 0.0848 - val_loss: 1.1680\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.082 - 0s 769us/step - loss: 0.0764 - val_loss: 0.8992\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.066 - 0s 817us/step - loss: 0.0789 - val_loss: 1.0784\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.063 - 0s 779us/step - loss: 0.0653 - val_loss: 0.8222\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.055 - 0s 807us/step - loss: 0.0635 - val_loss: 0.9816\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.081 - 0s 845us/step - loss: 0.0803 - val_loss: 1.1392\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.065 - 0s 883us/step - loss: 0.0610 - val_loss: 1.1780\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.064 - 0s 893us/step - loss: 0.0606 - val_loss: 1.2777\n",
      "(105,) (15,)\n",
      "model compiled 19\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.59 - ETA: 0s - loss: 0.8007 - 1s 9ms/step - loss: 0.7187 - val_loss: 0.5353\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.682 - ETA: 0s - loss: 0.503 - 0s 826us/step - loss: 0.4325 - val_loss: 0.9717\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.338 - 0s 807us/step - loss: 0.3841 - val_loss: 1.0192\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.712 - ETA: 0s - loss: 0.397 - 0s 826us/step - loss: 0.3420 - val_loss: 1.0650\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.316 - 0s 950us/step - loss: 0.3019 - val_loss: 1.0905\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.251 - 0s 893us/step - loss: 0.2658 - val_loss: 1.1113\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.275 - 0s 826us/step - loss: 0.2623 - val_loss: 1.5738\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.275 - 0s 864us/step - loss: 0.2501 - val_loss: 1.2238\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.186 - 0s 855us/step - loss: 0.2184 - val_loss: 1.3205\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.202 - 0s 883us/step - loss: 0.2155 - val_loss: 1.2010\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.179 - 0s 808us/step - loss: 0.1987 - val_loss: 1.5755\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.174 - 0s 874us/step - loss: 0.1935 - val_loss: 1.2326\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.158 - 0s 1ms/step - loss: 0.1758 - val_loss: 1.2741\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.165 - 0s 731us/step - loss: 0.1776 - val_loss: 1.4375\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.165 - 0s 1ms/step - loss: 0.1594 - val_loss: 1.3606\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.171 - 0s 902us/step - loss: 0.1634 - val_loss: 1.2579\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.147 - 0s 865us/step - loss: 0.1469 - val_loss: 1.4932\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.163 - 0s 807us/step - loss: 0.1419 - val_loss: 1.2759\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.137 - 0s 779us/step - loss: 0.1401 - val_loss: 1.4916\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.133 - 0s 912us/step - loss: 0.1301 - val_loss: 1.3934\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.128 - 0s 874us/step - loss: 0.1279 - val_loss: 1.6166\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.339 - ETA: 0s - loss: 0.123 - 0s 903us/step - loss: 0.1372 - val_loss: 1.3776\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.139 - 0s 864us/step - loss: 0.1334 - val_loss: 1.7881\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.114 - 0s 902us/step - loss: 0.1186 - val_loss: 1.5011\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.117 - 0s 902us/step - loss: 0.1097 - val_loss: 1.4861\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.112 - 0s 978us/step - loss: 0.1106 - val_loss: 1.4355\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.096 - 0s 902us/step - loss: 0.1004 - val_loss: 1.8955\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.099 - 0s 836us/step - loss: 0.1040 - val_loss: 1.0864\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.089 - 0s 978us/step - loss: 0.0917 - val_loss: 1.6220\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.096 - 0s 817us/step - loss: 0.0909 - val_loss: 1.3511\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.087 - 0s 903us/step - loss: 0.1001 - val_loss: 1.7549\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 20\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.63 - ETA: 0s - loss: 0.7479 - 1s 8ms/step - loss: 0.7967 - val_loss: 0.5860\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.603 - 0s 817us/step - loss: 0.6112 - val_loss: 0.6002\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.400 - ETA: 0s - loss: 0.598 - 0s 788us/step - loss: 0.5189 - val_loss: 0.4218\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.516 - ETA: 0s - loss: 0.501 - 0s 807us/step - loss: 0.5022 - val_loss: 0.4780\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.463 - 0s 855us/step - loss: 0.4464 - val_loss: 0.5367\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.403 - 0s 845us/step - loss: 0.4219 - val_loss: 0.4373\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.958 - ETA: 0s - loss: 0.296 - 0s 817us/step - loss: 0.3796 - val_loss: 0.4074\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.783 - ETA: 0s - loss: 0.272 - 0s 836us/step - loss: 0.3600 - val_loss: 0.3652\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.371 - 0s 798us/step - loss: 0.3507 - val_loss: 0.3324\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.281 - 0s 817us/step - loss: 0.3262 - val_loss: 0.4936\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.316 - 0s 807us/step - loss: 0.3019 - val_loss: 0.3781\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.346 - 0s 836us/step - loss: 0.3042 - val_loss: 0.4701\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.474 - ETA: 0s - loss: 0.278 - 0s 807us/step - loss: 0.2653 - val_loss: 0.4972\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.281 - 0s 817us/step - loss: 0.2675 - val_loss: 0.4556\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.260 - 0s 826us/step - loss: 0.2235 - val_loss: 0.4974\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.480 - ETA: 0s - loss: 0.210 - 0s 798us/step - loss: 0.2185 - val_loss: 0.5276\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.248 - 0s 874us/step - loss: 0.2251 - val_loss: 0.6497\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.224 - 0s 893us/step - loss: 0.2065 - val_loss: 0.5703\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.629 - ETA: 0s - loss: 0.206 - 0s 817us/step - loss: 0.2078 - val_loss: 0.6528\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.178 - 0s 769us/step - loss: 0.1858 - val_loss: 0.7044\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.166 - 0s 836us/step - loss: 0.1847 - val_loss: 0.6845\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.170 - 0s 940us/step - loss: 0.1673 - val_loss: 0.7265\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.166 - 0s 826us/step - loss: 0.1645 - val_loss: 0.7505\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.191 - 0s 788us/step - loss: 0.1850 - val_loss: 0.7389\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.136 - 0s 807us/step - loss: 0.1559 - val_loss: 0.7289\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.167 - 0s 940us/step - loss: 0.1499 - val_loss: 0.8833\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.148 - 0s 912us/step - loss: 0.1480 - val_loss: 0.8522\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.141 - 0s 884us/step - loss: 0.1423 - val_loss: 0.9446\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.141 - 0s 931us/step - loss: 0.1472 - val_loss: 0.8854\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.129 - 0s 874us/step - loss: 0.1370 - val_loss: 1.0031\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.133 - 0s 855us/step - loss: 0.1291 - val_loss: 0.9866\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.131 - 0s 921us/step - loss: 0.1213 - val_loss: 1.0169\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.133 - 0s 921us/step - loss: 0.1221 - val_loss: 0.9460\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.109 - 0s 855us/step - loss: 0.1166 - val_loss: 0.9902\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.123 - 0s 902us/step - loss: 0.1205 - val_loss: 1.0591\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.106 - 0s 883us/step - loss: 0.1049 - val_loss: 1.1081\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.103 - 0s 931us/step - loss: 0.1075 - val_loss: 1.1677\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.110 - 0s 950us/step - loss: 0.1173 - val_loss: 1.2104\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.105 - 0s 969us/step - loss: 0.1021 - val_loss: 1.3022\n",
      "Epoch 00039: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 21\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.77 - ETA: 0s - loss: 0.9715 - 1s 8ms/step - loss: 0.8565 - val_loss: 0.4686\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.045 - ETA: 0s - loss: 0.669 - 0s 864us/step - loss: 0.6276 - val_loss: 0.6829\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.591 - 0s 845us/step - loss: 0.5855 - val_loss: 0.5057\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.539 - 0s 845us/step - loss: 0.5641 - val_loss: 0.6746\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.525 - ETA: 0s - loss: 0.530 - 0s 855us/step - loss: 0.5283 - val_loss: 0.6511\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.425 - 0s 845us/step - loss: 0.5240 - val_loss: 0.7351\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.478 - 0s 865us/step - loss: 0.4810 - val_loss: 0.6989\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.553 - ETA: 0s - loss: 0.473 - 0s 884us/step - loss: 0.4615 - val_loss: 0.9583\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.934 - ETA: 0s - loss: 0.480 - 0s 950us/step - loss: 0.4564 - val_loss: 0.9710\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.448 - 0s 865us/step - loss: 0.4257 - val_loss: 1.0503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.369 - 0s 941us/step - loss: 0.4122 - val_loss: 0.9833\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.333 - 0s 874us/step - loss: 0.3864 - val_loss: 0.9854\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.365 - 0s 883us/step - loss: 0.3578 - val_loss: 0.8929\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.310 - 0s 874us/step - loss: 0.3566 - val_loss: 0.9444\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.287 - 0s 836us/step - loss: 0.3430 - val_loss: 1.1234\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.342 - 0s 893us/step - loss: 0.3445 - val_loss: 1.0734\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.946 - ETA: 0s - loss: 0.296 - 0s 826us/step - loss: 0.3198 - val_loss: 1.1238\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.280 - 0s 874us/step - loss: 0.3123 - val_loss: 0.8745\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.636 - ETA: 0s - loss: 0.319 - 0s 798us/step - loss: 0.2949 - val_loss: 1.3558\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.300 - 0s 846us/step - loss: 0.2838 - val_loss: 1.0735\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.274 - 0s 845us/step - loss: 0.2690 - val_loss: 1.1230\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.207 - 0s 1ms/step - loss: 0.2532 - val_loss: 1.1248\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.238 - 0s 836us/step - loss: 0.2360 - val_loss: 0.9511\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.594 - ETA: 0s - loss: 0.274 - 0s 817us/step - loss: 0.2356 - val_loss: 0.9687\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.228 - 0s 826us/step - loss: 0.2413 - val_loss: 1.1044\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.221 - 0s 817us/step - loss: 0.2332 - val_loss: 1.2343\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.205 - 0s 788us/step - loss: 0.2233 - val_loss: 0.9283\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.203 - 0s 788us/step - loss: 0.2260 - val_loss: 1.1818\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.183 - 0s 883us/step - loss: 0.1971 - val_loss: 0.9949\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.203 - 0s 1ms/step - loss: 0.2015 - val_loss: 1.0632\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.189 - 0s 836us/step - loss: 0.1973 - val_loss: 0.8085\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 22\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.32 - ETA: 0s - loss: 0.6323 - 1s 8ms/step - loss: 0.7646 - val_loss: 0.4949\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.218 - ETA: 0s - loss: 0.258 - 0s 855us/step - loss: 0.6160 - val_loss: 0.5525\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.675 - 0s 798us/step - loss: 0.5851 - val_loss: 0.7688\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.820 - 0s 940us/step - loss: 0.5423 - val_loss: 0.7617\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.547 - ETA: 0s - loss: 0.346 - 0s 798us/step - loss: 0.4937 - val_loss: 0.6371\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.642 - 0s 845us/step - loss: 0.4982 - val_loss: 0.8598\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.696 - ETA: 0s - loss: 0.482 - 0s 807us/step - loss: 0.4110 - val_loss: 0.8746\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.387 - ETA: 0s - loss: 0.447 - 0s 855us/step - loss: 0.3732 - val_loss: 1.1590\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.405 - 0s 950us/step - loss: 0.3537 - val_loss: 1.3058\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.448 - 0s 893us/step - loss: 0.3593 - val_loss: 1.4881\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.248 - 0s 874us/step - loss: 0.2957 - val_loss: 1.0540\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.247 - 0s 845us/step - loss: 0.2720 - val_loss: 1.6539\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.230 - 0s 836us/step - loss: 0.2454 - val_loss: 1.4871\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.221 - 0s 978us/step - loss: 0.2195 - val_loss: 1.6944\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.218 - 0s 874us/step - loss: 0.2030 - val_loss: 1.1367\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.210 - 0s 817us/step - loss: 0.2001 - val_loss: 1.6190\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.136 - 0s 836us/step - loss: 0.1729 - val_loss: 2.3075\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.153 - 0s 846us/step - loss: 0.1519 - val_loss: 1.5385\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.162 - 0s 836us/step - loss: 0.1759 - val_loss: 2.0508\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.156 - 0s 988us/step - loss: 0.1407 - val_loss: 2.0327\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.105 - 0s 931us/step - loss: 0.1177 - val_loss: 2.0850\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.104 - 0s 874us/step - loss: 0.1117 - val_loss: 2.1278\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.109 - 0s 1ms/step - loss: 0.1124 - val_loss: 2.1126\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.104 - 0s 3ms/step - loss: 0.1137 - val_loss: 1.9910\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.094 - 0s 1ms/step - loss: 0.0935 - val_loss: 1.9455\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.089 - 0s 2ms/step - loss: 0.0983 - val_loss: 2.5304\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.092 - 0s 2ms/step - loss: 0.0939 - val_loss: 1.7190\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.078 - 0s 1ms/step - loss: 0.0945 - val_loss: 2.3014\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.116 - 0s 1ms/step - loss: 0.0906 - val_loss: 2.0368\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.083 - 0s 693us/step - loss: 0.0860 - val_loss: 2.3739\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.099 - 0s 969us/step - loss: 0.0822 - val_loss: 1.9257\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 23\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.58 - ETA: 0s - loss: 0.5328 - 1s 8ms/step - loss: 0.6684 - val_loss: 0.5261\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.504 - 0s 788us/step - loss: 0.4419 - val_loss: 0.6512\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.173 - ETA: 0s - loss: 0.412 - 0s 836us/step - loss: 0.3833 - val_loss: 0.5650\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.626 - ETA: 0s - loss: 0.334 - 0s 1ms/step - loss: 0.3334 - val_loss: 0.6204\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.234 - 0s 807us/step - loss: 0.3053 - val_loss: 0.4613\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.273 - 0s 798us/step - loss: 0.2506 - val_loss: 0.6463\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.275 - 0s 808us/step - loss: 0.2482 - val_loss: 0.5763\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.202 - 0s 864us/step - loss: 0.2030 - val_loss: 0.8529\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.216 - 0s 902us/step - loss: 0.1832 - val_loss: 0.5988\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.144 - 0s 940us/step - loss: 0.1822 - val_loss: 0.7123\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.135 - 0s 836us/step - loss: 0.1491 - val_loss: 0.8229\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.169 - 0s 903us/step - loss: 0.1628 - val_loss: 0.7382\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.116 - 0s 988us/step - loss: 0.1362 - val_loss: 0.9252\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.129 - 0s 845us/step - loss: 0.1142 - val_loss: 0.8526\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.103 - 0s 902us/step - loss: 0.0967 - val_loss: 1.0204\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.083 - 0s 855us/step - loss: 0.1082 - val_loss: 0.6923\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.104 - 0s 978us/step - loss: 0.1162 - val_loss: 0.9864\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.104 - 0s 836us/step - loss: 0.0885 - val_loss: 0.9708\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.091 - 0s 883us/step - loss: 0.0844 - val_loss: 1.2027\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.093 - 0s 902us/step - loss: 0.0915 - val_loss: 0.9055\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.072 - 0s 883us/step - loss: 0.0759 - val_loss: 1.0632\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.067 - 0s 921us/step - loss: 0.0768 - val_loss: 1.0448\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - 0s 826us/step - loss: 0.0706 - val_loss: 0.8430\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.078 - 0s 883us/step - loss: 0.0759 - val_loss: 1.6108\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.092 - 0s 836us/step - loss: 0.0804 - val_loss: 0.9780\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.067 - 0s 855us/step - loss: 0.0615 - val_loss: 0.9158\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.063 - 0s 922us/step - loss: 0.0637 - val_loss: 1.2596\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.055 - 0s 978us/step - loss: 0.0693 - val_loss: 1.0185\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.061 - 0s 912us/step - loss: 0.0657 - val_loss: 1.2286\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.055 - 0s 893us/step - loss: 0.0653 - val_loss: 1.3145\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.050 - 0s 1ms/step - loss: 0.0520 - val_loss: 1.1136\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.041 - 0s 912us/step - loss: 0.0495 - val_loss: 1.2276\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.057 - 0s 846us/step - loss: 0.0572 - val_loss: 1.2241\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.055 - 0s 940us/step - loss: 0.0519 - val_loss: 0.9646\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.035 - 0s 1ms/step - loss: 0.0496 - val_loss: 1.2707\n",
      "Epoch 00035: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 24\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.34 - ETA: 0s - loss: 0.6287 - 1s 8ms/step - loss: 0.6573 - val_loss: 0.5566\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.810 - ETA: 0s - loss: 0.547 - 0s 959us/step - loss: 0.4906 - val_loss: 0.5650\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.296 - 0s 1ms/step - loss: 0.4444 - val_loss: 0.5718\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.634 - ETA: 0s - loss: 0.408 - 0s 798us/step - loss: 0.3846 - val_loss: 0.5912\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.585 - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.323 - 0s 1ms/step - loss: 0.3285 - val_loss: 0.3943\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.308 - 0s 1ms/step - loss: 0.3000 - val_loss: 0.4954\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.266 - 0s 893us/step - loss: 0.2898 - val_loss: 0.3645\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.203 - 0s 883us/step - loss: 0.2817 - val_loss: 0.5322\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.982 - ETA: 0s - loss: 0.287 - 0s 941us/step - loss: 0.2451 - val_loss: 0.5852\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.185 - 0s 826us/step - loss: 0.2182 - val_loss: 0.5590\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.185 - 0s 836us/step - loss: 0.1938 - val_loss: 0.6194\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.197 - 0s 864us/step - loss: 0.1929 - val_loss: 0.6173\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.216 - 0s 950us/step - loss: 0.1920 - val_loss: 0.6984\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.154 - 0s 883us/step - loss: 0.1585 - val_loss: 0.4792\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.145 - 0s 883us/step - loss: 0.1622 - val_loss: 0.7991\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.153 - 0s 959us/step - loss: 0.1412 - val_loss: 0.6458\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.132 - 0s 826us/step - loss: 0.1320 - val_loss: 0.6412\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.119 - 0s 788us/step - loss: 0.1254 - val_loss: 0.6794\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.105 - 0s 921us/step - loss: 0.1172 - val_loss: 0.7438\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.105 - 0s 940us/step - loss: 0.1073 - val_loss: 0.7316\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.132 - 0s 1ms/step - loss: 0.1046 - val_loss: 0.8718\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.089 - 0s 1ms/step - loss: 0.0968 - val_loss: 0.7769\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.113 - 0s 912us/step - loss: 0.1144 - val_loss: 0.9662\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.085 - 0s 883us/step - loss: 0.0887 - val_loss: 0.7118\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.106 - 0s 921us/step - loss: 0.0935 - val_loss: 1.0867\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.080 - 0s 798us/step - loss: 0.0790 - val_loss: 0.7798\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.090 - 0s 893us/step - loss: 0.0907 - val_loss: 0.5483\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.086 - 0s 940us/step - loss: 0.0992 - val_loss: 1.2454\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.075 - 0s 978us/step - loss: 0.0779 - val_loss: 0.5274\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.788 - ETA: 0s - loss: 0.124 - 0s 940us/step - loss: 0.0953 - val_loss: 0.8872\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.071 - 0s 903us/step - loss: 0.0739 - val_loss: 0.7706\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.070 - 0s 893us/step - loss: 0.0637 - val_loss: 0.8752\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.077 - 0s 902us/step - loss: 0.0665 - val_loss: 0.9219\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.053 - 0s 912us/step - loss: 0.0639 - val_loss: 0.8888\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.060 - 0s 988us/step - loss: 0.0595 - val_loss: 0.7977\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.075 - 0s 950us/step - loss: 0.0651 - val_loss: 1.0318\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.067 - 0s 883us/step - loss: 0.0636 - val_loss: 0.7196\n",
      "Epoch 00037: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 25\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.16 - ETA: 0s - loss: 0.9440 - 1s 8ms/step - loss: 0.8016 - val_loss: 0.3531\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.369 - ETA: 0s - loss: 0.422 - 0s 969us/step - loss: 0.6657 - val_loss: 0.5052\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.408 - 0s 798us/step - loss: 0.5615 - val_loss: 0.5301\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.397 - 0s 836us/step - loss: 0.4826 - val_loss: 0.4715\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.181 - ETA: 0s - loss: 0.402 - 0s 893us/step - loss: 0.4457 - val_loss: 0.7304\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.288 - 0s 931us/step - loss: 0.3401 - val_loss: 0.4439\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.321 - 0s 883us/step - loss: 0.3375 - val_loss: 0.7724\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.244 - 0s 874us/step - loss: 0.2821 - val_loss: 0.7577\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.178 - 0s 864us/step - loss: 0.2598 - val_loss: 0.7829\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.316 - 0s 1ms/step - loss: 0.2388 - val_loss: 0.8468\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.256 - 0s 912us/step - loss: 0.2057 - val_loss: 0.9084\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.183 - 0s 931us/step - loss: 0.1678 - val_loss: 0.9313\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.093 - 0s 903us/step - loss: 0.1939 - val_loss: 0.8048\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.130 - 0s 940us/step - loss: 0.1699 - val_loss: 0.9709\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.136 - 0s 817us/step - loss: 0.1814 - val_loss: 1.0309\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.104 - 0s 826us/step - loss: 0.1406 - val_loss: 1.1508\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.106 - 0s 798us/step - loss: 0.1501 - val_loss: 0.8191\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.169 - 0s 864us/step - loss: 0.1435 - val_loss: 1.5111\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.114 - 0s 826us/step - loss: 0.1147 - val_loss: 0.8927\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.118 - 0s 864us/step - loss: 0.1086 - val_loss: 1.0858\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.108 - 0s 950us/step - loss: 0.1022 - val_loss: 0.9211\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.133 - 0s 912us/step - loss: 0.1024 - val_loss: 1.0622\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.104 - 0s 1ms/step - loss: 0.1012 - val_loss: 1.1958\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.575 - ETA: 0s - loss: 0.118 - 0s 741us/step - loss: 0.1046 - val_loss: 1.0277\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.101 - 0s 779us/step - loss: 0.0930 - val_loss: 0.9445\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.097 - 0s 798us/step - loss: 0.0975 - val_loss: 1.0637\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.087 - 0s 817us/step - loss: 0.0763 - val_loss: 1.1159\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.080 - 0s 978us/step - loss: 0.0717 - val_loss: 0.9130\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.066 - 0s 1ms/step - loss: 0.0880 - val_loss: 1.3606\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.056 - 0s 769us/step - loss: 0.0729 - val_loss: 0.8588\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.069 - 0s 1ms/step - loss: 0.0670 - val_loss: 1.1870\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 26\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.64 - ETA: 0s - loss: 0.7223 - 1s 8ms/step - loss: 0.6640 - val_loss: 0.8235\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.460 - 0s 940us/step - loss: 0.4141 - val_loss: 1.1374\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.553 - ETA: 0s - loss: 0.407 - 0s 798us/step - loss: 0.3865 - val_loss: 1.3245\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.379 - 0s 826us/step - loss: 0.3525 - val_loss: 1.4008\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.268 - 0s 912us/step - loss: 0.3407 - val_loss: 1.4369\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.295 - 0s 1ms/step - loss: 0.3125 - val_loss: 1.6858\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.879 - ETA: 0s - loss: 0.281 - 0s 855us/step - loss: 0.3040 - val_loss: 1.4653\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.294 - 0s 1ms/step - loss: 0.2782 - val_loss: 1.9234\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.243 - 0s 912us/step - loss: 0.2628 - val_loss: 1.9728\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.225 - 0s 902us/step - loss: 0.2366 - val_loss: 1.8547\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.420 - ETA: 0s - loss: 0.217 - 0s 988us/step - loss: 0.2287 - val_loss: 2.2713\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.225 - 0s 864us/step - loss: 0.2172 - val_loss: 1.7662\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.193 - 0s 903us/step - loss: 0.2123 - val_loss: 2.1546\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.187 - 0s 855us/step - loss: 0.1965 - val_loss: 1.8364\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.187 - 0s 864us/step - loss: 0.2048 - val_loss: 2.0467\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.184 - 0s 817us/step - loss: 0.1863 - val_loss: 2.7098\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.176 - 0s 826us/step - loss: 0.1814 - val_loss: 1.8500\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.151 - 0s 921us/step - loss: 0.1729 - val_loss: 2.6818\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.153 - 0s 940us/step - loss: 0.1544 - val_loss: 2.5495\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.156 - 0s 855us/step - loss: 0.1554 - val_loss: 2.4462\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.387 - ETA: 0s - loss: 0.112 - 0s 874us/step - loss: 0.1310 - val_loss: 2.2734\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.414 - ETA: 0s - loss: 0.122 - 0s 912us/step - loss: 0.1265 - val_loss: 2.4190\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.168 - 0s 884us/step - loss: 0.1384 - val_loss: 2.7564\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.113 - 0s 978us/step - loss: 0.1160 - val_loss: 1.6500\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.127 - 0s 883us/step - loss: 0.1132 - val_loss: 2.4783\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.072 - 0s 874us/step - loss: 0.1036 - val_loss: 2.0317\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.092 - 0s 940us/step - loss: 0.0965 - val_loss: 2.6446\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.119 - 0s 874us/step - loss: 0.1094 - val_loss: 2.7893\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.066 - 0s 864us/step - loss: 0.0902 - val_loss: 2.3447\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.092 - 0s 940us/step - loss: 0.0911 - val_loss: 2.5118\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.058 - 0s 874us/step - loss: 0.0763 - val_loss: 2.2488\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 27\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.75 - ETA: 0s - loss: 0.9698 - 1s 8ms/step - loss: 0.8192 - val_loss: 0.5153\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.188 - ETA: 0s - loss: 0.577 - 0s 817us/step - loss: 0.5497 - val_loss: 0.6038\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.543 - 0s 922us/step - loss: 0.5025 - val_loss: 0.5872\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.487 - 0s 817us/step - loss: 0.4513 - val_loss: 0.6507\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.459 - 0s 798us/step - loss: 0.4259 - val_loss: 0.7311\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.356 - 0s 807us/step - loss: 0.3625 - val_loss: 0.7188\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.308 - 0s 855us/step - loss: 0.3658 - val_loss: 0.7765\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.110 - ETA: 0s - loss: 0.331 - 0s 836us/step - loss: 0.3180 - val_loss: 0.8890\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.339 - 0s 845us/step - loss: 0.3060 - val_loss: 0.7181\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.319 - 0s 788us/step - loss: 0.2885 - val_loss: 0.7912\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.261 - 0s 902us/step - loss: 0.2668 - val_loss: 0.7491\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.228 - 0s 931us/step - loss: 0.2478 - val_loss: 0.7062\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.465 - ETA: 0s - loss: 0.278 - 0s 826us/step - loss: 0.2447 - val_loss: 0.8115\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.208 - 0s 845us/step - loss: 0.2158 - val_loss: 0.8231\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.204 - 0s 817us/step - loss: 0.2074 - val_loss: 0.9233\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.247 - 0s 874us/step - loss: 0.2022 - val_loss: 0.9085\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.931 - ETA: 0s - loss: 0.202 - 0s 864us/step - loss: 0.1972 - val_loss: 0.8146\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.198 - 0s 836us/step - loss: 0.1839 - val_loss: 1.1395\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.156 - 0s 864us/step - loss: 0.1700 - val_loss: 0.9274\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.175 - 0s 769us/step - loss: 0.1728 - val_loss: 0.9489\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.177 - 0s 798us/step - loss: 0.1729 - val_loss: 1.0968\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.171 - 0s 817us/step - loss: 0.1570 - val_loss: 1.0845\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.158 - 0s 921us/step - loss: 0.1448 - val_loss: 1.0531\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.085 - 0s 959us/step - loss: 0.1337 - val_loss: 0.7399\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.163 - 0s 883us/step - loss: 0.1456 - val_loss: 1.2074\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.075 - 0s 836us/step - loss: 0.1264 - val_loss: 0.9018\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.095 - 0s 827us/step - loss: 0.1346 - val_loss: 0.8089\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.098 - 0s 883us/step - loss: 0.1267 - val_loss: 1.3333\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.130 - 0s 864us/step - loss: 0.1266 - val_loss: 0.9661\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.143 - 0s 864us/step - loss: 0.1234 - val_loss: 1.1572\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.794 - ETA: 0s - loss: 0.126 - 0s 931us/step - loss: 0.1150 - val_loss: 0.8945\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 28\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.19 - ETA: 0s - loss: 0.5036 - 1s 8ms/step - loss: 0.4995 - val_loss: 1.0925\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.400 - 0s 902us/step - loss: 0.3962 - val_loss: 1.2582\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.795 - ETA: 0s - loss: 0.407 - 0s 845us/step - loss: 0.3427 - val_loss: 1.0267\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.252 - 0s 817us/step - loss: 0.2901 - val_loss: 1.1955\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.193 - 0s 836us/step - loss: 0.2704 - val_loss: 1.1104\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.230 - 0s 807us/step - loss: 0.2400 - val_loss: 1.2304\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.640 - ETA: 0s - loss: 0.163 - 0s 798us/step - loss: 0.2086 - val_loss: 1.1745\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.221 - 0s 798us/step - loss: 0.2015 - val_loss: 1.4187\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.198 - 0s 826us/step - loss: 0.1870 - val_loss: 1.4438\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.203 - 0s 845us/step - loss: 0.1837 - val_loss: 1.3948\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.177 - 0s 931us/step - loss: 0.1561 - val_loss: 1.3125\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.143 - 0s 864us/step - loss: 0.1425 - val_loss: 1.4400\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.134 - 0s 845us/step - loss: 0.1314 - val_loss: 1.3244\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.107 - 0s 902us/step - loss: 0.1290 - val_loss: 1.3682\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.132 - 0s 912us/step - loss: 0.1354 - val_loss: 1.3245\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.125 - 0s 845us/step - loss: 0.1197 - val_loss: 1.3641\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.128 - 0s 864us/step - loss: 0.1116 - val_loss: 1.3576\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.099 - 0s 826us/step - loss: 0.1067 - val_loss: 1.4536\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.101 - 0s 836us/step - loss: 0.1017 - val_loss: 1.4104\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.113 - 0s 846us/step - loss: 0.1001 - val_loss: 1.6550\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.122 - 0s 779us/step - loss: 0.1127 - val_loss: 1.0519\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.097 - 0s 883us/step - loss: 0.1096 - val_loss: 1.3656\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.079 - 0s 779us/step - loss: 0.0907 - val_loss: 1.5633\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.105 - 0s 883us/step - loss: 0.0956 - val_loss: 1.7340\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.098 - 0s 826us/step - loss: 0.0914 - val_loss: 1.4078\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.075 - 0s 883us/step - loss: 0.0774 - val_loss: 1.9100\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.100 - 0s 855us/step - loss: 0.0829 - val_loss: 2.0263\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.088 - 0s 883us/step - loss: 0.0969 - val_loss: 1.2312\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.072 - 0s 931us/step - loss: 0.0761 - val_loss: 1.2224\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.080 - 0s 893us/step - loss: 0.0869 - val_loss: 1.2285\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.066 - 0s 836us/step - loss: 0.0779 - val_loss: 1.4998\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.062 - 0s 807us/step - loss: 0.0624 - val_loss: 1.2951\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.067 - 0s 931us/step - loss: 0.0611 - val_loss: 1.5775\n",
      "Epoch 00033: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 29\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.78 - ETA: 0s - loss: 0.6422 - 1s 8ms/step - loss: 0.5513 - val_loss: 1.0718\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.319 - 0s 826us/step - loss: 0.3945 - val_loss: 1.5253\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.232 - 0s 855us/step - loss: 0.3660 - val_loss: 1.2711\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.323 - 0s 959us/step - loss: 0.3179 - val_loss: 1.1837\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.707 - ETA: 0s - loss: 0.302 - 0s 836us/step - loss: 0.2804 - val_loss: 1.1323\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.323 - 0s 921us/step - loss: 0.2720 - val_loss: 1.2387\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.250 - 0s 836us/step - loss: 0.2434 - val_loss: 1.3803\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.200 - 0s 893us/step - loss: 0.2307 - val_loss: 1.8184\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.161 - 0s 931us/step - loss: 0.1958 - val_loss: 1.3374\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.155 - 0s 893us/step - loss: 0.1733 - val_loss: 1.7437\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.177 - 0s 902us/step - loss: 0.1854 - val_loss: 1.7657\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.130 - 0s 1ms/step - loss: 0.1633 - val_loss: 1.8231\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.169 - 0s 817us/step - loss: 0.1599 - val_loss: 2.0415\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.127 - 0s 959us/step - loss: 0.1640 - val_loss: 1.5773\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.173 - 0s 826us/step - loss: 0.1515 - val_loss: 2.3366\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.113 - 0s 921us/step - loss: 0.1382 - val_loss: 1.6328\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.558 - ETA: 0s - loss: 0.144 - 0s 855us/step - loss: 0.1452 - val_loss: 1.8913\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.112 - 0s 874us/step - loss: 0.1203 - val_loss: 1.7267\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.147 - 0s 864us/step - loss: 0.1255 - val_loss: 2.1620\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.119 - 0s 874us/step - loss: 0.1211 - val_loss: 2.1232\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.102 - 0s 969us/step - loss: 0.1194 - val_loss: 1.6509\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.109 - 0s 893us/step - loss: 0.1093 - val_loss: 1.9675\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.114 - 0s 940us/step - loss: 0.1064 - val_loss: 2.0388\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.097 - 0s 960us/step - loss: 0.1001 - val_loss: 2.0153\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.076 - 0s 921us/step - loss: 0.0952 - val_loss: 2.0396\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.079 - 0s 969us/step - loss: 0.0953 - val_loss: 2.2809\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.088 - 0s 1ms/step - loss: 0.0893 - val_loss: 1.8704\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.092 - 0s 826us/step - loss: 0.0840 - val_loss: 2.0607\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.088 - 0s 893us/step - loss: 0.0826 - val_loss: 1.8613\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.076 - 0s 902us/step - loss: 0.0869 - val_loss: 2.0668\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.082 - 0s 864us/step - loss: 0.0824 - val_loss: 1.9635\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 30\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.87 - ETA: 0s - loss: 0.5571 - 1s 8ms/step - loss: 0.6153 - val_loss: 0.6733\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.410 - 0s 798us/step - loss: 0.3981 - val_loss: 0.8896\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.473 - ETA: 0s - loss: 0.334 - 0s 883us/step - loss: 0.3294 - val_loss: 0.6570\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.294 - 0s 817us/step - loss: 0.3113 - val_loss: 0.5090\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.894 - ETA: 0s - loss: 0.307 - 0s 817us/step - loss: 0.2710 - val_loss: 0.5215\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.262 - 0s 808us/step - loss: 0.2433 - val_loss: 0.4727\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.250 - 0s 1ms/step - loss: 0.2464 - val_loss: 0.5684\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.196 - 0s 1ms/step - loss: 0.2092 - val_loss: 0.5224\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.080 - ETA: 0s - loss: 0.207 - 0s 845us/step - loss: 0.1965 - val_loss: 0.5539\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.147 - 0s 902us/step - loss: 0.1817 - val_loss: 0.5493\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.101 - 0s 940us/step - loss: 0.1818 - val_loss: 0.5804\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.139 - 0s 874us/step - loss: 0.1636 - val_loss: 0.6319\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.204 - 0s 912us/step - loss: 0.1681 - val_loss: 0.6583\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.169 - 0s 997us/step - loss: 0.1542 - val_loss: 0.7344\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.263 - ETA: 0s - loss: 0.154 - 0s 827us/step - loss: 0.1536 - val_loss: 0.7280\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.137 - 0s 846us/step - loss: 0.1435 - val_loss: 0.8176\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.144 - 0s 893us/step - loss: 0.1443 - val_loss: 0.8000\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.100 - 0s 864us/step - loss: 0.1358 - val_loss: 0.7481\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.150 - 0s 826us/step - loss: 0.1387 - val_loss: 0.8686\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.124 - 0s 836us/step - loss: 0.1318 - val_loss: 0.8825\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.540 - ETA: 0s - loss: 0.153 - 0s 788us/step - loss: 0.1318 - val_loss: 1.0391\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.762 - ETA: 0s - loss: 0.131 - 0s 883us/step - loss: 0.1200 - val_loss: 0.9403\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.107 - 0s 836us/step - loss: 0.1133 - val_loss: 0.9258\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.080 - 0s 845us/step - loss: 0.1195 - val_loss: 1.0610\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.138 - 0s 950us/step - loss: 0.1315 - val_loss: 0.9983\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.125 - 0s 836us/step - loss: 0.1142 - val_loss: 1.0322\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.124 - 0s 950us/step - loss: 0.1133 - val_loss: 1.0093\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.119 - 0s 883us/step - loss: 0.1095 - val_loss: 1.0141\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.078 - 0s 817us/step - loss: 0.1102 - val_loss: 0.9678\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.085 - 0s 845us/step - loss: 0.1079 - val_loss: 1.0691\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.129 - 0s 855us/step - loss: 0.1072 - val_loss: 1.0586\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.119 - 0s 864us/step - loss: 0.0979 - val_loss: 1.1406\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.104 - 0s 893us/step - loss: 0.0985 - val_loss: 1.0576\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.101 - 0s 921us/step - loss: 0.0963 - val_loss: 1.0101\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.078 - 0s 921us/step - loss: 0.0956 - val_loss: 1.0753\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.107 - 0s 997us/step - loss: 0.0966 - val_loss: 0.9866\n",
      "Epoch 00036: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 31\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.61 - ETA: 0s - loss: 0.5405 - 1s 8ms/step - loss: 0.6147 - val_loss: 0.8957\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.536 - 0s 883us/step - loss: 0.4560 - val_loss: 0.8331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.382 - ETA: 0s - loss: 0.356 - 0s 817us/step - loss: 0.3905 - val_loss: 0.8668\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.407 - 0s 836us/step - loss: 0.3486 - val_loss: 0.9423\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.268 - 0s 836us/step - loss: 0.3350 - val_loss: 0.8237\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.298 - 0s 798us/step - loss: 0.3097 - val_loss: 0.8021\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.261 - 0s 826us/step - loss: 0.2698 - val_loss: 0.9482\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.236 - 0s 789us/step - loss: 0.2503 - val_loss: 0.9124\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.673 - ETA: 0s - loss: 0.183 - 0s 864us/step - loss: 0.2208 - val_loss: 0.9826\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.227 - 0s 826us/step - loss: 0.2288 - val_loss: 0.8758\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.223 - 0s 817us/step - loss: 0.2106 - val_loss: 0.8802\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.170 - 0s 798us/step - loss: 0.1759 - val_loss: 0.8757\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.189 - 0s 836us/step - loss: 0.1721 - val_loss: 0.6868\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.190 - 0s 845us/step - loss: 0.1908 - val_loss: 0.9935\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.171 - 0s 912us/step - loss: 0.1486 - val_loss: 0.8480\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.156 - 0s 883us/step - loss: 0.1479 - val_loss: 0.8895\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.123 - 0s 817us/step - loss: 0.1305 - val_loss: 0.9350\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.118 - 0s 845us/step - loss: 0.1222 - val_loss: 1.0522\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.100 - 0s 798us/step - loss: 0.0941 - val_loss: 1.0890\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.098 - 0s 855us/step - loss: 0.1025 - val_loss: 0.8214\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.101 - 0s 760us/step - loss: 0.1077 - val_loss: 1.0705\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.079 - 0s 893us/step - loss: 0.0884 - val_loss: 1.3116\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.107 - 0s 969us/step - loss: 0.0981 - val_loss: 0.9742\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.109 - 0s 817us/step - loss: 0.0981 - val_loss: 1.0770\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.083 - 0s 978us/step - loss: 0.0959 - val_loss: 1.5170\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.084 - 0s 855us/step - loss: 0.0787 - val_loss: 1.0918\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.069 - 0s 893us/step - loss: 0.0710 - val_loss: 1.2266\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.050 - 0s 931us/step - loss: 0.0598 - val_loss: 1.2206\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.054 - 0s 883us/step - loss: 0.0671 - val_loss: 1.3529\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.063 - 0s 1ms/step - loss: 0.0696 - val_loss: 1.1513\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.071 - 0s 731us/step - loss: 0.0672 - val_loss: 1.1183\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.052 - 0s 798us/step - loss: 0.0545 - val_loss: 1.4811\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.048 - 0s 884us/step - loss: 0.0444 - val_loss: 1.0699\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.050 - 0s 836us/step - loss: 0.0588 - val_loss: 1.5437\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.054 - 0s 912us/step - loss: 0.0515 - val_loss: 1.4406\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.043 - 0s 864us/step - loss: 0.0532 - val_loss: 1.2620\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.056 - 0s 845us/step - loss: 0.0552 - val_loss: 1.2147\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.052 - 0s 836us/step - loss: 0.0482 - val_loss: 1.5341\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.037 - 0s 836us/step - loss: 0.0400 - val_loss: 1.2714\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.076 - 0s 845us/step - loss: 0.0674 - val_loss: 1.8907\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.052 - 0s 950us/step - loss: 0.0516 - val_loss: 0.9483\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.063 - 0s 855us/step - loss: 0.0625 - val_loss: 1.0335\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.059 - 0s 855us/step - loss: 0.0508 - val_loss: 1.1316\n",
      "Epoch 00043: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 32\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.17 - ETA: 0s - loss: 0.9569 - 1s 8ms/step - loss: 0.7273 - val_loss: 0.8124\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.734 - ETA: 0s - loss: 0.623 - 0s 1ms/step - loss: 0.4938 - val_loss: 0.6918\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.401 - ETA: 0s - loss: 0.284 - 0s 874us/step - loss: 0.4262 - val_loss: 0.6903\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.390 - 0s 1ms/step - loss: 0.3639 - val_loss: 0.6912\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.671 - ETA: 0s - loss: 0.343 - 0s 950us/step - loss: 0.3276 - val_loss: 0.7032\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.297 - 0s 864us/step - loss: 0.2712 - val_loss: 0.8176\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.223 - 0s 1ms/step - loss: 0.2498 - val_loss: 0.7661\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.203 - 0s 883us/step - loss: 0.2269 - val_loss: 0.5973\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.236 - 0s 826us/step - loss: 0.2074 - val_loss: 0.8014\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.217 - 0s 817us/step - loss: 0.1938 - val_loss: 0.7269\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.144 - 0s 817us/step - loss: 0.1643 - val_loss: 0.6782\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.140 - 0s 817us/step - loss: 0.1735 - val_loss: 0.6103\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.193 - 0s 836us/step - loss: 0.1736 - val_loss: 0.5953\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.175 - 0s 864us/step - loss: 0.1561 - val_loss: 0.7559\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.117 - 0s 855us/step - loss: 0.1528 - val_loss: 0.5285\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.158 - 0s 836us/step - loss: 0.1361 - val_loss: 0.8298\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.124 - 0s 807us/step - loss: 0.1317 - val_loss: 0.5903\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.113 - 0s 874us/step - loss: 0.1247 - val_loss: 0.6988\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.086 - 0s 931us/step - loss: 0.1021 - val_loss: 0.5550\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.119 - 0s 903us/step - loss: 0.1109 - val_loss: 0.8440\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.113 - 0s 903us/step - loss: 0.1012 - val_loss: 0.5381\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.108 - 0s 912us/step - loss: 0.1071 - val_loss: 0.6180\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.101 - 0s 855us/step - loss: 0.0911 - val_loss: 0.7128\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.096 - 0s 931us/step - loss: 0.0868 - val_loss: 0.7372\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.083 - 0s 912us/step - loss: 0.0812 - val_loss: 0.6768\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.066 - 0s 902us/step - loss: 0.0890 - val_loss: 0.7613\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.084 - 0s 855us/step - loss: 0.0789 - val_loss: 0.7999\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.070 - 0s 845us/step - loss: 0.0666 - val_loss: 0.7824\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.082 - 0s 855us/step - loss: 0.0878 - val_loss: 0.8586\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.062 - 0s 845us/step - loss: 0.0656 - val_loss: 0.6578\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.413 - ETA: 0s - loss: 0.073 - 0s 959us/step - loss: 0.0677 - val_loss: 0.8223\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.069 - 0s 902us/step - loss: 0.0741 - val_loss: 0.8050\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.049 - 0s 874us/step - loss: 0.0531 - val_loss: 0.8318\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.058 - 0s 959us/step - loss: 0.0526 - val_loss: 0.9159\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.049 - 0s 874us/step - loss: 0.0496 - val_loss: 0.8300\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.068 - 0s 893us/step - loss: 0.0623 - val_loss: 0.9777\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.050 - 0s 874us/step - loss: 0.0555 - val_loss: 0.8201\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.052 - 0s 893us/step - loss: 0.0465 - val_loss: 0.8305\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.061 - 0s 883us/step - loss: 0.0554 - val_loss: 0.8887\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.060 - 0s 931us/step - loss: 0.0566 - val_loss: 1.1101\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.050 - 0s 884us/step - loss: 0.0537 - val_loss: 0.9110\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.037 - 0s 931us/step - loss: 0.0454 - val_loss: 0.9640\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.039 - 0s 845us/step - loss: 0.0411 - val_loss: 0.8817\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.043 - 0s 959us/step - loss: 0.0509 - val_loss: 0.9669\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.043 - 0s 883us/step - loss: 0.0427 - val_loss: 1.0864\n",
      "Epoch 00045: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 33\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.11 - ETA: 0s - loss: 1.3679 - 1s 8ms/step - loss: 0.9234 - val_loss: 0.0936\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 1.234 - 0s 950us/step - loss: 0.8392 - val_loss: 0.1640\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 1.117 - 0s 912us/step - loss: 0.7862 - val_loss: 0.1639\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 1.017 - 0s 883us/step - loss: 0.7547 - val_loss: 0.2244\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.793 - 0s 941us/step - loss: 0.6807 - val_loss: 0.2914\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.664 - 0s 883us/step - loss: 0.6251 - val_loss: 0.3678\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.270 - 0s 874us/step - loss: 0.5776 - val_loss: 0.5073\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.760 - 0s 969us/step - loss: 0.5122 - val_loss: 0.7052\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - 0s 931us/step - loss: 0.4616 - val_loss: 0.7752\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.174 - 0s 921us/step - loss: 0.4460 - val_loss: 0.9027\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.554 - 0s 1ms/step - loss: 0.4128 - val_loss: 1.3298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.125 - 0s 902us/step - loss: 0.3486 - val_loss: 1.2501\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.105 - 0s 931us/step - loss: 0.3187 - val_loss: 1.5571\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.117 - 0s 855us/step - loss: 0.2914 - val_loss: 1.6999\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.170 - 0s 988us/step - loss: 0.3071 - val_loss: 1.9675\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.095 - 0s 874us/step - loss: 0.2404 - val_loss: 2.1047\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.223 - 0s 845us/step - loss: 0.1781 - val_loss: 2.3786\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.271 - 0s 921us/step - loss: 0.2032 - val_loss: 2.6881\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.078 - 0s 902us/step - loss: 0.1760 - val_loss: 2.5519\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.267 - 0s 836us/step - loss: 0.2232 - val_loss: 3.1236\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.649 - ETA: 0s - loss: 0.204 - 0s 826us/step - loss: 0.1827 - val_loss: 3.4247\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.161 - 0s 807us/step - loss: 0.1431 - val_loss: 3.4113\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.132 - 0s 789us/step - loss: 0.1250 - val_loss: 3.6207\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.108 - 0s 950us/step - loss: 0.0991 - val_loss: 3.7473\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.096 - 0s 893us/step - loss: 0.0913 - val_loss: 3.8225\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.116 - 0s 855us/step - loss: 0.0995 - val_loss: 3.7349\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.099 - 0s 912us/step - loss: 0.0878 - val_loss: 4.1036\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.052 - 0s 845us/step - loss: 0.1061 - val_loss: 3.9279\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.071 - 0s 893us/step - loss: 0.0702 - val_loss: 4.3999\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.077 - 0s 940us/step - loss: 0.0743 - val_loss: 4.3634\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.409 - ETA: 0s - loss: 0.084 - 0s 902us/step - loss: 0.0747 - val_loss: 4.1989\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 34\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 1.48 - ETA: 0s - loss: 0.8927 - 1s 8ms/step - loss: 0.7794 - val_loss: 0.6539\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.641 - ETA: 0s - loss: 0.555 - 0s 1ms/step - loss: 0.5656 - val_loss: 0.8461\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.471 - 0s 789us/step - loss: 0.5088 - val_loss: 0.9161\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.305 - ETA: 0s - loss: 0.459 - 0s 750us/step - loss: 0.4657 - val_loss: 1.3072\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.460 - 0s 845us/step - loss: 0.4272 - val_loss: 1.2310\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.399 - 0s 864us/step - loss: 0.4269 - val_loss: 1.4757\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.367 - 0s 883us/step - loss: 0.3583 - val_loss: 1.6331\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.778 - ETA: 0s - loss: 0.324 - 0s 883us/step - loss: 0.3455 - val_loss: 1.9527\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.299 - 0s 874us/step - loss: 0.3050 - val_loss: 2.1669\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.288 - 0s 940us/step - loss: 0.2967 - val_loss: 2.2581\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.328 - 0s 846us/step - loss: 0.3025 - val_loss: 2.4662\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.216 - 0s 931us/step - loss: 0.2824 - val_loss: 2.8571\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.298 - 0s 788us/step - loss: 0.2847 - val_loss: 2.3620\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.707 - ETA: 0s - loss: 0.264 - 0s 883us/step - loss: 0.2631 - val_loss: 3.0651\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.263 - 0s 883us/step - loss: 0.2613 - val_loss: 3.2245\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.201 - 0s 950us/step - loss: 0.2622 - val_loss: 2.7055\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.252 - 0s 883us/step - loss: 0.2506 - val_loss: 3.0790\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.179 - 0s 883us/step - loss: 0.2282 - val_loss: 2.4151\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.466 - ETA: 0s - loss: 0.222 - 0s 874us/step - loss: 0.2294 - val_loss: 2.8737\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.237 - 0s 931us/step - loss: 0.2472 - val_loss: 2.3977\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.279 - 0s 921us/step - loss: 0.2546 - val_loss: 3.1679\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.283 - 0s 988us/step - loss: 0.2331 - val_loss: 2.9335\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.210 - 0s 874us/step - loss: 0.2196 - val_loss: 2.4616\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.191 - 0s 874us/step - loss: 0.1988 - val_loss: 2.8133\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.692 - ETA: 0s - loss: 0.231 - 0s 903us/step - loss: 0.1999 - val_loss: 2.8067\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.225 - 0s 826us/step - loss: 0.1975 - val_loss: 2.6047\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.205 - 0s 798us/step - loss: 0.2139 - val_loss: 2.8793\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.174 - 0s 903us/step - loss: 0.1858 - val_loss: 2.9326\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.209 - 0s 855us/step - loss: 0.1970 - val_loss: 2.2633\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.195 - 0s 836us/step - loss: 0.2058 - val_loss: 2.7356\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.206 - 0s 807us/step - loss: 0.2039 - val_loss: 3.0061\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 35\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 6.48 - ETA: 0s - loss: 1.2569 - 1s 8ms/step - loss: 0.9128 - val_loss: 0.1408\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 15.94 - ETA: 0s - loss: 0.9203 - 0s 807us/step - loss: 0.8280 - val_loss: 0.0615\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 9.634 - ETA: 0s - loss: 0.676 - 0s 788us/step - loss: 0.7538 - val_loss: 0.0877\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.953 - 0s 912us/step - loss: 0.7448 - val_loss: 0.0970\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.714 - 0s 1ms/step - loss: 0.6816 - val_loss: 0.0950\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.458 - ETA: 0s - loss: 0.822 - 0s 836us/step - loss: 0.6511 - val_loss: 0.1108\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.764 - 0s 779us/step - loss: 0.6112 - val_loss: 0.1165\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.372 - 0s 855us/step - loss: 0.5799 - val_loss: 0.1251\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.709 - 0s 893us/step - loss: 0.5602 - val_loss: 0.1832\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.162 - 0s 1ms/step - loss: 0.5027 - val_loss: 0.1814\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.482 - ETA: 0s - loss: 0.623 - 0s 902us/step - loss: 0.4664 - val_loss: 0.2376\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.538 - 0s 893us/step - loss: 0.4200 - val_loss: 0.2337\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.577 - 0s 902us/step - loss: 0.4397 - val_loss: 0.2433\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.490 - 0s 997us/step - loss: 0.3914 - val_loss: 0.2985\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.444 - 0s 845us/step - loss: 0.3622 - val_loss: 0.2722\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.446 - 0s 864us/step - loss: 0.3445 - val_loss: 0.3168\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.338 - 0s 922us/step - loss: 0.3322 - val_loss: 0.3038\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.343 - 0s 902us/step - loss: 0.2791 - val_loss: 0.3474\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.330 - 0s 807us/step - loss: 0.2826 - val_loss: 0.3122\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.122 - 0s 855us/step - loss: 0.2488 - val_loss: 0.3115\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.245 - 0s 836us/step - loss: 0.2219 - val_loss: 0.3505\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.229 - 0s 846us/step - loss: 0.2309 - val_loss: 0.3563\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.122 - 0s 855us/step - loss: 0.2146 - val_loss: 0.4498\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.198 - 0s 969us/step - loss: 0.2148 - val_loss: 0.4391\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.124 - 0s 883us/step - loss: 0.1999 - val_loss: 0.4690\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.202 - 0s 883us/step - loss: 0.1629 - val_loss: 0.4500\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.156 - 0s 959us/step - loss: 0.1532 - val_loss: 0.4679\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.141 - 0s 883us/step - loss: 0.1222 - val_loss: 0.4189\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.157 - 0s 855us/step - loss: 0.1368 - val_loss: 0.4385\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.026 - ETA: 0s - loss: 0.162 - 0s 921us/step - loss: 0.1409 - val_loss: 0.5692\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.094 - 0s 874us/step - loss: 0.1224 - val_loss: 0.4791\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.070 - 0s 902us/step - loss: 0.1046 - val_loss: 0.4914\n",
      "Epoch 00032: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 36\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.59 - ETA: 0s - loss: 1.4783 - 1s 8ms/step - loss: 0.9714 - val_loss: 0.2499\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 17.29 - ETA: 0s - loss: 1.0777 - 0s 864us/step - loss: 0.8012 - val_loss: 0.3425\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.152 - 0s 855us/step - loss: 0.7491 - val_loss: 0.3494\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 14.78 - ETA: 0s - loss: 1.1676 - 0s 1ms/step - loss: 0.6904 - val_loss: 0.5185\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.094 - 0s 921us/step - loss: 0.5815 - val_loss: 0.5960\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.677 - 0s 912us/step - loss: 0.5347 - val_loss: 0.7774\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.745 - 0s 969us/step - loss: 0.4805 - val_loss: 0.7899\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.554 - 0s 912us/step - loss: 0.4348 - val_loss: 0.9061\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.490 - 0s 855us/step - loss: 0.3893 - val_loss: 0.9113\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.119 - 0s 817us/step - loss: 0.3543 - val_loss: 0.7676\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.380 - 0s 817us/step - loss: 0.3249 - val_loss: 1.1863\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.372 - 0s 845us/step - loss: 0.2962 - val_loss: 0.9480\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.095 - 0s 798us/step - loss: 0.2645 - val_loss: 1.0707\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.088 - 0s 1ms/step - loss: 0.1832 - val_loss: 0.9525\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.987 - ETA: 0s - loss: 0.211 - 0s 874us/step - loss: 0.1803 - val_loss: 0.7709\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.217 - 0s 884us/step - loss: 0.1796 - val_loss: 1.0265\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.095 - 0s 864us/step - loss: 0.1638 - val_loss: 1.0524\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.128 - 0s 969us/step - loss: 0.1217 - val_loss: 0.9132\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.076 - 0s 845us/step - loss: 0.1265 - val_loss: 1.0535\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.109 - 0s 883us/step - loss: 0.1029 - val_loss: 1.0383\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.104 - 0s 902us/step - loss: 0.0952 - val_loss: 0.9856\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.118 - 0s 1ms/step - loss: 0.1171 - val_loss: 1.4196\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.076 - 0s 750us/step - loss: 0.0823 - val_loss: 0.6932\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.053 - 0s 826us/step - loss: 0.0623 - val_loss: 1.1414\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.068 - 0s 941us/step - loss: 0.0769 - val_loss: 0.8004\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.051 - 0s 807us/step - loss: 0.0505 - val_loss: 1.0505\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.072 - 0s 846us/step - loss: 0.0684 - val_loss: 0.8846\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.059 - 0s 789us/step - loss: 0.0526 - val_loss: 1.0530\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.533 - ETA: 0s - loss: 0.076 - 0s 845us/step - loss: 0.0763 - val_loss: 0.8924\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.068 - 0s 931us/step - loss: 0.0625 - val_loss: 1.0537\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.053 - 0s 845us/step - loss: 0.0539 - val_loss: 0.9762\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 37\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.62 - ETA: 0s - loss: 0.7778 - 1s 8ms/step - loss: 0.7052 - val_loss: 0.5962\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.584 - ETA: 0s - loss: 0.503 - 0s 855us/step - loss: 0.5494 - val_loss: 0.8470\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.589 - 0s 883us/step - loss: 0.4898 - val_loss: 0.7852\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.091 - ETA: 0s - loss: 0.571 - 0s 931us/step - loss: 0.4401 - val_loss: 0.9075\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.637 - ETA: 0s - loss: 0.445 - 0s 855us/step - loss: 0.3878 - val_loss: 0.9857\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.293 - ETA: 0s - loss: 0.410 - 0s 969us/step - loss: 0.3792 - val_loss: 1.1479\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.276 - 0s 921us/step - loss: 0.3351 - val_loss: 1.2268\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.348 - 0s 883us/step - loss: 0.3244 - val_loss: 1.3749\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.274 - 0s 912us/step - loss: 0.2973 - val_loss: 1.3785\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.348 - 0s 883us/step - loss: 0.2956 - val_loss: 1.6467\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.271 - 0s 874us/step - loss: 0.2641 - val_loss: 1.5190\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.233 - 0s 893us/step - loss: 0.2260 - val_loss: 1.8520\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.208 - 0s 902us/step - loss: 0.2306 - val_loss: 1.5497\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.264 - 0s 855us/step - loss: 0.2447 - val_loss: 1.6615\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.443 - ETA: 0s - loss: 0.192 - 0s 969us/step - loss: 0.1991 - val_loss: 1.4148\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.178 - 0s 931us/step - loss: 0.1911 - val_loss: 1.7008\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.557 - ETA: 0s - loss: 0.194 - 0s 855us/step - loss: 0.1863 - val_loss: 1.6855\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.190 - 0s 864us/step - loss: 0.1725 - val_loss: 1.7634\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.205 - 0s 864us/step - loss: 0.1792 - val_loss: 1.5554\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.141 - 0s 931us/step - loss: 0.1670 - val_loss: 1.3883\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.179 - 0s 893us/step - loss: 0.1617 - val_loss: 1.6455\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.163 - 0s 912us/step - loss: 0.1467 - val_loss: 1.7586\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.126 - 0s 874us/step - loss: 0.1396 - val_loss: 1.7923\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.134 - 0s 902us/step - loss: 0.1312 - val_loss: 1.7161\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.129 - 0s 874us/step - loss: 0.1316 - val_loss: 1.9946\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.134 - 0s 931us/step - loss: 0.1342 - val_loss: 2.0438\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.105 - 0s 817us/step - loss: 0.1198 - val_loss: 1.4966\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.109 - 0s 874us/step - loss: 0.1207 - val_loss: 1.8824\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.108 - 0s 940us/step - loss: 0.1232 - val_loss: 1.7308\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.080 - 0s 846us/step - loss: 0.1024 - val_loss: 2.0982\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.132 - 0s 874us/step - loss: 0.1344 - val_loss: 1.5608\n",
      "Epoch 00031: early stopping\n",
      "MSE: 797.746839\n",
      "RMSE: 28.244413\n",
      "MAE: 22.797235\n",
      "MAPE: 27.787172\n",
      "\n",
      "Quantile 1, between 39.99999999999999 and 77.5\n",
      "MSE: 788.391808\n",
      "RMSE: 28.078316\n",
      "MAE: 23.449544\n",
      "MAPE: 40.191306\n",
      "\n",
      "Quantile 2, between 77.5 and 87.5\n",
      "MSE: 958.576579\n",
      "RMSE: 30.960888\n",
      "MAE: 21.316866\n",
      "MAPE: 25.815463\n",
      "\n",
      "Quantile 3, between 87.5 and 98.00000000000001\n",
      "MSE: 380.085464\n",
      "RMSE: 19.495781\n",
      "MAE: 16.311486\n",
      "MAPE: 17.314538\n",
      "\n",
      "Quantile 4, between 98.00000000000001 and 130.00000000000003\n",
      "MSE: 1038.250342\n",
      "RMSE: 32.221892\n",
      "MAE: 29.314432\n",
      "MAPE: 26.582948\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_test = 15\n",
    "n_hexa = 30\n",
    "name_error = 'LSTM predictions - 15 Days -30 hex - 4q'\n",
    "path_error = \"imgs/ips_predictions/prediction_lstm_15_days_30hex_4q.png\"\n",
    "name_map = '15 days - 30 hex - 4q'\n",
    "path_map = \"imgs/ips_predictions/maps_LSTM_prediction_15_total_data_30_hex_4q.png\"\n",
    "\n",
    "dict_pred,pred_lstm,pred_lstm_cases = lstm_hex_selected(data_days, dict_pred_sel_hexa,n_test,n_hexa,name_error,path_error,name_map,path_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,) (15,)\n",
      "model compiled 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.63 - ETA: 0s - loss: 0.8519 - 1s 8ms/step - loss: 0.7756 - val_loss: 0.5292\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.753 - ETA: 0s - loss: 0.723 - 0s 1ms/step - loss: 0.5724 - val_loss: 1.1171\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.515 - 0s 1ms/step - loss: 0.5266 - val_loss: 0.9600\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.584 - 0s 959us/step - loss: 0.5091 - val_loss: 1.0203\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.305 - ETA: 0s - loss: 0.524 - 0s 959us/step - loss: 0.4869 - val_loss: 0.9687\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.419 - 0s 950us/step - loss: 0.4566 - val_loss: 1.0184\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.492 - 0s 912us/step - loss: 0.4557 - val_loss: 1.0083\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.600 - ETA: 0s - loss: 0.413 - 0s 1ms/step - loss: 0.4413 - val_loss: 1.1096\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.496 - 0s 1ms/step - loss: 0.4275 - val_loss: 1.3033\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.340 - ETA: 0s - loss: 0.412 - 0s 1ms/step - loss: 0.4171 - val_loss: 1.3532\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.576 - ETA: 0s - loss: 0.414 - ETA: 0s - loss: 0.396 - 0s 1ms/step - loss: 0.3925 - val_loss: 1.3214\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.260 - 0s 1ms/step - loss: 0.3733 - val_loss: 1.2775\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.631 - ETA: 0s - loss: 0.376 - 0s 912us/step - loss: 0.3764 - val_loss: 1.5697\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.322 - 0s 1ms/step - loss: 0.3476 - val_loss: 1.5190\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.280 - 0s 1ms/step - loss: 0.3480 - val_loss: 1.5208\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.301 - 0s 1ms/step - loss: 0.3370 - val_loss: 1.7615\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.420 - ETA: 0s - loss: 0.385 - 0s 1ms/step - loss: 0.3303 - val_loss: 1.7003\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.356 - 0s 1ms/step - loss: 0.3094 - val_loss: 1.7003\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.345 - 0s 1ms/step - loss: 0.3198 - val_loss: 1.7074\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.263 - 0s 1ms/step - loss: 0.2992 - val_loss: 1.8998\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.315 - 0s 1ms/step - loss: 0.3091 - val_loss: 1.6664\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.286 - 0s 912us/step - loss: 0.2827 - val_loss: 1.9192\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.282 - 0s 2ms/step - loss: 0.2822 - val_loss: 1.9842\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.311 - 0s 1ms/step - loss: 0.2856 - val_loss: 1.9244\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.273 - 0s 1ms/step - loss: 0.2712 - val_loss: 1.9264\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.322 - 0s 1ms/step - loss: 0.2769 - val_loss: 2.1946\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.243 - 0s 1ms/step - loss: 0.2557 - val_loss: 2.1389\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.216 - 0s 959us/step - loss: 0.2424 - val_loss: 2.2106\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.234 - 0s 1ms/step - loss: 0.2307 - val_loss: 2.1562\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.307 - 0s 1ms/step - loss: 0.2324 - val_loss: 2.2965\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.255 - 0s 864us/step - loss: 0.2230 - val_loss: 2.4751\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.55 - ETA: 0s - loss: 0.9588 - 1s 8ms/step - loss: 0.7206 - val_loss: 0.5052\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.535 - 0s 874us/step - loss: 0.4705 - val_loss: 0.9943\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.434 - ETA: 0s - loss: 0.514 - 0s 950us/step - loss: 0.4203 - val_loss: 0.9746\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.523 - 0s 874us/step - loss: 0.4245 - val_loss: 0.9413\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.388 - 0s 2ms/step - loss: 0.3702 - val_loss: 0.8255\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.513 - ETA: 0s - loss: 0.406 - 0s 921us/step - loss: 0.3526 - val_loss: 0.9927\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.379 - 0s 959us/step - loss: 0.3340 - val_loss: 0.7984\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.333 - 0s 931us/step - loss: 0.3111 - val_loss: 0.9184\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.179 - 0s 1ms/step - loss: 0.2822 - val_loss: 0.8456\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.276 - 0s 969us/step - loss: 0.2839 - val_loss: 0.8947\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.427 - ETA: 0s - loss: 0.280 - 0s 931us/step - loss: 0.2753 - val_loss: 0.8898\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.294 - 0s 921us/step - loss: 0.2611 - val_loss: 0.8504\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.291 - 0s 826us/step - loss: 0.2640 - val_loss: 1.0014\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.227 - 0s 864us/step - loss: 0.2386 - val_loss: 0.7949\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.571 - ETA: 0s - loss: 0.276 - 0s 883us/step - loss: 0.2372 - val_loss: 1.0060\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.236 - 0s 798us/step - loss: 0.2448 - val_loss: 0.8586\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.224 - 0s 789us/step - loss: 0.2381 - val_loss: 1.0612\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.245 - 0s 798us/step - loss: 0.2263 - val_loss: 1.1676\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.201 - 0s 893us/step - loss: 0.2250 - val_loss: 0.8803\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.245 - 0s 864us/step - loss: 0.2070 - val_loss: 0.8998\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.215 - 0s 798us/step - loss: 0.2088 - val_loss: 0.9089\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.692 - ETA: 0s - loss: 0.171 - 0s 807us/step - loss: 0.2037 - val_loss: 0.9923\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.224 - 0s 950us/step - loss: 0.1956 - val_loss: 0.9578\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.158 - 0s 978us/step - loss: 0.1834 - val_loss: 0.8747\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.194 - 0s 750us/step - loss: 0.1824 - val_loss: 0.9354\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.174 - 0s 789us/step - loss: 0.1889 - val_loss: 0.9120\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.162 - 0s 817us/step - loss: 0.1911 - val_loss: 0.9700\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.145 - 0s 893us/step - loss: 0.1815 - val_loss: 0.8937\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.190 - 0s 807us/step - loss: 0.1658 - val_loss: 0.9670\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.145 - 0s 864us/step - loss: 0.1667 - val_loss: 0.9610\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.110 - 0s 836us/step - loss: 0.1564 - val_loss: 0.8712\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.47 - ETA: 0s - loss: 0.7608 - 1s 9ms/step - loss: 0.7088 - val_loss: 0.6615\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.643 - 0s 1ms/step - loss: 0.5150 - val_loss: 0.8440\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.240 - 0s 950us/step - loss: 0.4885 - val_loss: 0.6120\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.674 - ETA: 0s - loss: 0.561 - 0s 940us/step - loss: 0.4603 - val_loss: 0.6932\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.361 - ETA: 0s - loss: 0.255 - 0s 1ms/step - loss: 0.4173 - val_loss: 0.6431\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.621 - ETA: 0s - loss: 0.621 - ETA: 0s - loss: 0.419 - 0s 2ms/step - loss: 0.3882 - val_loss: 0.7008\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.377 - 0s 693us/step - loss: 0.3503 - val_loss: 0.6279\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.306 - 0s 760us/step - loss: 0.3432 - val_loss: 0.7464\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.314 - 0s 750us/step - loss: 0.3199 - val_loss: 0.7547\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.305 - ETA: 0s - loss: 0.286 - 0s 2ms/step - loss: 0.2971 - val_loss: 0.6915\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 1s - loss: 0.230 - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.268 - 0s 2ms/step - loss: 0.2906 - val_loss: 0.7041\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.275 - 0s 2ms/step - loss: 0.2857 - val_loss: 0.7730\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.227 - 0s 760us/step - loss: 0.2819 - val_loss: 0.7267\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.291 - 0s 741us/step - loss: 0.2668 - val_loss: 0.7065\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.211 - 0s 779us/step - loss: 0.2587 - val_loss: 0.5328\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.282 - 0s 883us/step - loss: 0.2581 - val_loss: 0.6231\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.331 - 0s 997us/step - loss: 0.2383 - val_loss: 0.7298\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.202 - 0s 836us/step - loss: 0.2349 - val_loss: 0.7192\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.069 - ETA: 0s - loss: 0.257 - 0s 912us/step - loss: 0.2446 - val_loss: 0.7687\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.463 - ETA: 0s - loss: 0.164 - 0s 959us/step - loss: 0.2364 - val_loss: 0.6696\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.262 - 0s 893us/step - loss: 0.2369 - val_loss: 0.7224\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.208 - 0s 931us/step - loss: 0.2185 - val_loss: 0.6938\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.185 - 0s 874us/step - loss: 0.2046 - val_loss: 0.8330\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.162 - 0s 1ms/step - loss: 0.2066 - val_loss: 0.8301\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.241 - 0s 912us/step - loss: 0.1972 - val_loss: 0.7584\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.205 - 0s 1ms/step - loss: 0.1925 - val_loss: 0.8346\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.223 - 0s 950us/step - loss: 0.1912 - val_loss: 0.6272\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.176 - 0s 969us/step - loss: 0.1844 - val_loss: 0.6951\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.150 - 0s 788us/step - loss: 0.1873 - val_loss: 0.8376\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.199 - 0s 921us/step - loss: 0.1870 - val_loss: 0.8310\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.169 - 0s 798us/step - loss: 0.1760 - val_loss: 0.7267\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.164 - 0s 931us/step - loss: 0.1813 - val_loss: 0.7612\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.144 - 0s 940us/step - loss: 0.1625 - val_loss: 0.7752\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.203 - 0s 817us/step - loss: 0.1698 - val_loss: 0.7397\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.155 - 0s 893us/step - loss: 0.1595 - val_loss: 0.8171\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.163 - 0s 921us/step - loss: 0.1459 - val_loss: 0.7540\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.165 - 0s 788us/step - loss: 0.1485 - val_loss: 0.7453\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.487 - ETA: 0s - loss: 0.166 - 0s 817us/step - loss: 0.1524 - val_loss: 0.6099\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.413 - ETA: 0s - loss: 0.126 - 0s 959us/step - loss: 0.1620 - val_loss: 0.7979\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.169 - 0s 845us/step - loss: 0.1543 - val_loss: 0.8328\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.118 - 0s 1ms/step - loss: 0.1456 - val_loss: 0.7125\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.152 - 0s 807us/step - loss: 0.1427 - val_loss: 0.7412\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.176 - 0s 988us/step - loss: 0.1396 - val_loss: 0.8517\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.169 - 0s 845us/step - loss: 0.1485 - val_loss: 0.6921\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.148 - 0s 807us/step - loss: 0.1355 - val_loss: 0.5898\n",
      "Epoch 00045: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.30 - ETA: 0s - loss: 0.5590 - 1s 8ms/step - loss: 0.5235 - val_loss: 1.0896\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.968 - ETA: 0s - loss: 0.350 - 0s 788us/step - loss: 0.3291 - val_loss: 1.6154\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.811 - ETA: 0s - loss: 0.323 - 0s 845us/step - loss: 0.2820 - val_loss: 1.3902\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.281 - 0s 921us/step - loss: 0.2496 - val_loss: 1.6197\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.253 - 0s 921us/step - loss: 0.2311 - val_loss: 1.7294\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.189 - 0s 950us/step - loss: 0.2030 - val_loss: 1.7868\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.212 - 0s 827us/step - loss: 0.1918 - val_loss: 1.9198\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.161 - 0s 883us/step - loss: 0.1887 - val_loss: 2.0986\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.159 - 0s 940us/step - loss: 0.1847 - val_loss: 2.1479\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.165 - 0s 836us/step - loss: 0.1688 - val_loss: 2.3152\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.152 - 0s 836us/step - loss: 0.1666 - val_loss: 2.1734\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.137 - 0s 845us/step - loss: 0.1541 - val_loss: 2.3404\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.158 - 0s 912us/step - loss: 0.1549 - val_loss: 2.3928\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.134 - 0s 978us/step - loss: 0.1479 - val_loss: 2.2796\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.156 - 0s 921us/step - loss: 0.1378 - val_loss: 2.4488\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.180 - 0s 959us/step - loss: 0.1395 - val_loss: 2.4776\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.130 - 0s 893us/step - loss: 0.1262 - val_loss: 2.9348\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.141 - 0s 855us/step - loss: 0.1254 - val_loss: 2.1198\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.113 - 0s 893us/step - loss: 0.1225 - val_loss: 2.4241\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.122 - 0s 883us/step - loss: 0.1264 - val_loss: 2.5004\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.110 - 0s 893us/step - loss: 0.1317 - val_loss: 2.5259\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.110 - 0s 912us/step - loss: 0.1130 - val_loss: 2.0301\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.108 - 0s 959us/step - loss: 0.1171 - val_loss: 2.6771\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.093 - 0s 978us/step - loss: 0.1209 - val_loss: 2.4300\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.5323e-0 - ETA: 0s - loss: 0.0981    - 0s 931us/step - loss: 0.1092 - val_loss: 2.6965\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.128 - 0s 931us/step - loss: 0.1137 - val_loss: 2.3422\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.078 - 0s 1ms/step - loss: 0.1057 - val_loss: 2.7192\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.119 - 0s 855us/step - loss: 0.1095 - val_loss: 3.0105\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.119 - 0s 902us/step - loss: 0.1015 - val_loss: 2.7365\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.081 - 0s 883us/step - loss: 0.0946 - val_loss: 2.6421\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.117 - 0s 893us/step - loss: 0.1041 - val_loss: 2.7834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.17 - ETA: 0s - loss: 0.9178 - 1s 8ms/step - loss: 0.8751 - val_loss: 0.3968\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.683 - 0s 798us/step - loss: 0.6357 - val_loss: 0.4335\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.568 - 0s 836us/step - loss: 0.5947 - val_loss: 0.5328\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.559 - ETA: 0s - loss: 0.601 - 0s 807us/step - loss: 0.5552 - val_loss: 0.5304\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.538 - 0s 712us/step - loss: 0.5365 - val_loss: 0.4848\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.509 - 0s 1ms/step - loss: 0.5106 - val_loss: 0.5780\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.355 - 0s 912us/step - loss: 0.4701 - val_loss: 0.4723\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.575 - 0s 921us/step - loss: 0.4603 - val_loss: 0.5564\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.418 - 0s 855us/step - loss: 0.4225 - val_loss: 0.5282\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.368 - ETA: 0s - loss: 0.457 - 0s 817us/step - loss: 0.4271 - val_loss: 0.5092\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.689 - ETA: 0s - loss: 0.348 - 0s 826us/step - loss: 0.4040 - val_loss: 0.5112\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.344 - 0s 874us/step - loss: 0.4125 - val_loss: 0.8133\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.279 - 0s 997us/step - loss: 0.3697 - val_loss: 0.5788\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.341 - 0s 836us/step - loss: 0.3702 - val_loss: 0.6173\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.790 - ETA: 0s - loss: 0.398 - 0s 808us/step - loss: 0.3430 - val_loss: 0.7576\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.324 - 0s 817us/step - loss: 0.3478 - val_loss: 0.7725\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.323 - 0s 845us/step - loss: 0.3618 - val_loss: 0.7229\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.350 - 0s 826us/step - loss: 0.3485 - val_loss: 0.6157\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.321 - 0s 893us/step - loss: 0.3103 - val_loss: 0.7088\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.550 - ETA: 0s - loss: 0.308 - 0s 884us/step - loss: 0.3085 - val_loss: 0.7003\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.986 - ETA: 0s - loss: 0.356 - 0s 940us/step - loss: 0.3193 - val_loss: 0.7400\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.185 - ETA: 0s - loss: 0.324 - 0s 864us/step - loss: 0.3124 - val_loss: 0.6711\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.304 - 0s 1ms/step - loss: 0.2981 - val_loss: 0.8438\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.298 - 0s 1ms/step - loss: 0.3061 - val_loss: 0.6160\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.318 - 0s 826us/step - loss: 0.3104 - val_loss: 0.9027\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.354 - 0s 902us/step - loss: 0.2994 - val_loss: 0.6519\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.291 - 0s 817us/step - loss: 0.2664 - val_loss: 0.7615\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.248 - 0s 836us/step - loss: 0.2576 - val_loss: 0.7659\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.224 - 0s 931us/step - loss: 0.2669 - val_loss: 0.7718\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.230 - 0s 845us/step - loss: 0.2468 - val_loss: 0.8065\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.227 - 0s 826us/step - loss: 0.2681 - val_loss: 0.9036\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.08 - ETA: 0s - loss: 0.7740 - 1s 8ms/step - loss: 0.7926 - val_loss: 0.2660\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.517 - ETA: 0s - loss: 0.728 - 0s 846us/step - loss: 0.6282 - val_loss: 0.5300\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.727 - 0s 950us/step - loss: 0.5494 - val_loss: 0.6528\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.554 - 0s 864us/step - loss: 0.4913 - val_loss: 0.7946\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.403 - 0s 978us/step - loss: 0.4451 - val_loss: 0.8025\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.267 - 0s 826us/step - loss: 0.4101 - val_loss: 0.7573\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.506 - 0s 931us/step - loss: 0.3815 - val_loss: 1.0982\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.310 - 0s 826us/step - loss: 0.3566 - val_loss: 0.9202\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.306 - 0s 959us/step - loss: 0.3317 - val_loss: 0.9949\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.317 - 0s 1ms/step - loss: 0.3123 - val_loss: 0.9885\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.259 - 0s 931us/step - loss: 0.2857 - val_loss: 1.0161\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.328 - 0s 864us/step - loss: 0.3014 - val_loss: 0.9185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.215 - 0s 978us/step - loss: 0.2713 - val_loss: 1.2327\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.288 - 0s 845us/step - loss: 0.2527 - val_loss: 1.1670\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.291 - 0s 798us/step - loss: 0.2597 - val_loss: 1.2094\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.239 - 0s 826us/step - loss: 0.2690 - val_loss: 1.1798\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.228 - 0s 855us/step - loss: 0.2388 - val_loss: 1.0640\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.217 - 0s 836us/step - loss: 0.2372 - val_loss: 1.4239\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.263 - 0s 798us/step - loss: 0.2295 - val_loss: 0.9808\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.242 - 0s 845us/step - loss: 0.2208 - val_loss: 1.0572\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.218 - 0s 865us/step - loss: 0.2186 - val_loss: 0.9291\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.260 - 0s 817us/step - loss: 0.2193 - val_loss: 1.1797\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.181 - 0s 836us/step - loss: 0.2144 - val_loss: 0.9462\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.243 - 0s 912us/step - loss: 0.2128 - val_loss: 1.1796\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.221 - 0s 788us/step - loss: 0.2023 - val_loss: 0.8684\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.205 - 0s 798us/step - loss: 0.2105 - val_loss: 1.1122\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.230 - 0s 845us/step - loss: 0.2010 - val_loss: 1.1779\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.174 - 0s 940us/step - loss: 0.1937 - val_loss: 0.9155\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.412 - ETA: 0s - loss: 0.223 - 0s 902us/step - loss: 0.2029 - val_loss: 0.9703\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.201 - 0s 817us/step - loss: 0.1902 - val_loss: 0.9832\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.154 - 0s 817us/step - loss: 0.1858 - val_loss: 0.7932\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.68 - ETA: 0s - loss: 0.6730 - 1s 8ms/step - loss: 0.8033 - val_loss: 0.4408\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.062 - ETA: 0s - loss: 0.717 - 0s 807us/step - loss: 0.6143 - val_loss: 0.6869\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.677 - ETA: 0s - loss: 0.585 - 0s 902us/step - loss: 0.5504 - val_loss: 0.7623\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.321 - 0s 931us/step - loss: 0.5034 - val_loss: 0.9308\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.450 - 0s 883us/step - loss: 0.4698 - val_loss: 1.2397\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.279 - 0s 950us/step - loss: 0.4500 - val_loss: 1.1593\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.514 - 0s 893us/step - loss: 0.4272 - val_loss: 1.2785\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.420 - ETA: 0s - loss: 0.434 - 0s 902us/step - loss: 0.4006 - val_loss: 1.2904\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.992 - ETA: 0s - loss: 0.483 - 0s 874us/step - loss: 0.4067 - val_loss: 1.2671\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.381 - 0s 1ms/step - loss: 0.3784 - val_loss: 1.2931\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.348 - 0s 874us/step - loss: 0.3667 - val_loss: 1.4498\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.446 - 0s 864us/step - loss: 0.3644 - val_loss: 1.6234\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.345 - 0s 1ms/step - loss: 0.3430 - val_loss: 1.4315\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.339 - 0s 903us/step - loss: 0.3392 - val_loss: 1.6042\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.374 - 0s 874us/step - loss: 0.3098 - val_loss: 1.4704\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.430 - ETA: 0s - loss: 0.357 - 0s 808us/step - loss: 0.3119 - val_loss: 1.4896\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.247 - 0s 950us/step - loss: 0.3102 - val_loss: 1.6352\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.469 - ETA: 0s - loss: 0.220 - 0s 950us/step - loss: 0.2950 - val_loss: 1.5766\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.305 - 0s 836us/step - loss: 0.3001 - val_loss: 1.8813\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.326 - 0s 902us/step - loss: 0.2684 - val_loss: 1.6159\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.292 - ETA: 0s - loss: 0.315 - 0s 978us/step - loss: 0.2778 - val_loss: 1.9736\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.245 - 0s 836us/step - loss: 0.2485 - val_loss: 1.8643\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.243 - 0s 950us/step - loss: 0.2626 - val_loss: 1.7345\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.278 - 0s 1ms/step - loss: 0.2519 - val_loss: 1.9874\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.259 - 0s 969us/step - loss: 0.2490 - val_loss: 1.9043\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.244 - 0s 903us/step - loss: 0.2432 - val_loss: 1.9356\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.320 - 0s 978us/step - loss: 0.2219 - val_loss: 2.1878\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.228 - 0s 874us/step - loss: 0.2174 - val_loss: 1.9504\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.175 - 0s 874us/step - loss: 0.2040 - val_loss: 1.9992\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.168 - 0s 883us/step - loss: 0.2147 - val_loss: 2.1238\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.186 - 0s 770us/step - loss: 0.1825 - val_loss: 2.4551\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 7\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.35 - ETA: 0s - loss: 0.8291 - 1s 8ms/step - loss: 0.6913 - val_loss: 0.8139\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.587 - 0s 788us/step - loss: 0.4871 - val_loss: 1.0403\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.444 - ETA: 0s - loss: 0.503 - 0s 788us/step - loss: 0.4686 - val_loss: 0.9790\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.437 - 0s 874us/step - loss: 0.4388 - val_loss: 0.9904\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.388 - 0s 883us/step - loss: 0.4050 - val_loss: 1.1502\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.432 - 0s 855us/step - loss: 0.3838 - val_loss: 1.0397\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.360 - 0s 826us/step - loss: 0.3626 - val_loss: 1.0123\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.307 - 0s 826us/step - loss: 0.3685 - val_loss: 0.9502\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.518 - ETA: 0s - loss: 0.380 - 0s 912us/step - loss: 0.3439 - val_loss: 0.9861\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.347 - 0s 978us/step - loss: 0.3287 - val_loss: 1.2547\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.312 - 0s 826us/step - loss: 0.3251 - val_loss: 0.8778\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.793 - ETA: 0s - loss: 0.330 - 0s 931us/step - loss: 0.3088 - val_loss: 1.1509\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.294 - 0s 855us/step - loss: 0.2969 - val_loss: 0.8157\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.820 - ETA: 0s - loss: 0.334 - 0s 921us/step - loss: 0.2908 - val_loss: 1.0730\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.263 - 0s 845us/step - loss: 0.2731 - val_loss: 0.8879\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.241 - 0s 826us/step - loss: 0.2735 - val_loss: 1.1235\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.308 - 0s 836us/step - loss: 0.2635 - val_loss: 0.9438\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.209 - 0s 931us/step - loss: 0.2478 - val_loss: 0.9202\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.289 - 0s 893us/step - loss: 0.2450 - val_loss: 1.0032\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.247 - 0s 883us/step - loss: 0.2434 - val_loss: 0.8021\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.211 - 0s 912us/step - loss: 0.2308 - val_loss: 1.0922\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.254 - 0s 912us/step - loss: 0.2217 - val_loss: 0.9327\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.232 - 0s 874us/step - loss: 0.2167 - val_loss: 0.9008\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.449 - ETA: 0s - loss: 0.207 - 0s 912us/step - loss: 0.2142 - val_loss: 0.8681\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.243 - 0s 826us/step - loss: 0.2103 - val_loss: 0.9760\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.205 - 0s 1ms/step - loss: 0.2008 - val_loss: 1.0279\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.628 - ETA: 0s - loss: 0.263 - 0s 883us/step - loss: 0.2095 - val_loss: 0.9219\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.152 - 0s 931us/step - loss: 0.1884 - val_loss: 1.1836\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.167 - 0s 855us/step - loss: 0.1936 - val_loss: 0.9080\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.188 - 0s 969us/step - loss: 0.1923 - val_loss: 1.2834\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.150 - 0s 864us/step - loss: 0.1738 - val_loss: 0.8615\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.149 - 0s 864us/step - loss: 0.1713 - val_loss: 1.0303\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.169 - 0s 893us/step - loss: 0.1678 - val_loss: 0.8138\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.182 - 0s 950us/step - loss: 0.1575 - val_loss: 1.1234\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.164 - 0s 864us/step - loss: 0.1562 - val_loss: 0.9011\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.148 - 0s 940us/step - loss: 0.1542 - val_loss: 1.0214\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.156 - 0s 1ms/step - loss: 0.1624 - val_loss: 0.9531\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.487 - ETA: 0s - loss: 0.136 - 0s 807us/step - loss: 0.1558 - val_loss: 1.0417\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.176 - 0s 817us/step - loss: 0.1647 - val_loss: 0.9808\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.124 - 0s 883us/step - loss: 0.1502 - val_loss: 1.0020\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.118 - 0s 912us/step - loss: 0.1358 - val_loss: 1.0311\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.132 - 0s 931us/step - loss: 0.1343 - val_loss: 0.9513\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.136 - 0s 903us/step - loss: 0.1380 - val_loss: 0.9444\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.099 - 0s 969us/step - loss: 0.1295 - val_loss: 1.2030\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.136 - 0s 893us/step - loss: 0.1353 - val_loss: 1.0896\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.113 - 0s 864us/step - loss: 0.1263 - val_loss: 1.0709\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.114 - 0s 902us/step - loss: 0.1214 - val_loss: 1.1270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.133 - 0s 836us/step - loss: 0.1328 - val_loss: 1.0403\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.150 - 0s 836us/step - loss: 0.1363 - val_loss: 1.0694\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.108 - 0s 817us/step - loss: 0.1250 - val_loss: 1.1940\n",
      "Epoch 00050: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 8\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.65 - ETA: 0s - loss: 0.7214 - 1s 8ms/step - loss: 0.6202 - val_loss: 0.8318\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.498 - 0s 808us/step - loss: 0.4407 - val_loss: 1.0011\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.166 - 0s 845us/step - loss: 0.3699 - val_loss: 0.7795\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.406 - 0s 807us/step - loss: 0.3563 - val_loss: 0.8411\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.335 - 0s 817us/step - loss: 0.2997 - val_loss: 0.6565\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.262 - 0s 769us/step - loss: 0.2569 - val_loss: 0.6469\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.279 - 0s 845us/step - loss: 0.2471 - val_loss: 0.4396\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.198 - 0s 883us/step - loss: 0.2149 - val_loss: 0.4902\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.232 - 0s 884us/step - loss: 0.2018 - val_loss: 0.4599\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.225 - 0s 940us/step - loss: 0.1878 - val_loss: 0.4667\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.162 - 0s 827us/step - loss: 0.1726 - val_loss: 0.5298\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.188 - 0s 826us/step - loss: 0.1712 - val_loss: 0.4586\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.149 - 0s 836us/step - loss: 0.1427 - val_loss: 0.4677\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.102 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.156 - 0s 2ms/step - loss: 0.1554 - val_loss: 0.4505\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.122 - 0s 855us/step - loss: 0.1233 - val_loss: 0.4856\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.121 - 0s 874us/step - loss: 0.1227 - val_loss: 0.5560\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.124 - 0s 921us/step - loss: 0.1181 - val_loss: 0.4896\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.102 - 0s 1ms/step - loss: 0.0933 - val_loss: 0.6389\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.091 - 0s 788us/step - loss: 0.1019 - val_loss: 0.4768\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.089 - 0s 731us/step - loss: 0.0911 - val_loss: 0.6225\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.082 - 0s 883us/step - loss: 0.0972 - val_loss: 0.5132\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.082 - 0s 1ms/step - loss: 0.0898 - val_loss: 0.4928\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.077 - 0s 3ms/step - loss: 0.0774 - val_loss: 0.5578\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.086 - 0s 3ms/step - loss: 0.0855 - val_loss: 0.5643\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.083 - 0s 2ms/step - loss: 0.0799 - val_loss: 0.5842\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - 0s 3ms/step - loss: 0.0779 - val_loss: 0.5352\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.073 - 0s 2ms/step - loss: 0.0771 - val_loss: 0.5424\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.070 - 0s 846us/step - loss: 0.0749 - val_loss: 0.5375\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.081 - 0s 817us/step - loss: 0.0780 - val_loss: 0.4673\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.080 - 0s 817us/step - loss: 0.0746 - val_loss: 0.4427\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.075 - 0s 864us/step - loss: 0.0687 - val_loss: 0.5531\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.056 - 0s 988us/step - loss: 0.0709 - val_loss: 0.4946\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.050 - 0s 1ms/step - loss: 0.0608 - val_loss: 0.5289\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.061 - 0s 675us/step - loss: 0.0636 - val_loss: 0.4546\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.057 - 0s 674us/step - loss: 0.0566 - val_loss: 0.5479\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.067 - 0s 665us/step - loss: 0.0715 - val_loss: 0.4618\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.052 - 0s 1ms/step - loss: 0.0536 - val_loss: 0.4396\n",
      "Epoch 00037: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 9\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 4.07 - ETA: 0s - loss: 0.5790 - 1s 8ms/step - loss: 0.7092 - val_loss: 0.5335\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.448 - 0s 798us/step - loss: 0.5386 - val_loss: 0.8314\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.588 - 0s 798us/step - loss: 0.4937 - val_loss: 0.8656\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.353 - 0s 807us/step - loss: 0.4401 - val_loss: 0.8346\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.377 - 0s 826us/step - loss: 0.3871 - val_loss: 0.7088\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.341 - 0s 893us/step - loss: 0.3248 - val_loss: 0.9112\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.257 - 0s 817us/step - loss: 0.2793 - val_loss: 0.6273\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.278 - 0s 798us/step - loss: 0.2320 - val_loss: 0.7005\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.238 - 0s 940us/step - loss: 0.2368 - val_loss: 0.5733\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.210 - 0s 817us/step - loss: 0.1925 - val_loss: 0.6425\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.193 - 0s 722us/step - loss: 0.1728 - val_loss: 0.4904\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.504 - ETA: 0s - loss: 0.168 - 0s 846us/step - loss: 0.1654 - val_loss: 0.6609\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.158 - 0s 826us/step - loss: 0.1401 - val_loss: 0.5343\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.135 - 0s 1ms/step - loss: 0.1280 - val_loss: 0.8285\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.146 - 0s 769us/step - loss: 0.1384 - val_loss: 0.6186\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.163 - 0s 902us/step - loss: 0.1315 - val_loss: 0.7785\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.133 - 0s 893us/step - loss: 0.1276 - val_loss: 1.0042\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.131 - 0s 826us/step - loss: 0.1280 - val_loss: 0.8371\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.118 - 0s 1ms/step - loss: 0.1179 - val_loss: 1.3679\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.089 - 0s 902us/step - loss: 0.1165 - val_loss: 0.8512\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.128 - 0s 798us/step - loss: 0.1203 - val_loss: 1.1334\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.088 - 0s 1ms/step - loss: 0.1239 - val_loss: 0.9317\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.111 - 0s 808us/step - loss: 0.1107 - val_loss: 0.8825\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.109 - 0s 1ms/step - loss: 0.1010 - val_loss: 1.0677\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.112 - 0s 1ms/step - loss: 0.1072 - val_loss: 1.1716\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.759 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.102 - 0s 1ms/step - loss: 0.1012 - val_loss: 1.1086\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.054 - 0s 1ms/step - loss: 0.1025 - val_loss: 1.5640\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.104 - 0s 959us/step - loss: 0.0942 - val_loss: 1.2977\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.069 - 0s 1ms/step - loss: 0.0926 - val_loss: 1.2216\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.060 - 0s 921us/step - loss: 0.0890 - val_loss: 1.4179\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.069 - 0s 874us/step - loss: 0.0987 - val_loss: 1.6876\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.078 - 0s 1ms/step - loss: 0.0756 - val_loss: 1.5057\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.084 - 0s 1ms/step - loss: 0.0824 - val_loss: 1.5062\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.074 - 0s 845us/step - loss: 0.0702 - val_loss: 1.4654\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.094 - 0s 931us/step - loss: 0.0880 - val_loss: 1.3636\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.092 - 0s 902us/step - loss: 0.0738 - val_loss: 1.4150\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.076 - 0s 997us/step - loss: 0.0666 - val_loss: 1.3315\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.082 - 0s 874us/step - loss: 0.0785 - val_loss: 1.7201\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.065 - 0s 950us/step - loss: 0.0832 - val_loss: 1.1719\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.066 - 0s 874us/step - loss: 0.0830 - val_loss: 1.5847\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.092 - 0s 883us/step - loss: 0.0739 - val_loss: 1.2472\n",
      "Epoch 00041: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.09 - ETA: 0s - loss: 0.8258 - 1s 8ms/step - loss: 0.7859 - val_loss: 0.5721\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.361 - 0s 845us/step - loss: 0.5442 - val_loss: 0.7648\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.372 - 0s 836us/step - loss: 0.4455 - val_loss: 0.8472\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.359 - 0s 959us/step - loss: 0.3876 - val_loss: 0.8610\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.584 - ETA: 0s - loss: 0.405 - 0s 798us/step - loss: 0.3566 - val_loss: 0.7351\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.290 - 0s 788us/step - loss: 0.2885 - val_loss: 0.9321\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.266 - 0s 1ms/step - loss: 0.2642 - val_loss: 0.5042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.300 - 0s 807us/step - loss: 0.2613 - val_loss: 0.5661\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.151 - 0s 817us/step - loss: 0.2189 - val_loss: 0.5148\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.244 - 0s 1ms/step - loss: 0.2055 - val_loss: 0.7859\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.142 - 0s 836us/step - loss: 0.1639 - val_loss: 0.5327\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.929 - ETA: 0s - loss: 0.226 - 0s 769us/step - loss: 0.1939 - val_loss: 0.6514\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.158 - 0s 988us/step - loss: 0.1470 - val_loss: 0.5915\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.107 - 0s 997us/step - loss: 0.1556 - val_loss: 0.5737\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.163 - 0s 788us/step - loss: 0.1329 - val_loss: 0.5919\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.125 - 0s 1ms/step - loss: 0.1214 - val_loss: 0.6256\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.130 - 0s 817us/step - loss: 0.1197 - val_loss: 0.7244\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.071 - 0s 826us/step - loss: 0.0975 - val_loss: 0.5616\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.095 - 0s 798us/step - loss: 0.1053 - val_loss: 0.5202\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.105 - 0s 817us/step - loss: 0.1050 - val_loss: 0.6253\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.078 - 0s 874us/step - loss: 0.0773 - val_loss: 0.6050\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - 0s 864us/step - loss: 0.0738 - val_loss: 0.5363\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.081 - 0s 845us/step - loss: 0.0776 - val_loss: 0.6413\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.078 - 0s 817us/step - loss: 0.0786 - val_loss: 0.4953\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.070 - 0s 902us/step - loss: 0.0723 - val_loss: 0.6027\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.102 - 0s 836us/step - loss: 0.0861 - val_loss: 0.6145\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.047 - 0s 855us/step - loss: 0.0580 - val_loss: 0.5613\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.056 - 0s 836us/step - loss: 0.0550 - val_loss: 0.6112\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.062 - 0s 845us/step - loss: 0.0617 - val_loss: 0.5808\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.066 - 0s 1ms/step - loss: 0.0572 - val_loss: 0.5882\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.067 - 0s 1ms/step - loss: 0.0666 - val_loss: 0.5394\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.048 - 0s 2ms/step - loss: 0.0516 - val_loss: 0.4505\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.069 - 0s 1ms/step - loss: 0.0632 - val_loss: 0.7776\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.052 - 0s 836us/step - loss: 0.0508 - val_loss: 0.5123\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.063 - 0s 731us/step - loss: 0.0638 - val_loss: 0.6297\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.047 - 0s 731us/step - loss: 0.0510 - val_loss: 0.5017\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.050 - 0s 779us/step - loss: 0.0461 - val_loss: 0.5914\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.042 - 0s 826us/step - loss: 0.0395 - val_loss: 0.5524\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.051 - 0s 807us/step - loss: 0.0512 - val_loss: 0.5227\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.042 - 0s 1ms/step - loss: 0.0394 - val_loss: 0.7102\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.045 - 0s 798us/step - loss: 0.0513 - val_loss: 0.5784\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.040 - 0s 741us/step - loss: 0.0427 - val_loss: 0.5404\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.048 - 0s 731us/step - loss: 0.0468 - val_loss: 0.5760\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.052 - 0s 789us/step - loss: 0.0463 - val_loss: 0.4522\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.049 - 0s 864us/step - loss: 0.0595 - val_loss: 0.7223\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.039 - 0s 769us/step - loss: 0.0349 - val_loss: 0.5832\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.049 - 0s 760us/step - loss: 0.0472 - val_loss: 0.5335\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.028 - 0s 741us/step - loss: 0.0331 - val_loss: 0.5527\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.036 - 0s 750us/step - loss: 0.0400 - val_loss: 0.7405\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.034 - 0s 779us/step - loss: 0.0314 - val_loss: 0.5827\n",
      "(105,) (15,)\n",
      "model compiled 11\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.99 - ETA: 0s - loss: 0.7539 - 1s 8ms/step - loss: 0.6119 - val_loss: 0.7518\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.331 - ETA: 0s - loss: 0.298 - 0s 845us/step - loss: 0.3844 - val_loss: 1.2061\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.405 - 0s 874us/step - loss: 0.3854 - val_loss: 1.3446\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.280 - 0s 884us/step - loss: 0.3272 - val_loss: 1.1509\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.291 - 0s 855us/step - loss: 0.2878 - val_loss: 1.2457\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.529 - ETA: 0s - loss: 0.286 - 0s 836us/step - loss: 0.2651 - val_loss: 1.1610\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.227 - 0s 826us/step - loss: 0.2388 - val_loss: 1.0682\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.209 - 0s 798us/step - loss: 0.2287 - val_loss: 1.0589\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.199 - 0s 788us/step - loss: 0.1971 - val_loss: 1.1784\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.216 - 0s 855us/step - loss: 0.1952 - val_loss: 1.1936\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.188 - 0s 817us/step - loss: 0.1948 - val_loss: 1.0850\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.188 - 0s 817us/step - loss: 0.1977 - val_loss: 0.9194\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.183 - 0s 902us/step - loss: 0.1985 - val_loss: 1.1107\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.196 - 0s 826us/step - loss: 0.1786 - val_loss: 1.2558\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.451 - ETA: 0s - loss: 0.196 - 0s 845us/step - loss: 0.1805 - val_loss: 1.1549\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.558 - ETA: 0s - loss: 0.184 - 0s 817us/step - loss: 0.1792 - val_loss: 1.4123\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.163 - 0s 836us/step - loss: 0.1597 - val_loss: 1.1959\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.148 - 0s 836us/step - loss: 0.1553 - val_loss: 1.2681\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.157 - 0s 845us/step - loss: 0.1745 - val_loss: 1.1699\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.123 - 0s 836us/step - loss: 0.1596 - val_loss: 1.4736\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.170 - 0s 864us/step - loss: 0.1495 - val_loss: 1.1939\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.134 - 0s 845us/step - loss: 0.1411 - val_loss: 1.2008\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.132 - 0s 912us/step - loss: 0.1464 - val_loss: 1.2897\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.122 - 0s 807us/step - loss: 0.1426 - val_loss: 1.2438\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.110 - 0s 836us/step - loss: 0.1276 - val_loss: 1.0488\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.153 - 0s 893us/step - loss: 0.1274 - val_loss: 1.4090\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.111 - 0s 864us/step - loss: 0.1101 - val_loss: 1.1449\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.103 - 0s 1ms/step - loss: 0.1108 - val_loss: 1.3731\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.125 - 0s 883us/step - loss: 0.1154 - val_loss: 1.1796\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.127 - 0s 855us/step - loss: 0.1226 - val_loss: 1.0661\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.122 - 0s 845us/step - loss: 0.1131 - val_loss: 1.6110\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 12\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 3.73 - ETA: 0s - loss: 0.8954 - 1s 9ms/step - loss: 0.7188 - val_loss: 0.5498\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.413 - 0s 826us/step - loss: 0.3732 - val_loss: 1.1849\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.329 - 0s 779us/step - loss: 0.3382 - val_loss: 1.3304\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.318 - 0s 921us/step - loss: 0.3054 - val_loss: 1.7504\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.205 - 0s 817us/step - loss: 0.2528 - val_loss: 1.7289\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.211 - 0s 2ms/step - loss: 0.2232 - val_loss: 2.1202\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.225 - 0s 864us/step - loss: 0.2004 - val_loss: 1.8832\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.228 - 0s 874us/step - loss: 0.1938 - val_loss: 2.1223\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.214 - 0s 817us/step - loss: 0.1847 - val_loss: 2.0673\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.172 - 0s 817us/step - loss: 0.1707 - val_loss: 2.1959\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.153 - 0s 817us/step - loss: 0.1591 - val_loss: 2.2861\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.147 - 0s 883us/step - loss: 0.1433 - val_loss: 2.3387\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.483 - ETA: 0s - loss: 0.151 - 0s 912us/step - loss: 0.1488 - val_loss: 2.4037\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.148 - 0s 912us/step - loss: 0.1337 - val_loss: 2.4560\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.135 - 0s 940us/step - loss: 0.1332 - val_loss: 2.4582\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.121 - 0s 855us/step - loss: 0.1271 - val_loss: 2.6525\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.118 - 0s 912us/step - loss: 0.1228 - val_loss: 2.2447\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.105 - 0s 912us/step - loss: 0.1175 - val_loss: 2.5756\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.103 - 0s 855us/step - loss: 0.1199 - val_loss: 2.5370\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.131 - 0s 1ms/step - loss: 0.1164 - val_loss: 2.4152\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.115 - 0s 845us/step - loss: 0.1088 - val_loss: 2.4024\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.118 - 0s 864us/step - loss: 0.1106 - val_loss: 2.6124\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.088 - 0s 912us/step - loss: 0.1015 - val_loss: 2.3604\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.118 - 0s 788us/step - loss: 0.1054 - val_loss: 2.4658\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.097 - 0s 779us/step - loss: 0.0988 - val_loss: 2.4906\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.098 - 0s 731us/step - loss: 0.0913 - val_loss: 2.3246\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.092 - 0s 807us/step - loss: 0.0941 - val_loss: 2.5107\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.073 - 0s 826us/step - loss: 0.0953 - val_loss: 1.9656\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.103 - 0s 798us/step - loss: 0.0979 - val_loss: 2.7697\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.086 - 0s 798us/step - loss: 0.0894 - val_loss: 2.3629\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.071 - 0s 798us/step - loss: 0.0915 - val_loss: 2.4539\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 13\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.22 - ETA: 0s - loss: 0.5083 - 1s 8ms/step - loss: 0.6932 - val_loss: 0.6442\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.463 - 0s 807us/step - loss: 0.5050 - val_loss: 1.0999\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.406 - ETA: 0s - loss: 0.510 - 0s 769us/step - loss: 0.4758 - val_loss: 1.0654\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.333 - ETA: 0s - loss: 0.522 - 0s 788us/step - loss: 0.4541 - val_loss: 1.0619\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.430 - 0s 826us/step - loss: 0.4258 - val_loss: 1.0413\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.680 - ETA: 0s - loss: 0.424 - 0s 760us/step - loss: 0.3919 - val_loss: 1.0945\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.353 - 0s 807us/step - loss: 0.3488 - val_loss: 1.0878\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.347 - 0s 817us/step - loss: 0.3308 - val_loss: 1.4301\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.970 - ETA: 0s - loss: 0.324 - 0s 836us/step - loss: 0.2986 - val_loss: 1.3216\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.326 - 0s 788us/step - loss: 0.3017 - val_loss: 1.4969\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.274 - 0s 798us/step - loss: 0.2698 - val_loss: 1.4434\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.332 - 0s 817us/step - loss: 0.2799 - val_loss: 1.4209\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.242 - 0s 836us/step - loss: 0.2495 - val_loss: 1.4680\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.257 - 0s 836us/step - loss: 0.2539 - val_loss: 1.6590\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.203 - 0s 807us/step - loss: 0.2349 - val_loss: 1.6010\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.223 - 0s 855us/step - loss: 0.2213 - val_loss: 2.0536\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.222 - 0s 788us/step - loss: 0.2209 - val_loss: 1.6508\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.208 - 0s 769us/step - loss: 0.1933 - val_loss: 1.8767\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.159 - 0s 855us/step - loss: 0.1880 - val_loss: 1.8523\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.208 - 0s 826us/step - loss: 0.2021 - val_loss: 1.9199\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.156 - 0s 788us/step - loss: 0.1809 - val_loss: 1.7236\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.180 - 0s 836us/step - loss: 0.1903 - val_loss: 1.9010\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.188 - 0s 817us/step - loss: 0.1727 - val_loss: 1.9812\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.187 - 0s 845us/step - loss: 0.1671 - val_loss: 1.8089\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.148 - 0s 807us/step - loss: 0.1560 - val_loss: 2.0418\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.118 - 0s 846us/step - loss: 0.1508 - val_loss: 1.7758\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.145 - 0s 874us/step - loss: 0.1654 - val_loss: 2.2078\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.123 - 0s 941us/step - loss: 0.1529 - val_loss: 1.8420\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.151 - 0s 798us/step - loss: 0.1413 - val_loss: 1.9495\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.143 - 0s 807us/step - loss: 0.1481 - val_loss: 1.9952\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.140 - 0s 912us/step - loss: 0.1384 - val_loss: 1.9923\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 14\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 3.63 - ETA: 0s - loss: 0.9820 - 1s 8ms/step - loss: 0.8035 - val_loss: 0.4129\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.527 - 0s 845us/step - loss: 0.5442 - val_loss: 0.8496\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.405 - 0s 864us/step - loss: 0.4852 - val_loss: 0.7784\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.327 - ETA: 0s - loss: 0.472 - 0s 826us/step - loss: 0.4315 - val_loss: 1.0560\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.255 - ETA: 0s - loss: 0.397 - 0s 807us/step - loss: 0.3900 - val_loss: 1.1627\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.310 - 0s 855us/step - loss: 0.3619 - val_loss: 1.2379\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.363 - 0s 807us/step - loss: 0.3218 - val_loss: 1.5108\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.263 - ETA: 0s - loss: 0.295 - 0s 817us/step - loss: 0.3013 - val_loss: 1.7768\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.274 - 0s 1ms/step - loss: 0.2726 - val_loss: 1.9978\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.472 - ETA: 0s - loss: 0.268 - 0s 826us/step - loss: 0.2586 - val_loss: 2.0574\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.203 - 0s 779us/step - loss: 0.2442 - val_loss: 2.4888\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.277 - 0s 817us/step - loss: 0.2411 - val_loss: 2.4734\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.226 - 0s 798us/step - loss: 0.2222 - val_loss: 2.6777\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.267 - 0s 817us/step - loss: 0.2376 - val_loss: 2.9003\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.251 - 0s 779us/step - loss: 0.2235 - val_loss: 2.9763\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.213 - 0s 817us/step - loss: 0.2213 - val_loss: 3.3213\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.221 - 0s 836us/step - loss: 0.2213 - val_loss: 3.4142\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.183 - 0s 779us/step - loss: 0.1993 - val_loss: 3.4003\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.206 - 0s 874us/step - loss: 0.1947 - val_loss: 3.2616\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.202 - 0s 674us/step - loss: 0.1938 - val_loss: 3.5423\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.198 - 0s 732us/step - loss: 0.1900 - val_loss: 3.4746\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.183 - 0s 751us/step - loss: 0.1949 - val_loss: 3.5335\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.208 - 0s 779us/step - loss: 0.1853 - val_loss: 3.7394\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.144 - 0s 760us/step - loss: 0.1691 - val_loss: 3.7592\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.159 - 0s 712us/step - loss: 0.1746 - val_loss: 3.4862\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.164 - 0s 741us/step - loss: 0.1633 - val_loss: 3.7062\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.787 - ETA: 0s - loss: 0.180 - 0s 751us/step - loss: 0.1694 - val_loss: 3.5650\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.163 - 0s 779us/step - loss: 0.1647 - val_loss: 3.5939\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.171 - 0s 741us/step - loss: 0.1638 - val_loss: 3.4241\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.148 - 0s 731us/step - loss: 0.1494 - val_loss: 3.3032\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.156 - 0s 836us/step - loss: 0.1561 - val_loss: 4.0349\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 15\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.48 - ETA: 0s - loss: 0.7337 - 1s 8ms/step - loss: 0.7274 - val_loss: 0.4534\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.499 - 0s 864us/step - loss: 0.5488 - val_loss: 0.7198\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.586 - ETA: 0s - loss: 0.361 - 0s 988us/step - loss: 0.4957 - val_loss: 0.7335\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.874 - ETA: 0s - loss: 0.495 - 0s 788us/step - loss: 0.4651 - val_loss: 0.6278\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.489 - ETA: 0s - loss: 0.367 - 0s 940us/step - loss: 0.4170 - val_loss: 0.5913\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.378 - 0s 826us/step - loss: 0.3688 - val_loss: 0.5477\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.339 - 0s 836us/step - loss: 0.3494 - val_loss: 0.5311\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.349 - 0s 826us/step - loss: 0.3361 - val_loss: 0.6079\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.519 - ETA: 0s - loss: 0.329 - 0s 836us/step - loss: 0.3025 - val_loss: 0.5050\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.285 - 0s 912us/step - loss: 0.2961 - val_loss: 0.4824\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.309 - 0s 874us/step - loss: 0.2789 - val_loss: 0.5022\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.242 - 0s 978us/step - loss: 0.2576 - val_loss: 0.4340\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.256 - 0s 940us/step - loss: 0.2471 - val_loss: 0.4679\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.279 - 0s 855us/step - loss: 0.2361 - val_loss: 0.4039\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.263 - 0s 912us/step - loss: 0.2432 - val_loss: 0.3801\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.243 - 0s 788us/step - loss: 0.2363 - val_loss: 0.4513\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.194 - 0s 845us/step - loss: 0.2119 - val_loss: 0.4368\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.230 - 0s 921us/step - loss: 0.2093 - val_loss: 0.4601\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.140 - 0s 855us/step - loss: 0.2077 - val_loss: 0.4000\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.430 - ETA: 0s - loss: 0.207 - 0s 864us/step - loss: 0.2057 - val_loss: 0.4777\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.155 - 0s 864us/step - loss: 0.1678 - val_loss: 0.4615\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.202 - 0s 807us/step - loss: 0.1774 - val_loss: 0.4147\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.178 - 0s 807us/step - loss: 0.1725 - val_loss: 0.4379\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.168 - 0s 874us/step - loss: 0.1641 - val_loss: 0.4880\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.168 - 0s 769us/step - loss: 0.1607 - val_loss: 0.4517\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.178 - 0s 788us/step - loss: 0.1681 - val_loss: 0.4782\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.145 - 0s 789us/step - loss: 0.1568 - val_loss: 0.3891\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.145 - 0s 826us/step - loss: 0.1753 - val_loss: 0.5241\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.156 - 0s 807us/step - loss: 0.1525 - val_loss: 0.4758\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.144 - 0s 826us/step - loss: 0.1456 - val_loss: 0.5513\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.143 - 0s 788us/step - loss: 0.1401 - val_loss: 0.4595\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.431 - ETA: 0s - loss: 0.165 - 0s 864us/step - loss: 0.1524 - val_loss: 0.4476\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.150 - 0s 893us/step - loss: 0.1296 - val_loss: 0.5363\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.382 - ETA: 0s - loss: 0.128 - 0s 836us/step - loss: 0.1237 - val_loss: 0.5431\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.139 - 0s 855us/step - loss: 0.1280 - val_loss: 0.5689\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.127 - 0s 827us/step - loss: 0.1289 - val_loss: 0.5575\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.102 - 0s 788us/step - loss: 0.1059 - val_loss: 0.4705\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.088 - 0s 855us/step - loss: 0.1093 - val_loss: 0.5463\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.482 - ETA: 0s - loss: 0.106 - 0s 902us/step - loss: 0.1010 - val_loss: 0.4693\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.115 - 0s 874us/step - loss: 0.1165 - val_loss: 0.5084\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.091 - 0s 855us/step - loss: 0.1097 - val_loss: 0.4987\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.092 - 0s 855us/step - loss: 0.1095 - val_loss: 0.5161\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.104 - 0s 845us/step - loss: 0.1035 - val_loss: 0.4760\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.085 - 0s 807us/step - loss: 0.0971 - val_loss: 0.5617\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.109 - 0s 940us/step - loss: 0.1171 - val_loss: 0.5875\n",
      "Epoch 00045: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 16\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.56 - ETA: 0s - loss: 0.8019 - 1s 8ms/step - loss: 0.7161 - val_loss: 0.4731\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.519 - 0s 845us/step - loss: 0.5171 - val_loss: 0.6386\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.535 - 0s 827us/step - loss: 0.4731 - val_loss: 0.6768\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.772 - 0s 978us/step - loss: 0.4591 - val_loss: 0.6515\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.451 - 0s 788us/step - loss: 0.4178 - val_loss: 0.6387\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.409 - 0s 760us/step - loss: 0.3921 - val_loss: 0.6537\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.244 - 0s 845us/step - loss: 0.3618 - val_loss: 0.5770\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.359 - 0s 760us/step - loss: 0.3697 - val_loss: 0.5593\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.236 - ETA: 0s - loss: 0.341 - 0s 732us/step - loss: 0.3257 - val_loss: 0.6135\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.342 - 0s 836us/step - loss: 0.3383 - val_loss: 0.7012\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.305 - 0s 874us/step - loss: 0.3126 - val_loss: 0.7111\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.277 - 0s 1ms/step - loss: 0.2958 - val_loss: 0.7026\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.470 - ETA: 0s - loss: 0.299 - 0s 760us/step - loss: 0.2873 - val_loss: 0.6442\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.243 - 0s 826us/step - loss: 0.2855 - val_loss: 0.7335\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.284 - 0s 893us/step - loss: 0.2782 - val_loss: 0.6625\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.283 - 0s 893us/step - loss: 0.2690 - val_loss: 0.6851\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.217 - 0s 845us/step - loss: 0.2633 - val_loss: 0.6914\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.243 - 0s 864us/step - loss: 0.2571 - val_loss: 0.6574\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.244 - 0s 836us/step - loss: 0.2645 - val_loss: 0.8350\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.305 - 0s 864us/step - loss: 0.2423 - val_loss: 0.8622\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.245 - 0s 864us/step - loss: 0.2273 - val_loss: 0.7927\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.215 - 0s 988us/step - loss: 0.2134 - val_loss: 0.8080\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.184 - 0s 855us/step - loss: 0.2200 - val_loss: 0.8128\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.196 - 0s 874us/step - loss: 0.2178 - val_loss: 0.8282\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.203 - 0s 855us/step - loss: 0.2105 - val_loss: 0.8382\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.207 - 0s 807us/step - loss: 0.1983 - val_loss: 0.7959\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.194 - 0s 789us/step - loss: 0.2077 - val_loss: 0.8777\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.213 - 0s 779us/step - loss: 0.2001 - val_loss: 0.8647\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.200 - 0s 779us/step - loss: 0.2077 - val_loss: 0.6827\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.169 - 0s 779us/step - loss: 0.1707 - val_loss: 0.7449\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.141 - 0s 845us/step - loss: 0.1827 - val_loss: 0.7238\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 17\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.16 - ETA: 0s - loss: 0.7510 - 1s 8ms/step - loss: 0.8187 - val_loss: 0.4337\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.660 - 0s 997us/step - loss: 0.6540 - val_loss: 0.7065\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.529 - 0s 826us/step - loss: 0.6245 - val_loss: 0.6274\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.150 - ETA: 0s - loss: 0.447 - 0s 788us/step - loss: 0.5849 - val_loss: 0.6659\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.464 - 0s 836us/step - loss: 0.5686 - val_loss: 0.8297\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.327 - ETA: 0s - loss: 0.608 - 0s 817us/step - loss: 0.5273 - val_loss: 0.7644\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.539 - 0s 760us/step - loss: 0.4790 - val_loss: 0.8589\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.533 - 0s 827us/step - loss: 0.4606 - val_loss: 0.8440\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.346 - 0s 827us/step - loss: 0.4427 - val_loss: 0.9009\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.451 - 0s 779us/step - loss: 0.4065 - val_loss: 1.0197\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.258 - 0s 874us/step - loss: 0.3918 - val_loss: 1.1190\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.345 - 0s 845us/step - loss: 0.3833 - val_loss: 1.2184\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.444 - 0s 893us/step - loss: 0.3710 - val_loss: 1.3701\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.362 - 0s 845us/step - loss: 0.3497 - val_loss: 1.2804\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.322 - 0s 845us/step - loss: 0.3239 - val_loss: 1.1959\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.320 - 0s 921us/step - loss: 0.3272 - val_loss: 1.5307\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.330 - 0s 893us/step - loss: 0.3145 - val_loss: 1.4492\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.261 - 0s 893us/step - loss: 0.3079 - val_loss: 1.7365\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.344 - 0s 912us/step - loss: 0.3074 - val_loss: 1.6366\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.407 - ETA: 0s - loss: 0.348 - 0s 874us/step - loss: 0.2909 - val_loss: 1.6243\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.219 - 0s 893us/step - loss: 0.2750 - val_loss: 1.5349\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.265 - 0s 826us/step - loss: 0.2807 - val_loss: 1.7276\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.268 - 0s 865us/step - loss: 0.2810 - val_loss: 1.5789\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.541 - ETA: 0s - loss: 0.318 - 0s 855us/step - loss: 0.2792 - val_loss: 1.8616\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.250 - 0s 855us/step - loss: 0.2640 - val_loss: 1.8145\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.195 - 0s 940us/step - loss: 0.2595 - val_loss: 2.3790\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.256 - 0s 855us/step - loss: 0.2547 - val_loss: 1.6129\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.262 - 0s 798us/step - loss: 0.2591 - val_loss: 2.3368\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.261 - 0s 817us/step - loss: 0.2364 - val_loss: 1.8026\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.202 - 0s 826us/step - loss: 0.2250 - val_loss: 2.2329\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.179 - 0s 864us/step - loss: 0.2240 - val_loss: 1.9284\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 18\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.19 - ETA: 0s - loss: 0.7003 - 1s 8ms/step - loss: 0.6609 - val_loss: 0.3908\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.506 - 0s 826us/step - loss: 0.5187 - val_loss: 0.7735\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.188 - ETA: 0s - loss: 0.548 - 0s 836us/step - loss: 0.4714 - val_loss: 0.6535\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.793 - ETA: 0s - loss: 0.429 - 0s 941us/step - loss: 0.4434 - val_loss: 0.7458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.463 - 0s 789us/step - loss: 0.4183 - val_loss: 0.7545\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.339 - 0s 788us/step - loss: 0.3882 - val_loss: 0.6722\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.957 - ETA: 0s - loss: 0.288 - 0s 836us/step - loss: 0.3693 - val_loss: 0.8389\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.365 - 0s 817us/step - loss: 0.3519 - val_loss: 0.8851\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - ETA: 0s - loss: 0.301 - 0s 798us/step - loss: 0.3247 - val_loss: 1.0127\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.288 - 0s 826us/step - loss: 0.3178 - val_loss: 1.0672\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.295 - 0s 789us/step - loss: 0.3143 - val_loss: 0.8950\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.268 - 0s 817us/step - loss: 0.2747 - val_loss: 0.8541\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.262 - 0s 769us/step - loss: 0.2871 - val_loss: 0.9843\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.178 - 0s 779us/step - loss: 0.2574 - val_loss: 1.1783\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.564 - ETA: 0s - loss: 0.258 - 0s 826us/step - loss: 0.2681 - val_loss: 1.1622\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.217 - 0s 788us/step - loss: 0.2288 - val_loss: 1.0885\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.224 - 0s 798us/step - loss: 0.2381 - val_loss: 1.0543\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.166 - 0s 893us/step - loss: 0.2283 - val_loss: 1.0148\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.174 - 0s 826us/step - loss: 0.2177 - val_loss: 1.1116\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.162 - 0s 807us/step - loss: 0.2274 - val_loss: 1.1002\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.204 - 0s 845us/step - loss: 0.2070 - val_loss: 1.0173\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.234 - 0s 855us/step - loss: 0.2232 - val_loss: 1.1408\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.173 - 0s 931us/step - loss: 0.2191 - val_loss: 1.0850\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.202 - 0s 864us/step - loss: 0.1789 - val_loss: 1.1188\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.157 - 0s 836us/step - loss: 0.1822 - val_loss: 1.1574\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.156 - 0s 779us/step - loss: 0.1786 - val_loss: 1.0580\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.166 - 0s 798us/step - loss: 0.1773 - val_loss: 1.2371\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.132 - 0s 807us/step - loss: 0.1634 - val_loss: 1.1299\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.204 - 0s 1ms/step - loss: 0.1868 - val_loss: 1.1880\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.119 - 0s 836us/step - loss: 0.1566 - val_loss: 1.0605\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.169 - 0s 874us/step - loss: 0.1560 - val_loss: 1.2386\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 19\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.47 - ETA: 0s - loss: 0.6753 - 1s 8ms/step - loss: 0.6385 - val_loss: 0.6869\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.511 - 0s 817us/step - loss: 0.4558 - val_loss: 1.0653\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.509 - ETA: 0s - loss: 0.496 - 0s 826us/step - loss: 0.4273 - val_loss: 1.3153\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.348 - 0s 855us/step - loss: 0.3585 - val_loss: 1.1761\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.417 - ETA: 0s - loss: 0.329 - 0s 855us/step - loss: 0.3336 - val_loss: 1.3793\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.265 - 0s 845us/step - loss: 0.3068 - val_loss: 1.3239\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.315 - 0s 893us/step - loss: 0.2996 - val_loss: 1.6352\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.313 - 0s 940us/step - loss: 0.2876 - val_loss: 1.6705\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.269 - 0s 883us/step - loss: 0.2685 - val_loss: 1.4695\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.473 - ETA: 0s - loss: 0.180 - 0s 959us/step - loss: 0.2609 - val_loss: 1.5988\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.266 - 0s 845us/step - loss: 0.2536 - val_loss: 1.9413\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.269 - 0s 845us/step - loss: 0.2439 - val_loss: 2.0385\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.285 - 0s 874us/step - loss: 0.2420 - val_loss: 1.6211\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.247 - 0s 846us/step - loss: 0.2352 - val_loss: 1.6556\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.238 - 0s 836us/step - loss: 0.2206 - val_loss: 1.8472\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.221 - 0s 855us/step - loss: 0.2206 - val_loss: 1.3602\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.241 - 0s 874us/step - loss: 0.2311 - val_loss: 1.9292\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.214 - 0s 874us/step - loss: 0.2107 - val_loss: 1.3702\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.220 - 0s 845us/step - loss: 0.2074 - val_loss: 2.0297\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.200 - 0s 893us/step - loss: 0.1961 - val_loss: 1.6964\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.232 - 0s 893us/step - loss: 0.2171 - val_loss: 1.6672\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.176 - 0s 902us/step - loss: 0.1913 - val_loss: 1.6358\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.214 - 0s 817us/step - loss: 0.1930 - val_loss: 1.6110\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.193 - 0s 817us/step - loss: 0.2036 - val_loss: 1.8509\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.532 - ETA: 0s - loss: 0.169 - 0s 798us/step - loss: 0.1822 - val_loss: 1.6258\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.209 - 0s 798us/step - loss: 0.1909 - val_loss: 1.6710\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.186 - 0s 779us/step - loss: 0.1837 - val_loss: 1.5890\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.174 - 0s 798us/step - loss: 0.1618 - val_loss: 1.7281\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.189 - 0s 807us/step - loss: 0.1608 - val_loss: 1.8175\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.190 - 0s 845us/step - loss: 0.1679 - val_loss: 1.8361\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.674 - ETA: 0s - loss: 0.190 - 0s 883us/step - loss: 0.1706 - val_loss: 1.2450\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 20\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 15s - loss: 0.07 - ETA: 0s - loss: 0.5792 - 1s 8ms/step - loss: 0.7114 - val_loss: 0.3104\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.462 - ETA: 0s - loss: 0.573 - 0s 779us/step - loss: 0.5618 - val_loss: 0.4723\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.149 - ETA: 0s - loss: 0.551 - 0s 827us/step - loss: 0.5127 - val_loss: 0.3964\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.516 - 0s 798us/step - loss: 0.4606 - val_loss: 0.3349\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - ETA: 0s - loss: 0.442 - 0s 836us/step - loss: 0.4350 - val_loss: 0.3676\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.435 - 0s 817us/step - loss: 0.4162 - val_loss: 0.3181\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.423 - 0s 798us/step - loss: 0.3865 - val_loss: 0.3783\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.442 - 0s 798us/step - loss: 0.3867 - val_loss: 0.4033\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.595 - ETA: 0s - loss: 0.370 - 0s 798us/step - loss: 0.3625 - val_loss: 0.4287\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.674 - ETA: 0s - loss: 0.351 - 0s 893us/step - loss: 0.3571 - val_loss: 0.4361\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.387 - 0s 807us/step - loss: 0.3501 - val_loss: 0.4817\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.285 - 0s 864us/step - loss: 0.3194 - val_loss: 0.5037\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.387 - 0s 836us/step - loss: 0.3559 - val_loss: 0.5603\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.371 - 0s 864us/step - loss: 0.3115 - val_loss: 0.6245\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.303 - 0s 902us/step - loss: 0.2972 - val_loss: 0.6222\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.278 - 0s 817us/step - loss: 0.2840 - val_loss: 0.6938\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.263 - 0s 874us/step - loss: 0.2831 - val_loss: 0.7004\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.253 - 0s 855us/step - loss: 0.2643 - val_loss: 0.7053\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.253 - 0s 883us/step - loss: 0.2692 - val_loss: 0.7113\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.285 - 0s 798us/step - loss: 0.2682 - val_loss: 0.7374\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.270 - 0s 845us/step - loss: 0.2505 - val_loss: 0.7958\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.251 - 0s 921us/step - loss: 0.2364 - val_loss: 0.8560\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.527 - ETA: 0s - loss: 0.236 - 0s 807us/step - loss: 0.2375 - val_loss: 0.8795\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.541 - ETA: 0s - loss: 0.253 - 0s 884us/step - loss: 0.2402 - val_loss: 0.9016\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.221 - 0s 864us/step - loss: 0.2155 - val_loss: 0.9061\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.236 - 0s 836us/step - loss: 0.2228 - val_loss: 0.9271\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.176 - 0s 940us/step - loss: 0.2084 - val_loss: 0.9599\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.205 - 0s 836us/step - loss: 0.2108 - val_loss: 1.0111\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.208 - 0s 836us/step - loss: 0.2000 - val_loss: 1.0820\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.162 - 0s 884us/step - loss: 0.1906 - val_loss: 1.0978\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.186 - 0s 902us/step - loss: 0.2013 - val_loss: 1.1299\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 21\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.03 - ETA: 0s - loss: 0.8499 - 1s 8ms/step - loss: 0.7760 - val_loss: 0.5761\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.553 - 0s 826us/step - loss: 0.6117 - val_loss: 0.9874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.718 - ETA: 0s - loss: 0.495 - 0s 826us/step - loss: 0.5791 - val_loss: 0.8703\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.514 - 0s 912us/step - loss: 0.5546 - val_loss: 0.8319\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.530 - 0s 826us/step - loss: 0.5452 - val_loss: 1.0326\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.602 - ETA: 0s - loss: 0.534 - 0s 836us/step - loss: 0.5297 - val_loss: 0.9288\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.483 - 0s 855us/step - loss: 0.5140 - val_loss: 1.1104\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.976 - ETA: 0s - loss: 0.501 - 0s 855us/step - loss: 0.4872 - val_loss: 1.0820\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.518 - 0s 798us/step - loss: 0.4872 - val_loss: 1.0520\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.479 - ETA: 0s - loss: 0.377 - 0s 855us/step - loss: 0.4725 - val_loss: 0.9309\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.446 - 0s 874us/step - loss: 0.4520 - val_loss: 1.1104\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.467 - ETA: 0s - loss: 0.445 - 0s 845us/step - loss: 0.4338 - val_loss: 1.0041\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.375 - ETA: 0s - loss: 0.443 - 0s 902us/step - loss: 0.4202 - val_loss: 1.0407\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.421 - 0s 864us/step - loss: 0.4204 - val_loss: 1.0576\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.526 - ETA: 0s - loss: 0.441 - 0s 846us/step - loss: 0.4176 - val_loss: 0.9891\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.365 - ETA: 0s - loss: 0.298 - 0s 874us/step - loss: 0.3948 - val_loss: 1.1101\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.431 - 0s 874us/step - loss: 0.3798 - val_loss: 1.2121\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.306 - 0s 893us/step - loss: 0.3903 - val_loss: 1.0925\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.626 - ETA: 0s - loss: 0.330 - 0s 864us/step - loss: 0.3903 - val_loss: 0.9037\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.748 - ETA: 0s - loss: 0.378 - 0s 845us/step - loss: 0.3566 - val_loss: 1.1283\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.347 - ETA: 0s - loss: 0.400 - 0s 855us/step - loss: 0.3554 - val_loss: 1.1740\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.357 - 0s 921us/step - loss: 0.3492 - val_loss: 1.0246\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.592 - ETA: 0s - loss: 0.300 - 0s 874us/step - loss: 0.3359 - val_loss: 1.1877\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.407 - ETA: 0s - loss: 0.349 - 0s 921us/step - loss: 0.3219 - val_loss: 1.1348\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.294 - 0s 931us/step - loss: 0.3125 - val_loss: 1.1336\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.328 - 0s 864us/step - loss: 0.2963 - val_loss: 1.0618\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.240 - 0s 902us/step - loss: 0.2864 - val_loss: 1.1770\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.346 - 0s 874us/step - loss: 0.3056 - val_loss: 1.2506\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.497 - ETA: 0s - loss: 0.334 - 0s 893us/step - loss: 0.3086 - val_loss: 1.1494\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.340 - 0s 884us/step - loss: 0.2999 - val_loss: 0.9937\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.356 - ETA: 0s - loss: 0.244 - 0s 893us/step - loss: 0.2752 - val_loss: 1.2772\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 22\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.25 - ETA: 0s - loss: 0.5183 - 1s 8ms/step - loss: 0.7277 - val_loss: 0.4256\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.446 - 0s 855us/step - loss: 0.6161 - val_loss: 0.5815\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.787 - 0s 864us/step - loss: 0.6346 - val_loss: 0.6142\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.627 - 0s 855us/step - loss: 0.5668 - val_loss: 0.5585\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.643 - 0s 978us/step - loss: 0.5678 - val_loss: 0.7667\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.631 - 0s 865us/step - loss: 0.5287 - val_loss: 0.7896\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.494 - 0s 798us/step - loss: 0.4957 - val_loss: 0.8506\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.496 - 0s 921us/step - loss: 0.4805 - val_loss: 0.6859\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.419 - ETA: 0s - loss: 0.614 - 0s 874us/step - loss: 0.5101 - val_loss: 0.8561\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.475 - 0s 912us/step - loss: 0.4545 - val_loss: 0.7917\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.324 - 0s 864us/step - loss: 0.4294 - val_loss: 1.1268\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.496 - 0s 940us/step - loss: 0.4200 - val_loss: 0.9291\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.293 - 0s 855us/step - loss: 0.3957 - val_loss: 1.0744\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.384 - 0s 874us/step - loss: 0.3856 - val_loss: 1.1234\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.166 - 0s 893us/step - loss: 0.3686 - val_loss: 0.8388\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.241 - 0s 912us/step - loss: 0.3352 - val_loss: 1.5212\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.300 - 0s 864us/step - loss: 0.2704 - val_loss: 1.1960\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.404 - 0s 978us/step - loss: 0.3111 - val_loss: 1.4770\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.347 - 0s 836us/step - loss: 0.3155 - val_loss: 1.3469\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.374 - 0s 883us/step - loss: 0.3274 - val_loss: 1.3660\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.265 - 0s 855us/step - loss: 0.2724 - val_loss: 1.7404\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.317 - 0s 1ms/step - loss: 0.2523 - val_loss: 1.5643\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.210 - 0s 855us/step - loss: 0.2512 - val_loss: 1.5725\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.197 - 0s 836us/step - loss: 0.2222 - val_loss: 2.0566\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.250 - 0s 836us/step - loss: 0.2498 - val_loss: 1.6269\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.289 - 0s 779us/step - loss: 0.2438 - val_loss: 1.9476\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.186 - 0s 836us/step - loss: 0.2247 - val_loss: 1.7527\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.214 - 0s 817us/step - loss: 0.2039 - val_loss: 1.8451\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.230 - 0s 807us/step - loss: 0.1973 - val_loss: 1.8783\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.192 - 0s 789us/step - loss: 0.1827 - val_loss: 1.7935\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.178 - 0s 817us/step - loss: 0.1886 - val_loss: 1.8532\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 23\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 2.13 - ETA: 0s - loss: 0.9101 - 1s 8ms/step - loss: 0.7932 - val_loss: 0.4930\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.490 - 0s 789us/step - loss: 0.5023 - val_loss: 0.8444\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.377 - 0s 769us/step - loss: 0.4232 - val_loss: 0.9455\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.340 - 0s 807us/step - loss: 0.3600 - val_loss: 1.0266\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.484 - ETA: 0s - loss: 0.333 - 0s 826us/step - loss: 0.3365 - val_loss: 1.1103\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.316 - 0s 874us/step - loss: 0.2937 - val_loss: 0.9035\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.271 - 0s 1ms/step - loss: 0.2619 - val_loss: 1.0316\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.246 - 0s 817us/step - loss: 0.2503 - val_loss: 0.8815\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.245 - 0s 826us/step - loss: 0.2447 - val_loss: 0.7977\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.250 - 0s 817us/step - loss: 0.2185 - val_loss: 1.3344\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.591 - ETA: 0s - loss: 0.172 - 0s 779us/step - loss: 0.2014 - val_loss: 1.0602\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.196 - 0s 855us/step - loss: 0.2028 - val_loss: 1.1931\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.174 - 0s 836us/step - loss: 0.1796 - val_loss: 1.1839\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.204 - 0s 874us/step - loss: 0.1873 - val_loss: 1.1016\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.119 - 0s 874us/step - loss: 0.1562 - val_loss: 1.1988\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.104 - 0s 883us/step - loss: 0.1542 - val_loss: 1.3719\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.155 - 0s 978us/step - loss: 0.1530 - val_loss: 1.1868\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.134 - 0s 950us/step - loss: 0.1424 - val_loss: 1.2426\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.133 - 0s 883us/step - loss: 0.1246 - val_loss: 1.5323\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.105 - 0s 902us/step - loss: 0.1155 - val_loss: 0.9574\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.121 - 0s 836us/step - loss: 0.1258 - val_loss: 1.6236\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.110 - 0s 950us/step - loss: 0.1110 - val_loss: 1.3835\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.130 - 0s 893us/step - loss: 0.1146 - val_loss: 1.4671\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.076 - 0s 912us/step - loss: 0.1050 - val_loss: 1.4559\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.105 - 0s 893us/step - loss: 0.0905 - val_loss: 1.3899\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.159 - 0s 959us/step - loss: 0.1212 - val_loss: 1.6609\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.119 - 0s 893us/step - loss: 0.1157 - val_loss: 1.5788\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.099 - 0s 903us/step - loss: 0.0968 - val_loss: 1.3071\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.095 - 0s 912us/step - loss: 0.0931 - val_loss: 1.7045\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.091 - 0s 903us/step - loss: 0.0873 - val_loss: 1.3714\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.076 - 0s 874us/step - loss: 0.0821 - val_loss: 1.5495\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 24\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.74 - ETA: 0s - loss: 0.8185 - 1s 8ms/step - loss: 0.7535 - val_loss: 0.5943\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.469 - 0s 779us/step - loss: 0.5468 - val_loss: 0.7339\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.533 - 0s 769us/step - loss: 0.4865 - val_loss: 0.8105\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.393 - 0s 760us/step - loss: 0.4587 - val_loss: 0.7446\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.339 - 0s 855us/step - loss: 0.4162 - val_loss: 0.8554\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.497 - ETA: 0s - loss: 0.370 - 0s 883us/step - loss: 0.3446 - val_loss: 0.8987\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.351 - 0s 893us/step - loss: 0.3641 - val_loss: 0.7038\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.281 - 0s 940us/step - loss: 0.3228 - val_loss: 0.8217\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.295 - 0s 874us/step - loss: 0.2872 - val_loss: 0.6510\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.306 - 0s 845us/step - loss: 0.2519 - val_loss: 0.8877\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.516 - ETA: 0s - loss: 0.240 - 0s 807us/step - loss: 0.2505 - val_loss: 0.7034\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.389 - ETA: 0s - loss: 0.243 - 0s 817us/step - loss: 0.2363 - val_loss: 0.6643\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.200 - 0s 827us/step - loss: 0.2210 - val_loss: 0.9834\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.221 - 0s 779us/step - loss: 0.2108 - val_loss: 0.9779\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.195 - 0s 845us/step - loss: 0.1830 - val_loss: 1.0131\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.185 - 0s 874us/step - loss: 0.1834 - val_loss: 1.1880\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.174 - 0s 864us/step - loss: 0.1746 - val_loss: 1.0438\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.193 - 0s 912us/step - loss: 0.1724 - val_loss: 1.1641\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.149 - 0s 845us/step - loss: 0.1471 - val_loss: 1.1318\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.131 - 0s 836us/step - loss: 0.1391 - val_loss: 1.1731\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.138 - 0s 845us/step - loss: 0.1309 - val_loss: 0.9715\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.124 - 0s 817us/step - loss: 0.1434 - val_loss: 1.1812\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.133 - 0s 750us/step - loss: 0.1404 - val_loss: 1.1044\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.101 - 0s 883us/step - loss: 0.1093 - val_loss: 1.2458\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.095 - 0s 978us/step - loss: 0.1123 - val_loss: 1.4173\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.129 - 0s 798us/step - loss: 0.1153 - val_loss: 1.0875\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.123 - 0s 1ms/step - loss: 0.1413 - val_loss: 0.9052\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.112 - 0s 997us/step - loss: 0.1151 - val_loss: 1.4209\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.112 - 0s 893us/step - loss: 0.1100 - val_loss: 1.2702\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.103 - 0s 1ms/step - loss: 0.0989 - val_loss: 1.3224\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.088 - 0s 2ms/step - loss: 0.0946 - val_loss: 1.2905\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 25\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.10 - ETA: 0s - loss: 0.5561 - 1s 8ms/step - loss: 0.7113 - val_loss: 0.3885\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.456 - 0s 845us/step - loss: 0.5927 - val_loss: 0.5699\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.283 - 0s 912us/step - loss: 0.5015 - val_loss: 0.5748\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.349 - 0s 883us/step - loss: 0.4210 - val_loss: 0.6409\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.284 - 0s 902us/step - loss: 0.3857 - val_loss: 0.6678\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.131 - 0s 984us/step - loss: 0.3021 - val_loss: 0.8937\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.190 - 0s 995us/step - loss: 0.2924 - val_loss: 0.7113\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.278 - 0s 845us/step - loss: 0.2633 - val_loss: 0.6901\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.248 - 0s 899us/step - loss: 0.2398 - val_loss: 0.6517\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.262 - 0s 902us/step - loss: 0.2347 - val_loss: 0.8605\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.226 - 0s 934us/step - loss: 0.2016 - val_loss: 0.7319\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.150 - 0s 876us/step - loss: 0.1913 - val_loss: 0.7685\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.188 - 0s 916us/step - loss: 0.1920 - val_loss: 0.8525\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.129 - 0s 912us/step - loss: 0.1815 - val_loss: 0.7117\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.141 - 0s 940us/step - loss: 0.1635 - val_loss: 0.7428\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.153 - 0s 1ms/step - loss: 0.1526 - val_loss: 0.8358\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.164 - 0s 808us/step - loss: 0.1629 - val_loss: 0.7964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.173 - 0s 817us/step - loss: 0.1567 - val_loss: 0.7402\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.149 - 0s 760us/step - loss: 0.1431 - val_loss: 0.8356\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.146 - 0s 817us/step - loss: 0.1445 - val_loss: 0.7647\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.144 - 0s 807us/step - loss: 0.1336 - val_loss: 0.8042\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.148 - 0s 826us/step - loss: 0.1345 - val_loss: 0.7972\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.099 - 0s 826us/step - loss: 0.1326 - val_loss: 0.7848\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.144 - 0s 842us/step - loss: 0.1290 - val_loss: 0.7767\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.087 - 0s 817us/step - loss: 0.1155 - val_loss: 0.7332\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.130 - 0s 788us/step - loss: 0.1185 - val_loss: 0.8223\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.092 - 0s 844us/step - loss: 0.1284 - val_loss: 0.7173\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.111 - 0s 807us/step - loss: 0.1064 - val_loss: 0.7672\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.124 - 0s 855us/step - loss: 0.1071 - val_loss: 0.7020\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.140 - 0s 788us/step - loss: 0.1243 - val_loss: 0.9831\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.086 - 0s 845us/step - loss: 0.0999 - val_loss: 0.7313\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 26\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.81 - ETA: 0s - loss: 0.8074 - 1s 8ms/step - loss: 0.6931 - val_loss: 0.6622\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.449 - 0s 807us/step - loss: 0.4229 - val_loss: 1.3274\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.395 - 0s 845us/step - loss: 0.4126 - val_loss: 1.4175\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.765 - ETA: 0s - loss: 0.407 - 0s 836us/step - loss: 0.3646 - val_loss: 1.2511\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.363 - 0s 836us/step - loss: 0.3331 - val_loss: 1.2171\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.774 - ETA: 0s - loss: 0.415 - 0s 893us/step - loss: 0.3412 - val_loss: 1.4250\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.616 - ETA: 0s - loss: 0.357 - 0s 893us/step - loss: 0.3154 - val_loss: 1.6085\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.212 - 0s 864us/step - loss: 0.3040 - val_loss: 1.4717\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.281 - 0s 959us/step - loss: 0.2943 - val_loss: 1.4928\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.353 - ETA: 0s - loss: 0.256 - 0s 845us/step - loss: 0.2861 - val_loss: 1.5063\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.576 - ETA: 0s - loss: 0.328 - 0s 845us/step - loss: 0.2986 - val_loss: 1.6408\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.232 - 0s 874us/step - loss: 0.2561 - val_loss: 1.6097\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.221 - 0s 826us/step - loss: 0.2715 - val_loss: 1.8010\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.234 - 0s 866us/step - loss: 0.2641 - val_loss: 1.6182\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.204 - 0s 801us/step - loss: 0.2525 - val_loss: 1.6105\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.246 - 0s 836us/step - loss: 0.2399 - val_loss: 1.8634\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.223 - 0s 817us/step - loss: 0.2418 - val_loss: 1.7689\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.228 - 0s 883us/step - loss: 0.2250 - val_loss: 1.7822\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.177 - 0s 874us/step - loss: 0.2393 - val_loss: 1.8012\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.250 - 0s 969us/step - loss: 0.2335 - val_loss: 1.9341\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.226 - 0s 826us/step - loss: 0.2104 - val_loss: 1.8108\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.224 - 0s 874us/step - loss: 0.2052 - val_loss: 1.9017\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.221 - 0s 845us/step - loss: 0.2009 - val_loss: 1.9646\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.209 - 0s 798us/step - loss: 0.2061 - val_loss: 1.9430\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.170 - 0s 836us/step - loss: 0.1881 - val_loss: 1.7915\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.196 - 0s 860us/step - loss: 0.1933 - val_loss: 1.8429\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.210 - 0s 779us/step - loss: 0.1815 - val_loss: 2.0544\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.226 - 0s 864us/step - loss: 0.1904 - val_loss: 2.0542\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.160 - 0s 798us/step - loss: 0.1740 - val_loss: 2.1047\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.181 - 0s 927us/step - loss: 0.1793 - val_loss: 2.1280\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.156 - 0s 879us/step - loss: 0.1536 - val_loss: 2.1221\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 27\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.31 - ETA: 0s - loss: 0.8394 - 1s 8ms/step - loss: 0.8480 - val_loss: 0.1968\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.358 - ETA: 0s - loss: 0.670 - 0s 788us/step - loss: 0.5953 - val_loss: 0.5063\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.598 - 0s 779us/step - loss: 0.5647 - val_loss: 0.5918\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.437 - 0s 750us/step - loss: 0.5074 - val_loss: 0.4530\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.436 - ETA: 0s - loss: 0.467 - 0s 798us/step - loss: 0.4667 - val_loss: 0.5177\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.484 - 0s 864us/step - loss: 0.4334 - val_loss: 0.5602\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.342 - 0s 798us/step - loss: 0.4011 - val_loss: 0.6551\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.460 - ETA: 0s - loss: 0.410 - 0s 807us/step - loss: 0.3928 - val_loss: 0.5786\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.343 - 0s 864us/step - loss: 0.3640 - val_loss: 0.4656\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.374 - 0s 841us/step - loss: 0.3297 - val_loss: 0.6607\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.348 - 0s 921us/step - loss: 0.3298 - val_loss: 0.6474\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.593 - ETA: 0s - loss: 0.335 - 0s 836us/step - loss: 0.3019 - val_loss: 0.7498\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.243 - 0s 786us/step - loss: 0.2957 - val_loss: 0.7792\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.331 - 0s 855us/step - loss: 0.2936 - val_loss: 0.7230\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.260 - 0s 817us/step - loss: 0.2589 - val_loss: 0.7136\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.208 - 0s 836us/step - loss: 0.2593 - val_loss: 0.7259\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.251 - 0s 874us/step - loss: 0.2454 - val_loss: 0.7979\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.768 - ETA: 0s - loss: 0.289 - 0s 883us/step - loss: 0.2632 - val_loss: 0.7252\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.244 - 0s 950us/step - loss: 0.2451 - val_loss: 0.8767\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.240 - 0s 855us/step - loss: 0.2254 - val_loss: 0.7530\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.241 - 0s 931us/step - loss: 0.2139 - val_loss: 0.8880\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.195 - 0s 864us/step - loss: 0.2076 - val_loss: 0.8301\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.230 - 0s 893us/step - loss: 0.2149 - val_loss: 0.8905\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.211 - 0s 874us/step - loss: 0.2097 - val_loss: 0.8245\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.213 - 0s 893us/step - loss: 0.2084 - val_loss: 0.8205\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.220 - 0s 950us/step - loss: 0.1911 - val_loss: 0.8907\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.842 - ETA: 0s - loss: 0.188 - 0s 844us/step - loss: 0.1925 - val_loss: 0.8462\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.174 - 0s 902us/step - loss: 0.2005 - val_loss: 0.8620\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.239 - 0s 826us/step - loss: 0.2046 - val_loss: 0.9870\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.915 - ETA: 0s - loss: 0.230 - 0s 893us/step - loss: 0.1798 - val_loss: 0.7818\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.172 - 0s 959us/step - loss: 0.1888 - val_loss: 0.9617\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 28\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.44 - ETA: 6s - loss: 0.4042 - ETA: 2s - loss: 0.657 - ETA: 1s - loss: 0.661 - ETA: 1s - loss: 0.535 - ETA: 0s - loss: 0.501 - ETA: 0s - loss: 0.531 - ETA: 0s - loss: 0.611 - 1s 13ms/step - loss: 0.6062 - val_loss: 0.5716\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.954 - ETA: 0s - loss: 0.740 - ETA: 0s - loss: 0.464 - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.546 - ETA: 0s - loss: 0.509 - ETA: 0s - loss: 0.487 - ETA: 0s - loss: 0.461 - ETA: 0s - loss: 0.426 - 1s 7ms/step - loss: 0.4367 - val_loss: 1.3057\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 1s - loss: 1.060 - ETA: 0s - loss: 0.551 - ETA: 0s - loss: 0.429 - ETA: 0s - loss: 0.479 - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.406 - ETA: 0s - loss: 0.368 - ETA: 0s - loss: 0.357 - 1s 5ms/step - loss: 0.3546 - val_loss: 1.0249\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.529 - ETA: 0s - loss: 0.354 - 0s 769us/step - loss: 0.3319 - val_loss: 1.0183\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.303 - 0s 788us/step - loss: 0.3070 - val_loss: 1.2466\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.236 - 0s 978us/step - loss: 0.2639 - val_loss: 1.3201\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.434 - ETA: 0s - loss: 0.251 - 0s 836us/step - loss: 0.2452 - val_loss: 1.2766\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.181 - 0s 845us/step - loss: 0.2246 - val_loss: 1.2331\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.210 - 0s 826us/step - loss: 0.2080 - val_loss: 1.4187\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.208 - 0s 788us/step - loss: 0.2001 - val_loss: 1.6441\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.180 - 0s 864us/step - loss: 0.2058 - val_loss: 1.4728\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.118 - 0s 912us/step - loss: 0.1670 - val_loss: 1.5970\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.176 - 0s 836us/step - loss: 0.1746 - val_loss: 1.4733\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.184 - 0s 769us/step - loss: 0.1729 - val_loss: 1.6368\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.208 - 0s 798us/step - loss: 0.1709 - val_loss: 1.7321\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.139 - 0s 874us/step - loss: 0.1564 - val_loss: 1.5992\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.149 - 0s 864us/step - loss: 0.1609 - val_loss: 1.8898\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.162 - 0s 921us/step - loss: 0.1621 - val_loss: 2.0173\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.152 - 0s 779us/step - loss: 0.1565 - val_loss: 1.4586\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.118 - 0s 779us/step - loss: 0.1391 - val_loss: 1.5479\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.146 - 0s 798us/step - loss: 0.1459 - val_loss: 1.8907\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.141 - 0s 817us/step - loss: 0.1513 - val_loss: 1.6783\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.148 - 0s 807us/step - loss: 0.1379 - val_loss: 1.6743\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.135 - 0s 807us/step - loss: 0.1392 - val_loss: 1.9987\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.130 - 0s 836us/step - loss: 0.1285 - val_loss: 1.6928\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.115 - 0s 817us/step - loss: 0.1233 - val_loss: 1.6165\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.121 - 0s 855us/step - loss: 0.1247 - val_loss: 1.9502\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.125 - 0s 940us/step - loss: 0.1196 - val_loss: 1.8365\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.116 - 0s 836us/step - loss: 0.1123 - val_loss: 1.3954\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.113 - 0s 836us/step - loss: 0.1207 - val_loss: 1.9482\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.118 - 0s 902us/step - loss: 0.1255 - val_loss: 1.7285\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 29\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.42 - ETA: 0s - loss: 0.6501 - 1s 8ms/step - loss: 0.6713 - val_loss: 0.5903\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.452 - 0s 855us/step - loss: 0.4010 - val_loss: 1.2381\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.377 - 0s 855us/step - loss: 0.3744 - val_loss: 1.3705\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.330 - 0s 807us/step - loss: 0.3590 - val_loss: 1.3800\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.628 - ETA: 0s - loss: 0.276 - 0s 1ms/step - loss: 0.3146 - val_loss: 1.1792\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.296 - 0s 817us/step - loss: 0.3009 - val_loss: 1.2920\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.245 - 0s 817us/step - loss: 0.2860 - val_loss: 1.1966\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.240 - 0s 893us/step - loss: 0.2596 - val_loss: 1.4095\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.270 - 0s 874us/step - loss: 0.2431 - val_loss: 1.4162\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.499 - ETA: 0s - loss: 0.229 - 0s 855us/step - loss: 0.2241 - val_loss: 1.4424\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.224 - 0s 855us/step - loss: 0.2208 - val_loss: 1.4395\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.227 - 0s 893us/step - loss: 0.2205 - val_loss: 1.5382\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.180 - 0s 912us/step - loss: 0.1929 - val_loss: 1.7106\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.186 - 0s 874us/step - loss: 0.1992 - val_loss: 1.6874\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.219 - 0s 893us/step - loss: 0.1970 - val_loss: 1.6952\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.180 - 0s 845us/step - loss: 0.1924 - val_loss: 1.8043\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.349 - ETA: 0s - loss: 0.149 - 0s 855us/step - loss: 0.1697 - val_loss: 1.8850\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.191 - 0s 826us/step - loss: 0.1692 - val_loss: 1.6543\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.186 - 0s 978us/step - loss: 0.1666 - val_loss: 1.8746\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.159 - 0s 902us/step - loss: 0.1447 - val_loss: 2.0397\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.173 - 0s 826us/step - loss: 0.1645 - val_loss: 1.6597\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.160 - 0s 893us/step - loss: 0.1424 - val_loss: 2.0073\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.121 - 0s 817us/step - loss: 0.1418 - val_loss: 1.9689\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.138 - 0s 931us/step - loss: 0.1365 - val_loss: 2.2645\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.143 - 0s 874us/step - loss: 0.1520 - val_loss: 1.6607\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.179 - 0s 940us/step - loss: 0.1448 - val_loss: 2.1015\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.133 - 0s 845us/step - loss: 0.1326 - val_loss: 2.1901\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.133 - 0s 903us/step - loss: 0.1230 - val_loss: 1.9257\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.129 - 0s 1ms/step - loss: 0.1308 - val_loss: 2.2529\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.111 - 0s 817us/step - loss: 0.1156 - val_loss: 2.1412\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.116 - 0s 788us/step - loss: 0.1145 - val_loss: 1.8817\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 30\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.11 - ETA: 0s - loss: 0.8101 - 1s 8ms/step - loss: 0.6649 - val_loss: 0.6235\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.774 - ETA: 0s - loss: 0.477 - 0s 826us/step - loss: 0.4612 - val_loss: 0.8662\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.626 - ETA: 0s - loss: 0.399 - 0s 769us/step - loss: 0.4147 - val_loss: 0.7830\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.104 - ETA: 0s - loss: 0.374 - 0s 798us/step - loss: 0.3644 - val_loss: 0.8246\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.329 - 0s 788us/step - loss: 0.3296 - val_loss: 0.8238\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.407 - ETA: 0s - loss: 0.229 - 0s 788us/step - loss: 0.2961 - val_loss: 0.8314\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.094 - ETA: 0s - loss: 0.286 - 0s 779us/step - loss: 0.2789 - val_loss: 0.9992\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.254 - 0s 750us/step - loss: 0.2545 - val_loss: 0.8485\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.259 - 0s 760us/step - loss: 0.2377 - val_loss: 0.6804\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.216 - 0s 826us/step - loss: 0.2330 - val_loss: 0.8470\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.142 - 0s 855us/step - loss: 0.2156 - val_loss: 0.8922\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.227 - 0s 902us/step - loss: 0.2046 - val_loss: 0.8787\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.183 - 0s 807us/step - loss: 0.2115 - val_loss: 0.9337\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.189 - 0s 807us/step - loss: 0.1914 - val_loss: 0.9068\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.209 - 0s 807us/step - loss: 0.1834 - val_loss: 0.9435\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.213 - 0s 826us/step - loss: 0.1894 - val_loss: 0.9631\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.196 - 0s 827us/step - loss: 0.1798 - val_loss: 1.1459\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.150 - 0s 836us/step - loss: 0.1715 - val_loss: 1.0407\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.161 - 0s 798us/step - loss: 0.1766 - val_loss: 1.0979\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.177 - 0s 808us/step - loss: 0.1547 - val_loss: 1.0911\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.179 - 0s 788us/step - loss: 0.1605 - val_loss: 1.1373\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.080 - 0s 883us/step - loss: 0.1629 - val_loss: 1.2042\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.134 - 0s 846us/step - loss: 0.1498 - val_loss: 1.2074\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.092 - 0s 884us/step - loss: 0.1576 - val_loss: 1.1557\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.134 - 0s 883us/step - loss: 0.1515 - val_loss: 1.3305\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.527 - ETA: 0s - loss: 0.120 - 0s 864us/step - loss: 0.1455 - val_loss: 1.2081\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.108 - 0s 883us/step - loss: 0.1398 - val_loss: 1.3749\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.121 - 0s 902us/step - loss: 0.1392 - val_loss: 1.3665\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.139 - 0s 893us/step - loss: 0.1437 - val_loss: 1.3572\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.119 - 0s 846us/step - loss: 0.1301 - val_loss: 1.3899\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.122 - 0s 826us/step - loss: 0.1421 - val_loss: 1.3866\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 31\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.40 - ETA: 0s - loss: 0.5467 - 1s 8ms/step - loss: 0.6549 - val_loss: 0.5172\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.480 - 0s 950us/step - loss: 0.4523 - val_loss: 0.9028\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.330 - 0s 883us/step - loss: 0.4257 - val_loss: 0.7178\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.318 - 0s 874us/step - loss: 0.3794 - val_loss: 0.8656\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.639 - ETA: 0s - loss: 0.370 - 0s 836us/step - loss: 0.3385 - val_loss: 1.0229\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.326 - 0s 941us/step - loss: 0.3173 - val_loss: 0.7517\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.357 - 0s 865us/step - loss: 0.3064 - val_loss: 0.8294\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.231 - 0s 921us/step - loss: 0.2767 - val_loss: 1.0069\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.286 - 0s 846us/step - loss: 0.2629 - val_loss: 0.8121\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.231 - 0s 931us/step - loss: 0.2388 - val_loss: 0.9194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.243 - 0s 845us/step - loss: 0.2349 - val_loss: 0.8607\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.290 - ETA: 0s - loss: 0.182 - 0s 836us/step - loss: 0.2452 - val_loss: 0.8171\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.230 - 0s 902us/step - loss: 0.2472 - val_loss: 0.8226\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.191 - 0s 807us/step - loss: 0.2097 - val_loss: 0.9075\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.229 - 0s 836us/step - loss: 0.2053 - val_loss: 0.9603\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.165 - 0s 788us/step - loss: 0.1903 - val_loss: 1.0790\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.162 - 0s 779us/step - loss: 0.1691 - val_loss: 1.0056\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.204 - 0s 788us/step - loss: 0.1813 - val_loss: 1.0075\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.197 - 0s 826us/step - loss: 0.1885 - val_loss: 0.8198\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.158 - 0s 826us/step - loss: 0.1624 - val_loss: 1.0999\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.157 - 0s 817us/step - loss: 0.1490 - val_loss: 0.9318\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.182 - 0s 789us/step - loss: 0.1647 - val_loss: 0.9449\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.127 - 0s 855us/step - loss: 0.1532 - val_loss: 1.0660\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.170 - 0s 921us/step - loss: 0.1499 - val_loss: 1.0629\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.171 - 0s 836us/step - loss: 0.1514 - val_loss: 1.0876\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.131 - 0s 855us/step - loss: 0.1226 - val_loss: 1.1479\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.133 - 0s 826us/step - loss: 0.1419 - val_loss: 1.0095\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.135 - 0s 817us/step - loss: 0.1432 - val_loss: 1.1013\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.127 - 0s 1ms/step - loss: 0.1201 - val_loss: 0.9938\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.120 - 0s 769us/step - loss: 0.1116 - val_loss: 1.1240\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.118 - 0s 845us/step - loss: 0.1172 - val_loss: 1.0617\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 32\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.65 - ETA: 0s - loss: 0.8049 - 1s 8ms/step - loss: 0.6762 - val_loss: 0.7380\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.099 - ETA: 0s - loss: 0.524 - 0s 807us/step - loss: 0.4984 - val_loss: 0.7792\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.487 - 0s 826us/step - loss: 0.4260 - val_loss: 0.7070\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.293 - 0s 874us/step - loss: 0.3694 - val_loss: 0.7685\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.549 - ETA: 0s - loss: 0.403 - 0s 912us/step - loss: 0.3587 - val_loss: 0.5839\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.352 - 0s 845us/step - loss: 0.3313 - val_loss: 0.8691\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.227 - 0s 864us/step - loss: 0.2933 - val_loss: 0.5446\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.268 - 0s 807us/step - loss: 0.2682 - val_loss: 0.7586\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.223 - 0s 846us/step - loss: 0.2722 - val_loss: 0.6969\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.263 - 0s 874us/step - loss: 0.2608 - val_loss: 0.7268\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.265 - 0s 836us/step - loss: 0.2312 - val_loss: 0.6518\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.214 - 0s 817us/step - loss: 0.2331 - val_loss: 0.5983\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.226 - 0s 922us/step - loss: 0.2434 - val_loss: 0.6537\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.615 - ETA: 0s - loss: 0.188 - 0s 893us/step - loss: 0.1972 - val_loss: 0.6676\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.238 - 0s 893us/step - loss: 0.2142 - val_loss: 0.7148\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.176 - 0s 874us/step - loss: 0.2011 - val_loss: 0.7021\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.155 - 0s 941us/step - loss: 0.1735 - val_loss: 0.6473\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.147 - 0s 845us/step - loss: 0.1802 - val_loss: 0.7502\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.124 - 0s 931us/step - loss: 0.1707 - val_loss: 0.8011\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.137 - 0s 902us/step - loss: 0.1538 - val_loss: 0.8118\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.176 - 0s 912us/step - loss: 0.1582 - val_loss: 0.6798\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.145 - 0s 874us/step - loss: 0.1576 - val_loss: 0.6193\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.146 - 0s 997us/step - loss: 0.1400 - val_loss: 0.6410\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.132 - 0s 893us/step - loss: 0.1290 - val_loss: 0.8693\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.107 - 0s 817us/step - loss: 0.1357 - val_loss: 0.7494\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.126 - 0s 865us/step - loss: 0.1239 - val_loss: 0.7256\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.118 - 0s 931us/step - loss: 0.1235 - val_loss: 0.9273\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.100 - 0s 902us/step - loss: 0.1079 - val_loss: 0.8510\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.120 - 0s 769us/step - loss: 0.1205 - val_loss: 0.7637\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.115 - 0s 779us/step - loss: 0.1114 - val_loss: 0.8864\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.096 - 0s 769us/step - loss: 0.0969 - val_loss: 0.7890\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.097 - 0s 770us/step - loss: 0.1021 - val_loss: 0.8765\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.095 - 0s 750us/step - loss: 0.0903 - val_loss: 0.8298\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.086 - 0s 741us/step - loss: 0.1064 - val_loss: 1.0250\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.086 - 0s 750us/step - loss: 0.0885 - val_loss: 0.8837\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.082 - 0s 788us/step - loss: 0.0810 - val_loss: 0.9586\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.091 - 0s 817us/step - loss: 0.0889 - val_loss: 0.9322\n",
      "Epoch 00037: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 33\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.22 - ETA: 0s - loss: 1.2594 - 1s 8ms/step - loss: 0.9029 - val_loss: 0.2529\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.821 - 0s 855us/step - loss: 0.8273 - val_loss: 0.1561\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.718 - 0s 836us/step - loss: 0.7566 - val_loss: 0.3564\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.770 - 0s 855us/step - loss: 0.7339 - val_loss: 0.4026\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.393 - ETA: 0s - loss: 0.941 - 0s 845us/step - loss: 0.6974 - val_loss: 0.4868\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.792 - 0s 846us/step - loss: 0.6226 - val_loss: 0.5755\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.757 - 0s 836us/step - loss: 0.5854 - val_loss: 0.8408\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.532 - 0s 827us/step - loss: 0.5438 - val_loss: 0.9140\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.233 - 0s 864us/step - loss: 0.5140 - val_loss: 1.1057\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.640 - 0s 883us/step - loss: 0.4683 - val_loss: 1.3923\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.601 - 0s 940us/step - loss: 0.4422 - val_loss: 1.7272\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.453 - ETA: 0s - loss: 0.468 - 0s 827us/step - loss: 0.4528 - val_loss: 1.8158\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.411 - 0s 902us/step - loss: 0.3676 - val_loss: 2.1450\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.412 - 0s 826us/step - loss: 0.3597 - val_loss: 2.3509\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.336 - 0s 827us/step - loss: 0.3031 - val_loss: 2.5734\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.369 - 0s 826us/step - loss: 0.3089 - val_loss: 2.7854\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.325 - 0s 827us/step - loss: 0.2876 - val_loss: 3.0036\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.304 - 0s 798us/step - loss: 0.2601 - val_loss: 3.2033\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.289 - 0s 931us/step - loss: 0.2516 - val_loss: 3.4073\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.258 - 0s 893us/step - loss: 0.2269 - val_loss: 3.7662\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.251 - 0s 912us/step - loss: 0.2185 - val_loss: 4.0270\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.269 - 0s 902us/step - loss: 0.2386 - val_loss: 4.0102\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.160 - ETA: 0s - loss: 0.256 - 0s 893us/step - loss: 0.2044 - val_loss: 4.2228\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.217 - 0s 807us/step - loss: 0.1884 - val_loss: 4.2588\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.157 - 0s 779us/step - loss: 0.1532 - val_loss: 4.6801\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.437 - ETA: 0s - loss: 0.158 - 0s 874us/step - loss: 0.1404 - val_loss: 4.7017\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.095 - 0s 903us/step - loss: 0.1339 - val_loss: 4.8624\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.198 - 0s 940us/step - loss: 0.1711 - val_loss: 5.1498\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.124 - 0s 874us/step - loss: 0.1231 - val_loss: 5.6160\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.161 - 0s 921us/step - loss: 0.1380 - val_loss: 5.3831\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.094 - 0s 921us/step - loss: 0.1243 - val_loss: 5.5501\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.161 - 0s 1ms/step - loss: 0.1244 - val_loss: 5.6795\n",
      "Epoch 00032: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 34\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.32 - ETA: 0s - loss: 0.5854 - 1s 8ms/step - loss: 0.7250 - val_loss: 0.6746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.464 - 0s 921us/step - loss: 0.5749 - val_loss: 0.9693\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 0.526 - 0s 836us/step - loss: 0.5328 - val_loss: 1.2639\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.584 - ETA: 0s - loss: 0.531 - 0s 845us/step - loss: 0.4913 - val_loss: 1.2546\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.553 - 0s 826us/step - loss: 0.4740 - val_loss: 1.3134\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.383 - ETA: 0s - loss: 0.470 - 0s 874us/step - loss: 0.4563 - val_loss: 1.4472\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.478 - 0s 836us/step - loss: 0.4226 - val_loss: 1.4317\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.351 - 0s 817us/step - loss: 0.4036 - val_loss: 1.5987\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.371 - 0s 855us/step - loss: 0.3942 - val_loss: 1.6332\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.405 - 0s 902us/step - loss: 0.3876 - val_loss: 1.7066\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.349 - 0s 864us/step - loss: 0.3575 - val_loss: 2.0473\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.332 - ETA: 0s - loss: 0.385 - 0s 864us/step - loss: 0.3426 - val_loss: 2.0011\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.386 - 0s 883us/step - loss: 0.3445 - val_loss: 2.1539\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.314 - 0s 912us/step - loss: 0.3390 - val_loss: 2.3671\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.311 - 0s 836us/step - loss: 0.3142 - val_loss: 2.0140\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.984 - ETA: 0s - loss: 0.305 - 0s 864us/step - loss: 0.3087 - val_loss: 2.2990\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.285 - 0s 798us/step - loss: 0.3116 - val_loss: 2.3823\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.090 - ETA: 0s - loss: 0.299 - 0s 1ms/step - loss: 0.3154 - val_loss: 2.3639\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.236 - 0s 817us/step - loss: 0.2992 - val_loss: 2.4225\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.359 - 0s 845us/step - loss: 0.3056 - val_loss: 2.4994\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.252 - 0s 826us/step - loss: 0.2997 - val_loss: 2.6186\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.296 - 0s 845us/step - loss: 0.2853 - val_loss: 2.3791\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.257 - 0s 931us/step - loss: 0.2736 - val_loss: 2.6950\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.247 - 0s 865us/step - loss: 0.2777 - val_loss: 2.6531\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.462 - ETA: 0s - loss: 0.260 - 0s 798us/step - loss: 0.2593 - val_loss: 2.4773\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.276 - 0s 817us/step - loss: 0.2659 - val_loss: 2.4606\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.237 - 0s 807us/step - loss: 0.2525 - val_loss: 2.5346\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.253 - 0s 883us/step - loss: 0.2536 - val_loss: 3.1197\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.261 - 0s 845us/step - loss: 0.2388 - val_loss: 2.5060\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.227 - 0s 940us/step - loss: 0.2341 - val_loss: 2.8194\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.248 - 0s 779us/step - loss: 0.2467 - val_loss: 2.5135\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 35\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.65 - ETA: 0s - loss: 0.7313 - 1s 8ms/step - loss: 1.0005 - val_loss: 0.1408\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.756 - ETA: 0s - loss: 1.112 - 0s 845us/step - loss: 0.8943 - val_loss: 0.1350\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.529 - 0s 922us/step - loss: 0.8269 - val_loss: 0.0835\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.659 - ETA: 0s - loss: 1.038 - 0s 826us/step - loss: 0.8013 - val_loss: 0.0646\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.942 - 0s 826us/step - loss: 0.7693 - val_loss: 0.1053\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.866 - 0s 826us/step - loss: 0.7199 - val_loss: 0.0808\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.658 - 0s 817us/step - loss: 0.6923 - val_loss: 0.0896\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.390 - 0s 798us/step - loss: 0.6487 - val_loss: 0.1242\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.736 - 0s 874us/step - loss: 0.6397 - val_loss: 0.1269\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.476 - 0s 779us/step - loss: 0.5858 - val_loss: 0.1011\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.717 - 0s 874us/step - loss: 0.5810 - val_loss: 0.1455\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.369 - 0s 807us/step - loss: 0.5554 - val_loss: 0.1392\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.483 - 0s 893us/step - loss: 0.5412 - val_loss: 0.1383\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.410 - 0s 1ms/step - loss: 0.5095 - val_loss: 0.1559\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.664 - 0s 950us/step - loss: 0.4801 - val_loss: 0.1746\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.581 - 0s 883us/step - loss: 0.4721 - val_loss: 0.1846\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.421 - 0s 931us/step - loss: 0.4583 - val_loss: 0.1936\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.418 - 0s 940us/step - loss: 0.4466 - val_loss: 0.1938\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.536 - 0s 922us/step - loss: 0.4155 - val_loss: 0.1994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.519 - 0s 855us/step - loss: 0.4127 - val_loss: 0.2267\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.465 - ETA: 0s - loss: 0.499 - 0s 912us/step - loss: 0.3928 - val_loss: 0.2580\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.257 - 0s 788us/step - loss: 0.3411 - val_loss: 0.2477\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.001 - ETA: 0s - loss: 0.455 - 0s 798us/step - loss: 0.3816 - val_loss: 0.2551\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.421 - 0s 817us/step - loss: 0.3477 - val_loss: 0.2252\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.138 - 0s 817us/step - loss: 0.3246 - val_loss: 0.2117\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.316 - 0s 779us/step - loss: 0.2911 - val_loss: 0.2851\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.383 - 0s 807us/step - loss: 0.3243 - val_loss: 0.2792\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.225 - 0s 827us/step - loss: 0.2798 - val_loss: 0.2718\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.339 - 0s 770us/step - loss: 0.2905 - val_loss: 0.2808\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.283 - 0s 855us/step - loss: 0.2604 - val_loss: 0.2668\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.613 - ETA: 0s - loss: 0.254 - 0s 779us/step - loss: 0.2660 - val_loss: 0.2557\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.258 - 0s 750us/step - loss: 0.2451 - val_loss: 0.3004\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.132 - 0s 864us/step - loss: 0.2438 - val_loss: 0.3008\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.274 - 0s 779us/step - loss: 0.2340 - val_loss: 0.2976\n",
      "Epoch 00034: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 36\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.54 - ETA: 0s - loss: 0.2464 - 1s 8ms/step - loss: 0.8151 - val_loss: 0.4088\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.180 - 0s 940us/step - loss: 0.7139 - val_loss: 0.3836\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.856 - 0s 912us/step - loss: 0.6368 - val_loss: 0.4713\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.922 - 0s 931us/step - loss: 0.6216 - val_loss: 0.5475\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.673 - 0s 874us/step - loss: 0.5385 - val_loss: 0.5193\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.157 - 0s 912us/step - loss: 0.4925 - val_loss: 0.4625\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.577 - 0s 836us/step - loss: 0.4782 - val_loss: 0.6658\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.145 - 0s 864us/step - loss: 0.4195 - val_loss: 0.5824\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.141 - 0s 864us/step - loss: 0.3106 - val_loss: 0.6468\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.373 - 0s 940us/step - loss: 0.3294 - val_loss: 0.7167\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.286 - 0s 807us/step - loss: 0.2723 - val_loss: 0.7535\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.538 - ETA: 0s - loss: 0.346 - 0s 807us/step - loss: 0.2869 - val_loss: 0.7378\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.273 - 0s 788us/step - loss: 0.2356 - val_loss: 0.6347\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.288 - 0s 940us/step - loss: 0.1992 - val_loss: 0.5639\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.229 - 0s 826us/step - loss: 0.2076 - val_loss: 0.8632\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.167 - 0s 818us/step - loss: 0.1694 - val_loss: 0.7168\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.186 - 0s 817us/step - loss: 0.1564 - val_loss: 0.7447\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.119 - 0s 817us/step - loss: 0.1782 - val_loss: 0.8017\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.164 - 0s 817us/step - loss: 0.1584 - val_loss: 0.7218\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.087 - 0s 931us/step - loss: 0.1205 - val_loss: 0.7249\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.142 - 0s 912us/step - loss: 0.1137 - val_loss: 0.6754\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.136 - 0s 903us/step - loss: 0.1191 - val_loss: 0.6123\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.119 - 0s 874us/step - loss: 0.1121 - val_loss: 0.6415\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.062 - 0s 988us/step - loss: 0.1090 - val_loss: 0.7265\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.077 - 0s 988us/step - loss: 0.0965 - val_loss: 0.7164\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.057 - 0s 893us/step - loss: 0.0788 - val_loss: 0.6510\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.072 - 0s 921us/step - loss: 0.0723 - val_loss: 0.7560\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.086 - 0s 874us/step - loss: 0.0853 - val_loss: 0.5860\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.082 - 0s 893us/step - loss: 0.0964 - val_loss: 0.6674\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.079 - 0s 931us/step - loss: 0.0736 - val_loss: 0.7072\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.052 - 0s 978us/step - loss: 0.0672 - val_loss: 0.8131\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.061 - 0s 883us/step - loss: 0.0618 - val_loss: 0.6052\n",
      "Epoch 00032: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 37\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 48,501\n",
      "Trainable params: 48,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.65 - ETA: 0s - loss: 0.7972 - 1s 8ms/step - loss: 0.7795 - val_loss: 0.4660\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.471 - 0s 855us/step - loss: 0.5532 - val_loss: 0.8694\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.516 - 0s 969us/step - loss: 0.5195 - val_loss: 0.9309\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.472 - 0s 817us/step - loss: 0.4464 - val_loss: 1.0403\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.487 - 0s 836us/step - loss: 0.4480 - val_loss: 1.2402\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.438 - 0s 855us/step - loss: 0.4105 - val_loss: 1.3359\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.378 - 0s 902us/step - loss: 0.3923 - val_loss: 1.5566\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.351 - 0s 845us/step - loss: 0.3583 - val_loss: 1.6998\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.354 - ETA: 0s - loss: 0.352 - 0s 817us/step - loss: 0.3446 - val_loss: 1.7936\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.337 - 0s 798us/step - loss: 0.3345 - val_loss: 1.5157\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.326 - 0s 874us/step - loss: 0.3220 - val_loss: 1.7773\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.294 - 0s 827us/step - loss: 0.3014 - val_loss: 1.8984\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.292 - 0s 1ms/step - loss: 0.2935 - val_loss: 1.8482\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.307 - 0s 722us/step - loss: 0.2814 - val_loss: 2.0176\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.284 - 0s 750us/step - loss: 0.2776 - val_loss: 2.0026\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.252 - 0s 731us/step - loss: 0.2485 - val_loss: 2.0074\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.271 - 0s 769us/step - loss: 0.2577 - val_loss: 1.8531\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.232 - 0s 826us/step - loss: 0.2358 - val_loss: 1.9611\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.221 - 0s 779us/step - loss: 0.2441 - val_loss: 1.9784\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.245 - 0s 855us/step - loss: 0.2461 - val_loss: 2.1967\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.224 - 0s 836us/step - loss: 0.2278 - val_loss: 2.1552\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.217 - 0s 826us/step - loss: 0.2114 - val_loss: 2.3407\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.222 - 0s 864us/step - loss: 0.2159 - val_loss: 2.1657\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.199 - 0s 921us/step - loss: 0.1949 - val_loss: 2.1628\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.439 - ETA: 0s - loss: 0.207 - 0s 845us/step - loss: 0.1945 - val_loss: 1.9907\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.140 - 0s 864us/step - loss: 0.1742 - val_loss: 2.1246\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.176 - 0s 760us/step - loss: 0.1858 - val_loss: 2.3135\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.182 - 0s 864us/step - loss: 0.1857 - val_loss: 2.2357\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.349 - ETA: 0s - loss: 0.172 - 0s 836us/step - loss: 0.1724 - val_loss: 2.4543\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.174 - 0s 807us/step - loss: 0.1668 - val_loss: 2.0712\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.172 - 0s 931us/step - loss: 0.1676 - val_loss: 2.2728\n",
      "Epoch 00031: early stopping\n",
      "MSE: 804.896970\n",
      "RMSE: 28.370706\n",
      "MAE: 21.897695\n",
      "MAPE: 25.940729\n",
      "\n",
      "Quantile 1, between 39.99999999999999 and 77.5\n",
      "MSE: 568.806573\n",
      "RMSE: 23.849666\n",
      "MAE: 20.456937\n",
      "MAPE: 34.047569\n",
      "\n",
      "Quantile 2, between 77.5 and 87.5\n",
      "MSE: 1084.511926\n",
      "RMSE: 32.931929\n",
      "MAE: 22.415277\n",
      "MAPE: 27.053479\n",
      "\n",
      "Quantile 3, between 87.5 and 98.00000000000001\n",
      "MSE: 399.208200\n",
      "RMSE: 19.980195\n",
      "MAE: 17.992259\n",
      "MAPE: 19.149773\n",
      "\n",
      "Quantile 4, between 98.00000000000001 and 130.00000000000003\n",
      "MSE: 1154.453799\n",
      "RMSE: 33.977254\n",
      "MAE: 26.387521\n",
      "MAPE: 22.944274\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_test = 15\n",
    "n_hexa = 20\n",
    "name_error = 'LSTM predictions - 15 Days - 20 hex - 4q'\n",
    "path_error = \"imgs/ips_predictions/prediction_lstm_15_days_20hex_4q.png\"\n",
    "name_map = '15 days - 20 hex - 4q'\n",
    "path_map = \"imgs/ips_predictions/maps_LSTM_prediction_15_total_data_20_hex_4q.png\"\n",
    "\n",
    "dict_pred,pred_lstm,pred_lstm_cases = lstm_hex_selected(data_days, dict_pred_sel_hexa,n_test,n_hexa,name_error,path_error,name_map,path_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,) (15,)\n",
      "model compiled 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.36 - ETA: 0s - loss: 0.7742 - 1s 8ms/step - loss: 0.7057 - val_loss: 0.5145\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.597 - 0s 864us/step - loss: 0.5715 - val_loss: 1.0652\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.523 - ETA: 0s - loss: 0.469 - 0s 979us/step - loss: 0.5470 - val_loss: 0.9749\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.499 - 0s 836us/step - loss: 0.5368 - val_loss: 0.8508\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.557 - 0s 874us/step - loss: 0.4994 - val_loss: 1.0406\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.389 - ETA: 0s - loss: 0.498 - 0s 893us/step - loss: 0.4801 - val_loss: 1.0372\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.539 - ETA: 0s - loss: 0.391 - 0s 855us/step - loss: 0.4571 - val_loss: 1.1285\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.552 - ETA: 0s - loss: 0.494 - 0s 864us/step - loss: 0.4579 - val_loss: 1.0671\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.917 - ETA: 0s - loss: 0.460 - 0s 883us/step - loss: 0.4264 - val_loss: 1.1080\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.554 - ETA: 0s - loss: 0.486 - 0s 959us/step - loss: 0.4316 - val_loss: 1.0383\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.395 - 0s 864us/step - loss: 0.4180 - val_loss: 1.0920\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.369 - 0s 912us/step - loss: 0.4215 - val_loss: 1.1290\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.749 - ETA: 0s - loss: 0.372 - 0s 1ms/step - loss: 0.4096 - val_loss: 1.2259\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.414 - 0s 912us/step - loss: 0.4191 - val_loss: 1.2664\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.576 - ETA: 0s - loss: 0.433 - 0s 912us/step - loss: 0.4021 - val_loss: 1.1694\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.335 - ETA: 0s - loss: 0.368 - 0s 979us/step - loss: 0.3857 - val_loss: 1.2324\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.486 - 0s 950us/step - loss: 0.3745 - val_loss: 1.2040\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.438 - 0s 902us/step - loss: 0.3727 - val_loss: 1.3232\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.274 - 0s 893us/step - loss: 0.3624 - val_loss: 1.3025\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.631 - ETA: 0s - loss: 0.327 - 0s 845us/step - loss: 0.3868 - val_loss: 1.4429\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.363 - 0s 874us/step - loss: 0.3763 - val_loss: 1.3653\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.458 - 0s 902us/step - loss: 0.3630 - val_loss: 1.2408\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.367 - 0s 950us/step - loss: 0.3613 - val_loss: 1.3489\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.377 - 0s 931us/step - loss: 0.3614 - val_loss: 1.5175\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.662 - ETA: 0s - loss: 0.316 - 0s 969us/step - loss: 0.3450 - val_loss: 1.3668\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.311 - 0s 988us/step - loss: 0.3485 - val_loss: 1.3564\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.337 - 0s 912us/step - loss: 0.3445 - val_loss: 1.4253\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.349 - 0s 940us/step - loss: 0.3270 - val_loss: 1.3435\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.315 - 0s 988us/step - loss: 0.3134 - val_loss: 1.3321\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.355 - 0s 950us/step - loss: 0.3206 - val_loss: 1.5475\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.287 - 0s 845us/step - loss: 0.3147 - val_loss: 1.3894\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.93 - ETA: 0s - loss: 0.7762 - 1s 8ms/step - loss: 0.7367 - val_loss: 0.4660\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.538 - 0s 1ms/step - loss: 0.5282 - val_loss: 0.8009\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.384 - 0s 902us/step - loss: 0.4678 - val_loss: 0.9425\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.175 - ETA: 0s - loss: 0.367 - 0s 826us/step - loss: 0.4354 - val_loss: 0.7965\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.416 - 0s 826us/step - loss: 0.4054 - val_loss: 0.9744\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.411 - ETA: 0s - loss: 0.386 - 0s 864us/step - loss: 0.3832 - val_loss: 0.8266\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.344 - 0s 807us/step - loss: 0.3461 - val_loss: 0.8525\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.667 - ETA: 0s - loss: 0.317 - 0s 826us/step - loss: 0.3342 - val_loss: 0.8745\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.321 - 0s 788us/step - loss: 0.3200 - val_loss: 0.8702\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.350 - 0s 807us/step - loss: 0.3076 - val_loss: 0.8737\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.249 - 0s 798us/step - loss: 0.2897 - val_loss: 0.8200\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.306 - 0s 826us/step - loss: 0.2901 - val_loss: 0.7143\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.282 - 0s 884us/step - loss: 0.2808 - val_loss: 0.9558\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.340 - ETA: 0s - loss: 0.254 - 0s 779us/step - loss: 0.2769 - val_loss: 0.7440\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.292 - 0s 807us/step - loss: 0.2752 - val_loss: 0.8920\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.275 - 0s 750us/step - loss: 0.2570 - val_loss: 0.8726\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.268 - 0s 760us/step - loss: 0.2531 - val_loss: 0.8735\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.276 - 0s 741us/step - loss: 0.2697 - val_loss: 0.8384\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.232 - 0s 712us/step - loss: 0.2461 - val_loss: 0.9747\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.244 - 0s 751us/step - loss: 0.2513 - val_loss: 0.7621\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.433 - ETA: 0s - loss: 0.273 - 0s 760us/step - loss: 0.2501 - val_loss: 0.8757\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.232 - 0s 779us/step - loss: 0.2310 - val_loss: 0.8436\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.236 - 0s 750us/step - loss: 0.2371 - val_loss: 0.8320\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.264 - 0s 760us/step - loss: 0.2327 - val_loss: 1.0399\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.204 - 0s 3ms/step - loss: 0.2246 - val_loss: 0.8718\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.248 - 0s 836us/step - loss: 0.2198 - val_loss: 0.8507\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.232 - 0s 845us/step - loss: 0.2151 - val_loss: 1.0141\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.208 - 0s 732us/step - loss: 0.2174 - val_loss: 0.8505\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.232 - 0s 817us/step - loss: 0.2163 - val_loss: 0.9151\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.195 - 0s 836us/step - loss: 0.2133 - val_loss: 0.8504\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.221 - 0s 807us/step - loss: 0.2042 - val_loss: 1.0248\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.62 - ETA: 0s - loss: 0.8044 - 1s 8ms/step - loss: 0.7083 - val_loss: 0.4865\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.043 - ETA: 0s - loss: 0.565 - 0s 845us/step - loss: 0.5540 - val_loss: 0.7668\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.551 - 0s 760us/step - loss: 0.5028 - val_loss: 0.7469\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.543 - 0s 807us/step - loss: 0.4630 - val_loss: 0.8103\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.552 - 0s 940us/step - loss: 0.4387 - val_loss: 0.7424\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.315 - 0s 817us/step - loss: 0.4146 - val_loss: 0.7586\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.463 - 0s 826us/step - loss: 0.3919 - val_loss: 0.8962\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.408 - 0s 817us/step - loss: 0.3987 - val_loss: 0.8327\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.354 - 0s 997us/step - loss: 0.3562 - val_loss: 0.8068\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.216 - 0s 845us/step - loss: 0.3529 - val_loss: 0.8410\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.363 - 0s 769us/step - loss: 0.3393 - val_loss: 1.0050\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.338 - 0s 807us/step - loss: 0.3310 - val_loss: 0.8611\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.373 - 0s 807us/step - loss: 0.3346 - val_loss: 0.9163\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.573 - ETA: 0s - loss: 0.261 - 0s 826us/step - loss: 0.3188 - val_loss: 0.8757\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.289 - 0s 940us/step - loss: 0.2912 - val_loss: 1.0042\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.856 - ETA: 0s - loss: 0.340 - 0s 836us/step - loss: 0.3035 - val_loss: 1.0697\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.233 - 0s 855us/step - loss: 0.3077 - val_loss: 1.0345\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.283 - 0s 845us/step - loss: 0.2848 - val_loss: 0.9831\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.325 - 0s 855us/step - loss: 0.2757 - val_loss: 1.0686\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.296 - 0s 807us/step - loss: 0.2693 - val_loss: 1.0632\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.248 - 0s 865us/step - loss: 0.2800 - val_loss: 1.2131\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.281 - 0s 883us/step - loss: 0.2757 - val_loss: 1.0910\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.193 - 0s 798us/step - loss: 0.2665 - val_loss: 1.1277\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.538 - ETA: 0s - loss: 0.232 - 0s 893us/step - loss: 0.2609 - val_loss: 1.1788\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.280 - 0s 883us/step - loss: 0.2651 - val_loss: 1.1635\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.239 - 0s 978us/step - loss: 0.2429 - val_loss: 1.1040\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.296 - 0s 826us/step - loss: 0.2722 - val_loss: 0.9942\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.255 - 0s 836us/step - loss: 0.2474 - val_loss: 1.0736\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.215 - 0s 807us/step - loss: 0.2410 - val_loss: 1.0971\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.262 - 0s 855us/step - loss: 0.2410 - val_loss: 1.0400\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.197 - 0s 779us/step - loss: 0.2284 - val_loss: 1.1166\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.48 - ETA: 0s - loss: 0.5951 - 1s 8ms/step - loss: 0.5945 - val_loss: 0.6457\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.332 - 0s 779us/step - loss: 0.3163 - val_loss: 1.5157\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.273 - 0s 807us/step - loss: 0.2799 - val_loss: 1.5869\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.230 - 0s 760us/step - loss: 0.2500 - val_loss: 1.6363\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.584 - ETA: 0s - loss: 0.223 - 0s 855us/step - loss: 0.2434 - val_loss: 1.8071\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.245 - 0s 788us/step - loss: 0.2277 - val_loss: 1.3784\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.234 - 0s 940us/step - loss: 0.2073 - val_loss: 1.8618\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.934 - ETA: 0s - loss: 0.239 - 0s 817us/step - loss: 0.2114 - val_loss: 1.7644\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.202 - 0s 788us/step - loss: 0.1856 - val_loss: 1.7507\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.154 - 0s 807us/step - loss: 0.1907 - val_loss: 1.8718\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.189 - 0s 807us/step - loss: 0.2020 - val_loss: 1.8737\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.163 - 0s 817us/step - loss: 0.1761 - val_loss: 1.9422\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.170 - 0s 789us/step - loss: 0.1708 - val_loss: 1.8644\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.171 - 0s 788us/step - loss: 0.1837 - val_loss: 1.7712\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.188 - 0s 770us/step - loss: 0.1719 - val_loss: 1.9804\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.166 - 0s 788us/step - loss: 0.1718 - val_loss: 2.0602\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.124 - 0s 779us/step - loss: 0.1580 - val_loss: 2.2167\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.173 - 0s 874us/step - loss: 0.1737 - val_loss: 2.1237\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.167 - 0s 769us/step - loss: 0.1663 - val_loss: 2.3179\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.153 - 0s 798us/step - loss: 0.1494 - val_loss: 2.1968\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.150 - 0s 798us/step - loss: 0.1502 - val_loss: 2.2640\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.144 - 0s 826us/step - loss: 0.1555 - val_loss: 2.3146\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.140 - 0s 788us/step - loss: 0.1381 - val_loss: 2.1348\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.142 - 0s 845us/step - loss: 0.1508 - val_loss: 2.2312\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.128 - 0s 788us/step - loss: 0.1417 - val_loss: 2.0790\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.382 - ETA: 0s - loss: 0.137 - 0s 779us/step - loss: 0.1540 - val_loss: 2.2645\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.134 - 0s 845us/step - loss: 0.1262 - val_loss: 2.4961\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.122 - 0s 807us/step - loss: 0.1354 - val_loss: 2.5085\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.138 - 0s 1ms/step - loss: 0.1339 - val_loss: 2.4529\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.497 - ETA: 0s - loss: 0.138 - 0s 874us/step - loss: 0.1294 - val_loss: 2.2441\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.135 - 0s 912us/step - loss: 0.1253 - val_loss: 2.2550\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.91 - ETA: 0s - loss: 0.8106 - 1s 8ms/step - loss: 0.8631 - val_loss: 0.2793\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.602 - 0s 845us/step - loss: 0.6554 - val_loss: 0.6153\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.673 - ETA: 0s - loss: 0.629 - 0s 836us/step - loss: 0.6043 - val_loss: 0.5834\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.747 - ETA: 0s - loss: 0.582 - 0s 807us/step - loss: 0.5749 - val_loss: 0.6034\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.214 - ETA: 0s - loss: 0.643 - 0s 817us/step - loss: 0.5611 - val_loss: 0.6556\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.947 - ETA: 0s - loss: 0.577 - 0s 874us/step - loss: 0.5200 - val_loss: 0.6766\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.582 - ETA: 0s - loss: 0.381 - 0s 855us/step - loss: 0.5185 - val_loss: 0.6913\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.880 - ETA: 0s - loss: 0.534 - 0s 798us/step - loss: 0.4819 - val_loss: 0.6723\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.496 - 0s 836us/step - loss: 0.4936 - val_loss: 0.6433\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.473 - 0s 798us/step - loss: 0.4653 - val_loss: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.520 - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.466 - 0s 1ms/step - loss: 0.4578 - val_loss: 0.8128\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.499 - ETA: 0s - loss: 0.424 - 0s 779us/step - loss: 0.4446 - val_loss: 0.8439\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.487 - ETA: 0s - loss: 0.400 - 0s 779us/step - loss: 0.4342 - val_loss: 0.8461\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.475 - 0s 845us/step - loss: 0.4324 - val_loss: 0.9585\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.514 - ETA: 0s - loss: 0.518 - 0s 836us/step - loss: 0.4202 - val_loss: 0.9145\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.468 - 0s 855us/step - loss: 0.3954 - val_loss: 1.0579\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.819 - ETA: 0s - loss: 0.460 - 0s 921us/step - loss: 0.3843 - val_loss: 1.0875\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.523 - ETA: 0s - loss: 0.412 - 0s 864us/step - loss: 0.4081 - val_loss: 1.0777\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.363 - 0s 855us/step - loss: 0.4038 - val_loss: 0.8451\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.435 - 0s 902us/step - loss: 0.4025 - val_loss: 1.0493\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.377 - 0s 922us/step - loss: 0.3819 - val_loss: 1.0549\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.453 - 0s 874us/step - loss: 0.3892 - val_loss: 1.0284\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.514 - ETA: 0s - loss: 0.340 - 0s 817us/step - loss: 0.3647 - val_loss: 1.1008\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.319 - 0s 826us/step - loss: 0.3570 - val_loss: 1.1370\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.466 - ETA: 0s - loss: 0.327 - 0s 836us/step - loss: 0.3623 - val_loss: 1.1245\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.384 - 0s 798us/step - loss: 0.3416 - val_loss: 1.1450\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.252 - 0s 845us/step - loss: 0.3561 - val_loss: 1.2747\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.318 - 0s 817us/step - loss: 0.3225 - val_loss: 1.1344\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.317 - 0s 855us/step - loss: 0.3430 - val_loss: 1.2144\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.886 - ETA: 0s - loss: 0.332 - 0s 788us/step - loss: 0.3209 - val_loss: 1.2625\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.337 - 0s 864us/step - loss: 0.3264 - val_loss: 1.2939\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.64 - ETA: 0s - loss: 0.7309 - 1s 8ms/step - loss: 0.8233 - val_loss: 0.2380\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.661 - 0s 931us/step - loss: 0.6147 - val_loss: 0.5097\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.572 - 0s 845us/step - loss: 0.5579 - val_loss: 0.7183\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.543 - ETA: 0s - loss: 0.557 - 0s 798us/step - loss: 0.5230 - val_loss: 0.7518\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.475 - 0s 1ms/step - loss: 0.4863 - val_loss: 0.7245\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.448 - 0s 750us/step - loss: 0.4425 - val_loss: 0.8110\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.928 - ETA: 0s - loss: 0.474 - 0s 874us/step - loss: 0.4176 - val_loss: 0.9306\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.386 - 0s 826us/step - loss: 0.3783 - val_loss: 0.9221\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.700 - ETA: 0s - loss: 0.258 - 0s 807us/step - loss: 0.3460 - val_loss: 0.9704\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.385 - 0s 807us/step - loss: 0.3457 - val_loss: 1.1353\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.269 - 0s 855us/step - loss: 0.3307 - val_loss: 1.2042\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.351 - 0s 864us/step - loss: 0.3189 - val_loss: 1.2926\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.292 - 0s 817us/step - loss: 0.3089 - val_loss: 1.2354\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.240 - 0s 836us/step - loss: 0.2956 - val_loss: 1.2250\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.689 - ETA: 0s - loss: 0.312 - 0s 817us/step - loss: 0.2910 - val_loss: 1.2811\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.324 - 0s 836us/step - loss: 0.2909 - val_loss: 1.5117\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.208 - 0s 826us/step - loss: 0.2729 - val_loss: 1.3047\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.280 - 0s 808us/step - loss: 0.2658 - val_loss: 1.2250\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.295 - 0s 864us/step - loss: 0.2767 - val_loss: 1.4141\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.286 - 0s 845us/step - loss: 0.2697 - val_loss: 1.4491\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.191 - 0s 817us/step - loss: 0.2359 - val_loss: 1.2261\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.275 - 0s 826us/step - loss: 0.2612 - val_loss: 1.2835\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.041 - ETA: 0s - loss: 0.317 - 0s 959us/step - loss: 0.2657 - val_loss: 1.4101\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.676 - ETA: 0s - loss: 0.296 - 0s 902us/step - loss: 0.2591 - val_loss: 1.2153\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.197 - 0s 817us/step - loss: 0.2459 - val_loss: 1.2459\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.231 - 0s 893us/step - loss: 0.2364 - val_loss: 1.3257\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.235 - 0s 836us/step - loss: 0.2519 - val_loss: 1.5366\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.141 - 0s 798us/step - loss: 0.2270 - val_loss: 1.3553\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.790 - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.185 - 0s 1ms/step - loss: 0.2294 - val_loss: 1.4660\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.394 - ETA: 0s - loss: 0.281 - 0s 807us/step - loss: 0.2313 - val_loss: 1.4773\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.216 - 0s 855us/step - loss: 0.2214 - val_loss: 1.5720\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.63 - ETA: 0s - loss: 0.8297 - 1s 8ms/step - loss: 0.8347 - val_loss: 0.2436\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.439 - ETA: 0s - loss: 0.826 - 0s 807us/step - loss: 0.6761 - val_loss: 0.5003\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.681 - 0s 817us/step - loss: 0.6397 - val_loss: 0.5828\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.596 - 0s 845us/step - loss: 0.5680 - val_loss: 0.6507\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.356 - ETA: 0s - loss: 0.619 - 0s 855us/step - loss: 0.5560 - val_loss: 0.8542\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.519 - 0s 864us/step - loss: 0.5319 - val_loss: 0.7913\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.558 - 0s 874us/step - loss: 0.4782 - val_loss: 0.9422\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.460 - 0s 836us/step - loss: 0.4595 - val_loss: 1.1431\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.374 - 0s 826us/step - loss: 0.4357 - val_loss: 1.1403\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.586 - ETA: 0s - loss: 0.454 - 0s 826us/step - loss: 0.4336 - val_loss: 1.1363\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.339 - 0s 902us/step - loss: 0.4106 - val_loss: 0.9949\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.429 - ETA: 0s - loss: 0.449 - 0s 836us/step - loss: 0.3952 - val_loss: 1.2703\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.318 - 0s 826us/step - loss: 0.3714 - val_loss: 1.1454\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.343 - 0s 817us/step - loss: 0.3707 - val_loss: 1.2655\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.446 - 0s 893us/step - loss: 0.3645 - val_loss: 1.2905\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.320 - 0s 836us/step - loss: 0.3496 - val_loss: 1.2699\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.362 - 0s 826us/step - loss: 0.3428 - val_loss: 1.3600\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.372 - 0s 846us/step - loss: 0.3355 - val_loss: 1.5750\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.622 - ETA: 0s - loss: 0.377 - 0s 826us/step - loss: 0.3299 - val_loss: 1.4743\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.572 - ETA: 0s - loss: 0.343 - 0s 826us/step - loss: 0.3092 - val_loss: 1.3813\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.383 - ETA: 0s - loss: 0.333 - 0s 798us/step - loss: 0.2906 - val_loss: 1.2193\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.756 - ETA: 0s - loss: 0.292 - 0s 817us/step - loss: 0.2976 - val_loss: 1.6214\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.434 - ETA: 0s - loss: 0.252 - 0s 845us/step - loss: 0.3138 - val_loss: 1.3539\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.711 - ETA: 0s - loss: 0.276 - 0s 817us/step - loss: 0.2884 - val_loss: 1.5998\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.292 - 0s 1ms/step - loss: 0.2900 - val_loss: 1.5672\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.484 - ETA: 0s - loss: 0.298 - 0s 874us/step - loss: 0.2787 - val_loss: 1.7703\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.209 - 0s 874us/step - loss: 0.2738 - val_loss: 1.5390\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.350 - 0s 893us/step - loss: 0.2846 - val_loss: 1.6966\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.814 - ETA: 0s - loss: 0.247 - 0s 874us/step - loss: 0.2479 - val_loss: 1.5637\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.252 - 0s 912us/step - loss: 0.2501 - val_loss: 1.6264\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.191 - 0s 893us/step - loss: 0.2453 - val_loss: 1.5211\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 7\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.82 - ETA: 0s - loss: 0.6509 - 1s 8ms/step - loss: 0.6685 - val_loss: 0.6016\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.572 - ETA: 0s - loss: 0.494 - 0s 874us/step - loss: 0.5035 - val_loss: 1.0449\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.404 - 0s 893us/step - loss: 0.4671 - val_loss: 0.9456\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.597 - ETA: 0s - loss: 0.496 - 0s 836us/step - loss: 0.4742 - val_loss: 0.8711\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.467 - 0s 874us/step - loss: 0.4371 - val_loss: 1.0168\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.493 - ETA: 0s - loss: 0.390 - 0s 807us/step - loss: 0.4264 - val_loss: 1.0502\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.368 - ETA: 0s - loss: 0.420 - 0s 855us/step - loss: 0.4141 - val_loss: 0.9991\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.386 - 0s 836us/step - loss: 0.4050 - val_loss: 0.9468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.447 - 0s 817us/step - loss: 0.4005 - val_loss: 1.1195\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.437 - ETA: 0s - loss: 0.348 - 0s 779us/step - loss: 0.3723 - val_loss: 0.9672\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.332 - 0s 817us/step - loss: 0.3897 - val_loss: 1.1813\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.339 - 0s 779us/step - loss: 0.3738 - val_loss: 0.9915\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.378 - 0s 836us/step - loss: 0.3567 - val_loss: 1.1936\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.548 - ETA: 0s - loss: 0.391 - 0s 826us/step - loss: 0.3602 - val_loss: 1.1406\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.730 - ETA: 0s - loss: 0.342 - 0s 788us/step - loss: 0.3333 - val_loss: 1.2337\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.386 - 0s 874us/step - loss: 0.3485 - val_loss: 1.1624\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.323 - 0s 826us/step - loss: 0.3249 - val_loss: 1.2333\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.331 - 0s 798us/step - loss: 0.3403 - val_loss: 1.1149\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.326 - 0s 798us/step - loss: 0.3254 - val_loss: 1.3230\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.296 - 0s 779us/step - loss: 0.3232 - val_loss: 1.2483\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.342 - 0s 770us/step - loss: 0.3048 - val_loss: 1.2058\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.281 - 0s 741us/step - loss: 0.2896 - val_loss: 1.2613\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.297 - 0s 769us/step - loss: 0.2993 - val_loss: 1.1924\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.248 - 0s 779us/step - loss: 0.2850 - val_loss: 1.2815\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.436 - ETA: 0s - loss: 0.263 - 0s 760us/step - loss: 0.2830 - val_loss: 1.4035\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.302 - 0s 779us/step - loss: 0.2889 - val_loss: 1.3054\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.055 - ETA: 0s - loss: 0.280 - 0s 798us/step - loss: 0.2656 - val_loss: 1.3063\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.259 - 0s 731us/step - loss: 0.2509 - val_loss: 1.4286\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.269 - 0s 722us/step - loss: 0.2653 - val_loss: 1.3017\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.236 - 0s 789us/step - loss: 0.2478 - val_loss: 1.3591\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.281 - 0s 760us/step - loss: 0.2547 - val_loss: 1.2657\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 8\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.51 - ETA: 0s - loss: 1.0067 - 1s 8ms/step - loss: 0.7972 - val_loss: 0.2729\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.589 - 0s 798us/step - loss: 0.4882 - val_loss: 0.6886\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.486 - 0s 827us/step - loss: 0.4129 - val_loss: 0.5954\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.416 - 0s 817us/step - loss: 0.3675 - val_loss: 0.6837\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.373 - 0s 864us/step - loss: 0.3252 - val_loss: 0.5069\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.306 - 0s 798us/step - loss: 0.2759 - val_loss: 0.4760\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.330 - 0s 750us/step - loss: 0.2896 - val_loss: 0.3946\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 0.311 - 0s 988us/step - loss: 0.2520 - val_loss: 0.3827\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.381 - 0s 1ms/step - loss: 0.2223 - val_loss: 0.2836\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.146 - 0s 912us/step - loss: 0.2200 - val_loss: 0.3237\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.183 - 0s 693us/step - loss: 0.1912 - val_loss: 0.3257\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.196 - 0s 741us/step - loss: 0.1827 - val_loss: 0.3090\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.152 - 0s 950us/step - loss: 0.1546 - val_loss: 0.3593\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.171 - 0s 845us/step - loss: 0.1539 - val_loss: 0.2689\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.140 - 0s 893us/step - loss: 0.1578 - val_loss: 0.3380\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.098 - 0s 845us/step - loss: 0.1567 - val_loss: 0.3464\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.116 - 0s 902us/step - loss: 0.1398 - val_loss: 0.3119\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.140 - 0s 855us/step - loss: 0.1311 - val_loss: 0.2890\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.137 - 0s 845us/step - loss: 0.1303 - val_loss: 0.3894\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.116 - 0s 845us/step - loss: 0.1239 - val_loss: 0.3137\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.115 - 0s 874us/step - loss: 0.1105 - val_loss: 0.3001\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.095 - 0s 836us/step - loss: 0.1150 - val_loss: 0.3038\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.134 - 0s 845us/step - loss: 0.1160 - val_loss: 0.2976\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.142 - 0s 836us/step - loss: 0.1305 - val_loss: 0.3843\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.082 - 0s 798us/step - loss: 0.0921 - val_loss: 0.3133\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.105 - 0s 864us/step - loss: 0.1073 - val_loss: 0.3400\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.114 - 0s 760us/step - loss: 0.1230 - val_loss: 0.3544\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.109 - 0s 741us/step - loss: 0.1070 - val_loss: 0.3914\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.110 - 0s 703us/step - loss: 0.1107 - val_loss: 0.3410\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.092 - 0s 788us/step - loss: 0.0910 - val_loss: 0.3583\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.090 - 0s 713us/step - loss: 0.0938 - val_loss: 0.2526\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.123 - 0s 826us/step - loss: 0.1175 - val_loss: 0.3889\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.099 - 0s 770us/step - loss: 0.0895 - val_loss: 0.2348\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.100 - 0s 760us/step - loss: 0.0885 - val_loss: 0.3258\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.088 - 0s 788us/step - loss: 0.0883 - val_loss: 0.2493\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.096 - 0s 798us/step - loss: 0.0863 - val_loss: 0.2843\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.076 - 0s 779us/step - loss: 0.0896 - val_loss: 0.2413\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.100 - 0s 798us/step - loss: 0.0949 - val_loss: 0.3846\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.093 - 0s 779us/step - loss: 0.0904 - val_loss: 0.2720\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.091 - 0s 836us/step - loss: 0.0857 - val_loss: 0.2778\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.080 - 0s 865us/step - loss: 0.0839 - val_loss: 0.3017\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.077 - 0s 798us/step - loss: 0.0891 - val_loss: 0.3737\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.063 - 0s 1ms/step - loss: 0.0869 - val_loss: 0.2233\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.104 - 0s 779us/step - loss: 0.0969 - val_loss: 0.3137\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.073 - 0s 798us/step - loss: 0.0751 - val_loss: 0.2829\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.079 - 0s 817us/step - loss: 0.0779 - val_loss: 0.3220\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.061 - 0s 807us/step - loss: 0.0704 - val_loss: 0.3024\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.071 - 0s 836us/step - loss: 0.0725 - val_loss: 0.2426\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.082 - 0s 836us/step - loss: 0.0734 - val_loss: 0.2701\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.087 - 0s 788us/step - loss: 0.0806 - val_loss: 0.2942\n",
      "(105,) (15,)\n",
      "model compiled 9\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.17 - ETA: 0s - loss: 0.8593 - 1s 8ms/step - loss: 0.6903 - val_loss: 0.3879\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.606 - 0s 779us/step - loss: 0.5761 - val_loss: 0.8459\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.465 - 0s 826us/step - loss: 0.4602 - val_loss: 0.5632\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.453 - 0s 883us/step - loss: 0.4327 - val_loss: 0.5527\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.388 - ETA: 0s - loss: 0.453 - 0s 807us/step - loss: 0.3733 - val_loss: 0.6628\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.285 - 0s 874us/step - loss: 0.3480 - val_loss: 0.5935\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.355 - 0s 836us/step - loss: 0.3212 - val_loss: 0.6505\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.339 - 0s 845us/step - loss: 0.3124 - val_loss: 0.4531\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.194 - 0s 826us/step - loss: 0.2743 - val_loss: 0.4716\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.267 - 0s 807us/step - loss: 0.2711 - val_loss: 0.5802\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.256 - 0s 855us/step - loss: 0.2241 - val_loss: 0.4028\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.211 - 0s 855us/step - loss: 0.2066 - val_loss: 0.4918\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.219 - 0s 912us/step - loss: 0.2069 - val_loss: 0.4396\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.193 - 0s 2ms/step - loss: 0.1981 - val_loss: 0.5591\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.171 - 0s 846us/step - loss: 0.1811 - val_loss: 0.5248\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.134 - 0s 845us/step - loss: 0.1772 - val_loss: 0.3730\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.184 - 0s 807us/step - loss: 0.1788 - val_loss: 0.6015\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.190 - 0s 836us/step - loss: 0.1719 - val_loss: 0.5485\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.167 - 0s 779us/step - loss: 0.1766 - val_loss: 0.5256\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.168 - 0s 893us/step - loss: 0.1817 - val_loss: 0.5692\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.168 - 0s 1ms/step - loss: 0.1667 - val_loss: 0.7338\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.176 - 0s 874us/step - loss: 0.1700 - val_loss: 0.6261\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.139 - 0s 883us/step - loss: 0.1401 - val_loss: 0.6628\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.140 - 0s 893us/step - loss: 0.1399 - val_loss: 0.6975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.145 - 0s 798us/step - loss: 0.1385 - val_loss: 0.7784\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.571 - ETA: 0s - loss: 0.160 - 0s 798us/step - loss: 0.1494 - val_loss: 0.6386\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.131 - 0s 779us/step - loss: 0.1181 - val_loss: 0.7205\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.131 - 0s 817us/step - loss: 0.1361 - val_loss: 0.7530\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.116 - 0s 855us/step - loss: 0.1423 - val_loss: 0.7631\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.458 - ETA: 0s - loss: 0.136 - 0s 836us/step - loss: 0.1279 - val_loss: 0.8008\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.121 - 0s 1ms/step - loss: 0.1265 - val_loss: 0.8664\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.151 - 0s 845us/step - loss: 0.1362 - val_loss: 0.7624\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.155 - 0s 836us/step - loss: 0.1419 - val_loss: 0.5607\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.134 - 0s 826us/step - loss: 0.1388 - val_loss: 0.7666\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.133 - 0s 893us/step - loss: 0.1228 - val_loss: 0.7449\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.135 - 0s 836us/step - loss: 0.1179 - val_loss: 0.6580\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.105 - 0s 827us/step - loss: 0.1197 - val_loss: 0.8008\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.136 - 0s 836us/step - loss: 0.1082 - val_loss: 0.7361\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.063 - 0s 788us/step - loss: 0.0992 - val_loss: 0.7720\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.114 - 0s 845us/step - loss: 0.1254 - val_loss: 1.0316\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.085 - 0s 788us/step - loss: 0.1163 - val_loss: 0.7164\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.116 - 0s 836us/step - loss: 0.1086 - val_loss: 0.7197\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.094 - 0s 826us/step - loss: 0.1008 - val_loss: 0.7994\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.076 - 0s 788us/step - loss: 0.0959 - val_loss: 0.8151\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.112 - 0s 855us/step - loss: 0.1003 - val_loss: 0.7903\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.107 - 0s 798us/step - loss: 0.1021 - val_loss: 0.7044\n",
      "Epoch 00046: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.18 - ETA: 0s - loss: 0.5264 - 1s 9ms/step - loss: 0.6167 - val_loss: 0.3713\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.544 - 0s 798us/step - loss: 0.4597 - val_loss: 0.6570\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.416 - 0s 808us/step - loss: 0.4208 - val_loss: 0.5393\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.351 - ETA: 0s - loss: 0.420 - 0s 807us/step - loss: 0.3844 - val_loss: 0.6096\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.202 - 0s 788us/step - loss: 0.3547 - val_loss: 0.4745\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.239 - 0s 836us/step - loss: 0.3072 - val_loss: 0.4883\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.408 - 0s 836us/step - loss: 0.3192 - val_loss: 0.5500\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.237 - 0s 788us/step - loss: 0.2974 - val_loss: 0.4493\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.297 - 0s 883us/step - loss: 0.2318 - val_loss: 0.4187\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.211 - 0s 845us/step - loss: 0.2254 - val_loss: 0.4200\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.679 - ETA: 0s - loss: 0.251 - 0s 902us/step - loss: 0.2141 - val_loss: 0.4445\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.193 - 0s 788us/step - loss: 0.2182 - val_loss: 0.4061\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.226 - 0s 826us/step - loss: 0.2151 - val_loss: 0.4893\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.224 - 0s 864us/step - loss: 0.1849 - val_loss: 0.3510\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.186 - 0s 855us/step - loss: 0.1852 - val_loss: 0.4109\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.197 - 0s 941us/step - loss: 0.1876 - val_loss: 0.4853\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.156 - 0s 921us/step - loss: 0.1534 - val_loss: 0.3870\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.160 - 0s 807us/step - loss: 0.1426 - val_loss: 0.3237\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.171 - 0s 836us/step - loss: 0.1364 - val_loss: 0.3838\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.155 - 0s 855us/step - loss: 0.1625 - val_loss: 0.4402\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.010 - ETA: 0s - loss: 0.142 - 0s 798us/step - loss: 0.1532 - val_loss: 0.3666\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.129 - 0s 1ms/step - loss: 0.1372 - val_loss: 0.3587\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.137 - 0s 893us/step - loss: 0.1268 - val_loss: 0.2975\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.119 - 0s 836us/step - loss: 0.1257 - val_loss: 0.3544\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.152 - 0s 855us/step - loss: 0.1336 - val_loss: 0.3574\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.118 - 0s 902us/step - loss: 0.1146 - val_loss: 0.3643\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.145 - 0s 902us/step - loss: 0.1201 - val_loss: 0.3802\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.089 - 0s 798us/step - loss: 0.1054 - val_loss: 0.3815\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.105 - 0s 807us/step - loss: 0.0951 - val_loss: 0.4055\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.081 - 0s 826us/step - loss: 0.0916 - val_loss: 0.3029\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.079 - 0s 788us/step - loss: 0.0960 - val_loss: 0.3433\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.094 - 0s 902us/step - loss: 0.0811 - val_loss: 0.3194\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - 0s 750us/step - loss: 0.0950 - val_loss: 0.3255\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.095 - 0s 750us/step - loss: 0.0857 - val_loss: 0.2931\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.090 - 0s 779us/step - loss: 0.0864 - val_loss: 0.3397\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.089 - 0s 836us/step - loss: 0.0840 - val_loss: 0.3052\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.071 - 0s 722us/step - loss: 0.0739 - val_loss: 0.3441\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.078 - 0s 769us/step - loss: 0.0750 - val_loss: 0.3120\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.086 - 0s 808us/step - loss: 0.0798 - val_loss: 0.3042\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.085 - 0s 817us/step - loss: 0.0802 - val_loss: 0.3024\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.073 - 0s 817us/step - loss: 0.0730 - val_loss: 0.3288\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.072 - 0s 751us/step - loss: 0.0744 - val_loss: 0.2929\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.076 - 0s 883us/step - loss: 0.0723 - val_loss: 0.3393\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.067 - 0s 807us/step - loss: 0.0733 - val_loss: 0.3265\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.068 - 0s 798us/step - loss: 0.0633 - val_loss: 0.2702\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.054 - 0s 826us/step - loss: 0.0617 - val_loss: 0.2844\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.061 - 0s 817us/step - loss: 0.0610 - val_loss: 0.2780\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.073 - 0s 846us/step - loss: 0.0708 - val_loss: 0.3341\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.059 - 0s 826us/step - loss: 0.0624 - val_loss: 0.2946\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.064 - 0s 807us/step - loss: 0.0607 - val_loss: 0.3270\n",
      "(105,) (15,)\n",
      "model compiled 11\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.75 - ETA: 0s - loss: 0.7724 - 1s 8ms/step - loss: 0.6888 - val_loss: 0.4646\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.535 - 0s 836us/step - loss: 0.4666 - val_loss: 1.1731\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.429 - 0s 836us/step - loss: 0.4103 - val_loss: 1.1180\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.374 - 0s 836us/step - loss: 0.3936 - val_loss: 1.0951\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.320 - 0s 845us/step - loss: 0.3481 - val_loss: 1.5555\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.345 - 0s 807us/step - loss: 0.3276 - val_loss: 1.2630\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.302 - 0s 817us/step - loss: 0.3184 - val_loss: 1.1250\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.249 - 0s 826us/step - loss: 0.3196 - val_loss: 1.0734\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.292 - 0s 855us/step - loss: 0.2905 - val_loss: 1.4910\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.282 - 0s 836us/step - loss: 0.2966 - val_loss: 1.1472\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.272 - 0s 845us/step - loss: 0.2878 - val_loss: 1.2236\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.275 - 0s 807us/step - loss: 0.2696 - val_loss: 1.1112\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.234 - 0s 798us/step - loss: 0.2554 - val_loss: 1.1254\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.573 - ETA: 0s - loss: 0.221 - 0s 836us/step - loss: 0.2580 - val_loss: 1.3157\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.270 - 0s 817us/step - loss: 0.2752 - val_loss: 1.1420\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.247 - 0s 883us/step - loss: 0.2474 - val_loss: 1.3043\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.216 - 0s 826us/step - loss: 0.2531 - val_loss: 1.1745\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.257 - 0s 826us/step - loss: 0.2442 - val_loss: 1.3266\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.280 - 0s 893us/step - loss: 0.2395 - val_loss: 1.2391\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.215 - 0s 836us/step - loss: 0.2295 - val_loss: 1.3449\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.215 - 0s 817us/step - loss: 0.2379 - val_loss: 1.2809\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.188 - 0s 874us/step - loss: 0.2255 - val_loss: 1.0888\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.236 - 0s 826us/step - loss: 0.2267 - val_loss: 1.3018\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.226 - 0s 836us/step - loss: 0.2347 - val_loss: 1.1621\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.444 - ETA: 0s - loss: 0.228 - 0s 845us/step - loss: 0.2177 - val_loss: 1.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.637 - ETA: 0s - loss: 0.226 - 0s 883us/step - loss: 0.2118 - val_loss: 1.2146\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.223 - 0s 817us/step - loss: 0.2211 - val_loss: 1.0782\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.490 - ETA: 0s - loss: 0.226 - 0s 769us/step - loss: 0.2159 - val_loss: 1.1681\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.177 - 0s 769us/step - loss: 0.1888 - val_loss: 1.4174\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.192 - 0s 741us/step - loss: 0.1971 - val_loss: 1.2408\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.210 - 0s 712us/step - loss: 0.2046 - val_loss: 1.4842\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 12\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.91 - ETA: 0s - loss: 0.7633 - 1s 8ms/step - loss: 0.6792 - val_loss: 0.5933\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.424 - 0s 826us/step - loss: 0.4125 - val_loss: 1.0364\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.760 - ETA: 0s - loss: 0.401 - 0s 769us/step - loss: 0.3474 - val_loss: 1.2635\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.306 - 0s 760us/step - loss: 0.3204 - val_loss: 1.4142\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.611 - ETA: 0s - loss: 0.308 - 0s 807us/step - loss: 0.3073 - val_loss: 1.5562\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.535 - ETA: 0s - loss: 0.291 - 0s 817us/step - loss: 0.2681 - val_loss: 1.5443\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.241 - 0s 845us/step - loss: 0.2553 - val_loss: 1.6614\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.238 - 0s 846us/step - loss: 0.2255 - val_loss: 1.6380\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.229 - 0s 874us/step - loss: 0.2388 - val_loss: 1.8226\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.229 - 0s 807us/step - loss: 0.2170 - val_loss: 1.8053\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.205 - 0s 836us/step - loss: 0.2077 - val_loss: 1.9865\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.237 - 0s 912us/step - loss: 0.2102 - val_loss: 1.9416\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.205 - 0s 788us/step - loss: 0.1972 - val_loss: 1.5660\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.196 - 0s 846us/step - loss: 0.1949 - val_loss: 2.2343\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.189 - 0s 874us/step - loss: 0.1738 - val_loss: 1.8470\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.198 - 0s 893us/step - loss: 0.1983 - val_loss: 2.1183\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.142 - 0s 903us/step - loss: 0.1726 - val_loss: 2.2337\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.152 - 0s 912us/step - loss: 0.1571 - val_loss: 2.1665\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.150 - 0s 1ms/step - loss: 0.1567 - val_loss: 1.8553\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.161 - 0s 788us/step - loss: 0.1719 - val_loss: 2.1264\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.151 - 0s 808us/step - loss: 0.1558 - val_loss: 1.9912\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.155 - 0s 807us/step - loss: 0.1556 - val_loss: 2.5113\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.157 - 0s 845us/step - loss: 0.1458 - val_loss: 2.1914\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.140 - 0s 845us/step - loss: 0.1434 - val_loss: 2.0991\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.141 - 0s 845us/step - loss: 0.1484 - val_loss: 2.0779\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.113 - 0s 845us/step - loss: 0.1352 - val_loss: 2.5003\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.144 - 0s 817us/step - loss: 0.1390 - val_loss: 1.9037\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.132 - 0s 807us/step - loss: 0.1276 - val_loss: 2.6389\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.116 - 0s 845us/step - loss: 0.1324 - val_loss: 2.1791\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.134 - 0s 902us/step - loss: 0.1313 - val_loss: 2.4060\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.119 - 0s 855us/step - loss: 0.1268 - val_loss: 2.5518\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 13\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.96 - ETA: 0s - loss: 0.6597 - 1s 8ms/step - loss: 0.7060 - val_loss: 0.5328\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.864 - ETA: 0s - loss: 0.562 - 0s 817us/step - loss: 0.5274 - val_loss: 0.8932\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.489 - 0s 836us/step - loss: 0.4763 - val_loss: 1.0796\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.430 - 0s 864us/step - loss: 0.4420 - val_loss: 1.0661\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.784 - ETA: 0s - loss: 0.365 - 0s 855us/step - loss: 0.4104 - val_loss: 1.1521\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.436 - 0s 921us/step - loss: 0.4028 - val_loss: 1.5751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.464 - 0s 826us/step - loss: 0.3968 - val_loss: 1.3853\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.316 - 0s 845us/step - loss: 0.3677 - val_loss: 1.5189\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.734 - ETA: 0s - loss: 0.355 - 0s 807us/step - loss: 0.3363 - val_loss: 1.5593\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.303 - 0s 836us/step - loss: 0.3315 - val_loss: 1.5995\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.635 - ETA: 0s - loss: 0.322 - 0s 779us/step - loss: 0.3350 - val_loss: 1.7986\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.590 - ETA: 0s - loss: 0.346 - 0s 750us/step - loss: 0.3093 - val_loss: 1.4841\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.283 - 0s 789us/step - loss: 0.2913 - val_loss: 1.8787\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.378 - ETA: 0s - loss: 0.317 - 0s 836us/step - loss: 0.2982 - val_loss: 1.8920\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.317 - 0s 807us/step - loss: 0.3003 - val_loss: 1.7731\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.408 - ETA: 0s - loss: 0.270 - 0s 807us/step - loss: 0.2786 - val_loss: 2.1245\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.454 - ETA: 0s - loss: 0.240 - 0s 817us/step - loss: 0.2845 - val_loss: 2.0118\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.427 - ETA: 0s - loss: 0.244 - 0s 798us/step - loss: 0.2746 - val_loss: 2.0624\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.289 - 0s 788us/step - loss: 0.2687 - val_loss: 2.0257\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.271 - 0s 922us/step - loss: 0.2732 - val_loss: 1.9868\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.435 - ETA: 0s - loss: 0.284 - 0s 865us/step - loss: 0.2613 - val_loss: 1.9542\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.237 - 0s 798us/step - loss: 0.2533 - val_loss: 2.2108\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.226 - 0s 826us/step - loss: 0.2330 - val_loss: 2.0449\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.210 - 0s 883us/step - loss: 0.2466 - val_loss: 2.3456\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.509 - ETA: 0s - loss: 0.213 - 0s 788us/step - loss: 0.2051 - val_loss: 2.0877\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.225 - 0s 741us/step - loss: 0.2343 - val_loss: 2.3204\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.194 - 0s 826us/step - loss: 0.2054 - val_loss: 1.9944\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.218 - 0s 855us/step - loss: 0.2180 - val_loss: 2.2710\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.176 - 0s 855us/step - loss: 0.1995 - val_loss: 2.3196\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.211 - 0s 912us/step - loss: 0.2103 - val_loss: 2.2677\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.193 - 0s 921us/step - loss: 0.1939 - val_loss: 2.6555\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 14\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.51 - ETA: 0s - loss: 0.7267 - 1s 8ms/step - loss: 0.7487 - val_loss: 0.4066\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.700 - 0s 940us/step - loss: 0.5704 - val_loss: 0.7673\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.465 - ETA: 0s - loss: 0.502 - 0s 788us/step - loss: 0.4897 - val_loss: 0.9596\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.592 - ETA: 0s - loss: 0.429 - 0s 864us/step - loss: 0.4345 - val_loss: 0.8177\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.398 - 0s 902us/step - loss: 0.3893 - val_loss: 1.0382\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.271 - 0s 855us/step - loss: 0.3705 - val_loss: 1.4461\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.627 - ETA: 0s - loss: 0.324 - 0s 921us/step - loss: 0.3372 - val_loss: 1.5989\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.323 - 0s 826us/step - loss: 0.2958 - val_loss: 1.8674\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.336 - 0s 826us/step - loss: 0.2975 - val_loss: 2.1551\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.299 - 0s 893us/step - loss: 0.2650 - val_loss: 2.2377\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.231 - 0s 921us/step - loss: 0.2709 - val_loss: 2.2740\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.653 - ETA: 0s - loss: 0.333 - 0s 988us/step - loss: 0.2811 - val_loss: 2.5670\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.272 - 0s 874us/step - loss: 0.2575 - val_loss: 2.7714\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.271 - 0s 959us/step - loss: 0.2586 - val_loss: 2.9012\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.273 - 0s 836us/step - loss: 0.2444 - val_loss: 2.7178\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.262 - 0s 760us/step - loss: 0.2485 - val_loss: 2.7821\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.273 - 0s 845us/step - loss: 0.2452 - val_loss: 3.1296\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.181 - 0s 836us/step - loss: 0.2276 - val_loss: 3.0057\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.214 - 0s 836us/step - loss: 0.2194 - val_loss: 3.3671\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.244 - 0s 855us/step - loss: 0.2242 - val_loss: 3.1440\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.666 - ETA: 0s - loss: 0.197 - 0s 865us/step - loss: 0.2167 - val_loss: 3.4572\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.233 - 0s 817us/step - loss: 0.2125 - val_loss: 3.2825\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.217 - 0s 874us/step - loss: 0.2176 - val_loss: 3.5693\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.211 - 0s 883us/step - loss: 0.2102 - val_loss: 3.4209\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.199 - 0s 769us/step - loss: 0.2010 - val_loss: 3.5722\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.213 - 0s 798us/step - loss: 0.1970 - val_loss: 3.5617\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.185 - 0s 807us/step - loss: 0.1987 - val_loss: 3.1941\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.177 - 0s 817us/step - loss: 0.1864 - val_loss: 3.3767\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.200 - 0s 827us/step - loss: 0.1892 - val_loss: 3.5122\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.171 - 0s 826us/step - loss: 0.1789 - val_loss: 2.9466\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.171 - 0s 769us/step - loss: 0.1958 - val_loss: 3.3257\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 15\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.42 - ETA: 0s - loss: 0.8486 - 1s 8ms/step - loss: 0.7446 - val_loss: 0.6271\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.408 - ETA: 0s - loss: 0.388 - 0s 807us/step - loss: 0.5729 - val_loss: 0.6978\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.441 - 0s 760us/step - loss: 0.5055 - val_loss: 0.7231\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.660 - ETA: 0s - loss: 0.486 - 0s 845us/step - loss: 0.4616 - val_loss: 0.9221\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.564 - ETA: 0s - loss: 0.464 - 0s 845us/step - loss: 0.4267 - val_loss: 0.7451\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.447 - 0s 826us/step - loss: 0.4219 - val_loss: 0.7363\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.352 - 0s 817us/step - loss: 0.4129 - val_loss: 0.6264\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.364 - 0s 779us/step - loss: 0.3891 - val_loss: 0.7301\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.395 - 0s 826us/step - loss: 0.3614 - val_loss: 0.6758\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.380 - 0s 836us/step - loss: 0.3508 - val_loss: 0.8329\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.301 - 0s 941us/step - loss: 0.3512 - val_loss: 0.7658\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.352 - 0s 874us/step - loss: 0.3533 - val_loss: 0.6979\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.212 - 0s 845us/step - loss: 0.3368 - val_loss: 0.5734\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.294 - 0s 836us/step - loss: 0.3189 - val_loss: 0.5838\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.347 - 0s 864us/step - loss: 0.2965 - val_loss: 0.6224\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.340 - 0s 855us/step - loss: 0.3030 - val_loss: 0.5963\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.284 - 0s 845us/step - loss: 0.3017 - val_loss: 0.6297\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.272 - 0s 798us/step - loss: 0.2875 - val_loss: 0.5307\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.651 - ETA: 0s - loss: 0.276 - 0s 807us/step - loss: 0.2906 - val_loss: 0.6305\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.293 - 0s 779us/step - loss: 0.2821 - val_loss: 0.5388\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.264 - 0s 855us/step - loss: 0.2766 - val_loss: 0.5528\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.300 - 0s 874us/step - loss: 0.2549 - val_loss: 0.5389\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.279 - 0s 855us/step - loss: 0.2536 - val_loss: 0.6097\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.241 - 0s 883us/step - loss: 0.2574 - val_loss: 0.5988\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.223 - 0s 997us/step - loss: 0.2464 - val_loss: 0.4833\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.231 - 0s 893us/step - loss: 0.2464 - val_loss: 0.5191\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.270 - 0s 855us/step - loss: 0.2477 - val_loss: 0.5011\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.211 - 0s 912us/step - loss: 0.2241 - val_loss: 0.5391\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.248 - 0s 883us/step - loss: 0.2354 - val_loss: 0.5709\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.188 - 0s 846us/step - loss: 0.2101 - val_loss: 0.5691\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.222 - 0s 950us/step - loss: 0.2039 - val_loss: 0.6250\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.235 - 0s 902us/step - loss: 0.2187 - val_loss: 0.5979\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.467 - ETA: 0s - loss: 0.165 - 0s 865us/step - loss: 0.1889 - val_loss: 0.5851\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.178 - 0s 1ms/step - loss: 0.1915 - val_loss: 0.6676\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.194 - 0s 902us/step - loss: 0.1747 - val_loss: 0.5477\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.190 - 0s 884us/step - loss: 0.1955 - val_loss: 0.6519\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.183 - 0s 807us/step - loss: 0.1851 - val_loss: 0.6654\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.164 - 0s 798us/step - loss: 0.1822 - val_loss: 0.5935\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.195 - 0s 950us/step - loss: 0.1804 - val_loss: 0.6831\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.168 - 0s 893us/step - loss: 0.1742 - val_loss: 0.7198\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.139 - 0s 883us/step - loss: 0.1705 - val_loss: 0.7345\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.186 - 0s 788us/step - loss: 0.1711 - val_loss: 0.6738\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.163 - 0s 798us/step - loss: 0.1562 - val_loss: 0.6789\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.128 - 0s 817us/step - loss: 0.1535 - val_loss: 0.8047\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.143 - 0s 845us/step - loss: 0.1561 - val_loss: 0.6918\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.149 - 0s 855us/step - loss: 0.1468 - val_loss: 0.7943\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.132 - 0s 798us/step - loss: 0.1356 - val_loss: 0.6830\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.168 - 0s 817us/step - loss: 0.1560 - val_loss: 0.8514\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.113 - 0s 788us/step - loss: 0.1359 - val_loss: 0.6993\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.133 - 0s 826us/step - loss: 0.1291 - val_loss: 0.6620\n",
      "(105,) (15,)\n",
      "model compiled 16\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.31 - ETA: 0s - loss: 0.6625 - 1s 8ms/step - loss: 0.7372 - val_loss: 0.4195\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.377 - ETA: 0s - loss: 0.428 - 0s 807us/step - loss: 0.5078 - val_loss: 0.8510\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.519 - 0s 826us/step - loss: 0.4740 - val_loss: 1.2363\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.416 - 0s 788us/step - loss: 0.4718 - val_loss: 0.9411\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.154 - ETA: 0s - loss: 0.499 - 0s 864us/step - loss: 0.4313 - val_loss: 0.9569\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.450 - 0s 826us/step - loss: 0.4128 - val_loss: 0.9924\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.998 - ETA: 0s - loss: 0.418 - 0s 865us/step - loss: 0.3979 - val_loss: 1.0346\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.422 - 0s 912us/step - loss: 0.3948 - val_loss: 0.9699\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.300 - 0s 826us/step - loss: 0.3646 - val_loss: 0.9570\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.601 - ETA: 0s - loss: 0.381 - 0s 817us/step - loss: 0.3441 - val_loss: 1.1439\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.378 - 0s 788us/step - loss: 0.3549 - val_loss: 1.1524\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.982 - ETA: 0s - loss: 0.344 - 0s 779us/step - loss: 0.3267 - val_loss: 1.1791\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.361 - ETA: 0s - loss: 0.342 - 0s 788us/step - loss: 0.3170 - val_loss: 1.1917\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.450 - ETA: 0s - loss: 0.402 - 0s 893us/step - loss: 0.3236 - val_loss: 1.1709\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.369 - 0s 874us/step - loss: 0.3001 - val_loss: 1.3036\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.269 - 0s 760us/step - loss: 0.2936 - val_loss: 1.4532\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.263 - 0s 883us/step - loss: 0.3013 - val_loss: 1.3385\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.345 - 0s 798us/step - loss: 0.2933 - val_loss: 1.5314\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.275 - 0s 1ms/step - loss: 0.2968 - val_loss: 1.5189\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.298 - 0s 845us/step - loss: 0.2756 - val_loss: 1.5938\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.286 - 0s 845us/step - loss: 0.2883 - val_loss: 1.7377\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.308 - 0s 864us/step - loss: 0.2854 - val_loss: 1.7366\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.298 - 0s 864us/step - loss: 0.2570 - val_loss: 1.6478\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.230 - 0s 864us/step - loss: 0.2611 - val_loss: 1.9412\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.512 - ETA: 0s - loss: 0.203 - 0s 959us/step - loss: 0.2452 - val_loss: 1.6740\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.218 - 0s 845us/step - loss: 0.2573 - val_loss: 1.7325\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.279 - 0s 902us/step - loss: 0.2606 - val_loss: 1.7372\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.191 - 0s 874us/step - loss: 0.2357 - val_loss: 1.8143\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.193 - 0s 931us/step - loss: 0.2388 - val_loss: 1.8046\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.267 - 0s 883us/step - loss: 0.2475 - val_loss: 1.9111\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.283 - 0s 921us/step - loss: 0.2273 - val_loss: 1.8477\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 17\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.99 - ETA: 0s - loss: 0.8727 - 1s 8ms/step - loss: 0.8564 - val_loss: 0.3442\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.857 - 0s 845us/step - loss: 0.7205 - val_loss: 0.5302\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.387 - ETA: 0s - loss: 0.772 - 0s 808us/step - loss: 0.6766 - val_loss: 0.5839\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.419 - ETA: 0s - loss: 0.581 - 0s 817us/step - loss: 0.6274 - val_loss: 0.5426\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.967 - ETA: 0s - loss: 0.540 - 0s 855us/step - loss: 0.6003 - val_loss: 0.5633\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.684 - ETA: 0s - loss: 0.635 - 0s 788us/step - loss: 0.5586 - val_loss: 0.6378\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.581 - 0s 836us/step - loss: 0.5247 - val_loss: 0.6490\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.788 - ETA: 0s - loss: 0.548 - 0s 760us/step - loss: 0.5069 - val_loss: 0.6722\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.494 - 0s 836us/step - loss: 0.4726 - val_loss: 0.7494\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.707 - ETA: 0s - loss: 0.429 - 0s 770us/step - loss: 0.4477 - val_loss: 0.8308\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.465 - 0s 732us/step - loss: 0.4726 - val_loss: 0.7608\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.387 - 0s 760us/step - loss: 0.4258 - val_loss: 0.8486\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.390 - 0s 741us/step - loss: 0.4168 - val_loss: 0.9250\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.419 - 0s 750us/step - loss: 0.4014 - val_loss: 1.0250\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.426 - 0s 807us/step - loss: 0.3806 - val_loss: 1.1888\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.630 - ETA: 0s - loss: 0.415 - 0s 893us/step - loss: 0.4095 - val_loss: 1.1848\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.424 - 0s 826us/step - loss: 0.3699 - val_loss: 1.2653\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.625 - ETA: 0s - loss: 0.334 - 0s 817us/step - loss: 0.3844 - val_loss: 1.1524\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.320 - 0s 864us/step - loss: 0.3476 - val_loss: 1.3097\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.164 - ETA: 0s - loss: 0.349 - 0s 902us/step - loss: 0.3801 - val_loss: 1.1933\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.346 - 0s 798us/step - loss: 0.3304 - val_loss: 1.2884\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.353 - 0s 836us/step - loss: 0.3534 - val_loss: 1.3273\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.251 - 0s 855us/step - loss: 0.3126 - val_loss: 1.4071\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.383 - 0s 817us/step - loss: 0.3328 - val_loss: 1.6268\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.300 - 0s 836us/step - loss: 0.3158 - val_loss: 1.4792\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.352 - 0s 855us/step - loss: 0.3290 - val_loss: 1.4827\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.309 - 0s 902us/step - loss: 0.3013 - val_loss: 1.7118\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.275 - 0s 845us/step - loss: 0.2914 - val_loss: 1.6047\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.332 - 0s 864us/step - loss: 0.3289 - val_loss: 1.9277\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.267 - 0s 817us/step - loss: 0.2906 - val_loss: 1.7367\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.276 - 0s 959us/step - loss: 0.2609 - val_loss: 1.9755\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 18\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.41 - ETA: 0s - loss: 1.0983 - 1s 8ms/step - loss: 0.9040 - val_loss: 0.1764\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.180 - ETA: 0s - loss: 0.625 - 0s 950us/step - loss: 0.6358 - val_loss: 0.5610\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.544 - 0s 865us/step - loss: 0.5584 - val_loss: 0.6965\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.459 - 0s 855us/step - loss: 0.4978 - val_loss: 0.6842\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.368 - ETA: 0s - loss: 0.357 - 0s 807us/step - loss: 0.4541 - val_loss: 0.7460\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.448 - 0s 884us/step - loss: 0.4415 - val_loss: 0.8841\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.201 - ETA: 0s - loss: 0.506 - 0s 883us/step - loss: 0.4208 - val_loss: 0.8828\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.374 - 0s 922us/step - loss: 0.3713 - val_loss: 0.7743\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.740 - ETA: 0s - loss: 0.387 - 0s 855us/step - loss: 0.3673 - val_loss: 1.0473\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.307 - 0s 855us/step - loss: 0.3407 - val_loss: 0.9710\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.362 - 0s 950us/step - loss: 0.3327 - val_loss: 1.0435\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.181 - 0s 1ms/step - loss: 0.3294 - val_loss: 1.0602\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.347 - 0s 722us/step - loss: 0.3170 - val_loss: 0.8126\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.239 - 0s 750us/step - loss: 0.3051 - val_loss: 0.9843\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.654 - ETA: 0s - loss: 0.255 - 0s 836us/step - loss: 0.3083 - val_loss: 1.0411\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.310 - 0s 883us/step - loss: 0.2948 - val_loss: 1.1179\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.349 - 0s 912us/step - loss: 0.2971 - val_loss: 0.9545\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.311 - 0s 921us/step - loss: 0.2798 - val_loss: 0.9823\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.308 - 0s 807us/step - loss: 0.2786 - val_loss: 1.0301\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.307 - 0s 922us/step - loss: 0.2801 - val_loss: 0.9380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.311 - 0s 845us/step - loss: 0.2760 - val_loss: 0.9980\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.294 - 0s 798us/step - loss: 0.2608 - val_loss: 0.9882\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.318 - 0s 893us/step - loss: 0.2603 - val_loss: 0.9693\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.757 - ETA: 0s - loss: 0.227 - 0s 864us/step - loss: 0.2550 - val_loss: 1.0182\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.224 - 0s 826us/step - loss: 0.2485 - val_loss: 1.0596\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.265 - 0s 845us/step - loss: 0.2415 - val_loss: 1.0086\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.460 - ETA: 0s - loss: 0.231 - 0s 817us/step - loss: 0.2465 - val_loss: 1.1932\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.275 - 0s 855us/step - loss: 0.2490 - val_loss: 1.0525\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.172 - 0s 826us/step - loss: 0.2378 - val_loss: 1.0230\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.283 - 0s 827us/step - loss: 0.2362 - val_loss: 1.0130\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.215 - 0s 864us/step - loss: 0.2254 - val_loss: 1.0986\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 19\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.49 - ETA: 0s - loss: 0.5495 - 1s 8ms/step - loss: 0.6037 - val_loss: 0.5580\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.447 - 0s 845us/step - loss: 0.4405 - val_loss: 1.1631\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.416 - 0s 950us/step - loss: 0.4119 - val_loss: 1.1713\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.789 - ETA: 0s - loss: 0.410 - 0s 826us/step - loss: 0.3855 - val_loss: 1.1403\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.401 - 0s 845us/step - loss: 0.3642 - val_loss: 1.1025\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.429 - ETA: 0s - loss: 0.344 - 0s 865us/step - loss: 0.3430 - val_loss: 1.3715\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.334 - 0s 874us/step - loss: 0.3398 - val_loss: 1.3369\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.327 - 0s 855us/step - loss: 0.3134 - val_loss: 1.4235\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.730 - ETA: 0s - loss: 0.332 - 0s 836us/step - loss: 0.2958 - val_loss: 1.4366\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.283 - 0s 883us/step - loss: 0.2901 - val_loss: 1.4030\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.501 - ETA: 0s - loss: 0.255 - 0s 826us/step - loss: 0.2761 - val_loss: 1.3475\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.229 - 0s 807us/step - loss: 0.2772 - val_loss: 1.4200\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.292 - 0s 1ms/step - loss: 0.2634 - val_loss: 1.9032\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.271 - 0s 855us/step - loss: 0.2659 - val_loss: 1.4665\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.240 - 0s 874us/step - loss: 0.2523 - val_loss: 1.6543\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.222 - 0s 893us/step - loss: 0.2432 - val_loss: 1.7016\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.242 - 0s 874us/step - loss: 0.2506 - val_loss: 1.6456\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.273 - 0s 883us/step - loss: 0.2574 - val_loss: 1.5730\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.185 - 0s 865us/step - loss: 0.2268 - val_loss: 1.6686\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.246 - 0s 845us/step - loss: 0.2409 - val_loss: 1.8982\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.216 - 0s 931us/step - loss: 0.2327 - val_loss: 1.5715\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.206 - 0s 874us/step - loss: 0.2228 - val_loss: 1.4419\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.227 - 0s 959us/step - loss: 0.2256 - val_loss: 1.8504\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.217 - 0s 836us/step - loss: 0.2187 - val_loss: 1.5600\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.191 - 0s 874us/step - loss: 0.2080 - val_loss: 1.9356\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.229 - 0s 798us/step - loss: 0.2163 - val_loss: 1.5912\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.332 - ETA: 0s - loss: 0.180 - 0s 836us/step - loss: 0.2153 - val_loss: 1.6832\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.196 - 0s 817us/step - loss: 0.2189 - val_loss: 1.7051\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.204 - 0s 3ms/step - loss: 0.2096 - val_loss: 1.7457\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.211 - 0s 722us/step - loss: 0.2014 - val_loss: 1.6478\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.207 - 0s 779us/step - loss: 0.2051 - val_loss: 1.4135\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 20\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.78 - ETA: 0s - loss: 1.0399 - 1s 8ms/step - loss: 0.7923 - val_loss: 0.3643\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.470 - 0s 817us/step - loss: 0.6098 - val_loss: 0.5676\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.354 - ETA: 0s - loss: 0.574 - 0s 921us/step - loss: 0.5763 - val_loss: 0.6238\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.607 - 0s 807us/step - loss: 0.5231 - val_loss: 0.5245\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.479 - 0s 835us/step - loss: 0.4913 - val_loss: 0.6283\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.441 - 0s 836us/step - loss: 0.4618 - val_loss: 0.5710\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.734 - ETA: 0s - loss: 0.496 - 0s 807us/step - loss: 0.4505 - val_loss: 0.6674\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.419 - 0s 883us/step - loss: 0.4299 - val_loss: 0.7006\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.365 - ETA: 0s - loss: 0.449 - 0s 902us/step - loss: 0.4188 - val_loss: 0.6675\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.015 - ETA: 0s - loss: 0.447 - 0s 808us/step - loss: 0.4161 - val_loss: 0.7012\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.389 - 0s 836us/step - loss: 0.4008 - val_loss: 0.8473\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.420 - 0s 798us/step - loss: 0.3895 - val_loss: 0.8692\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.659 - ETA: 0s - loss: 0.379 - 0s 817us/step - loss: 0.3785 - val_loss: 0.8150\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.427 - 0s 874us/step - loss: 0.3819 - val_loss: 0.8430\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.344 - 0s 883us/step - loss: 0.3694 - val_loss: 0.8881\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.340 - 0s 846us/step - loss: 0.3637 - val_loss: 0.9705\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.338 - 0s 836us/step - loss: 0.3477 - val_loss: 1.0262\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.473 - ETA: 0s - loss: 0.292 - 0s 845us/step - loss: 0.3357 - val_loss: 1.0340\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.518 - ETA: 0s - loss: 0.337 - 0s 921us/step - loss: 0.3499 - val_loss: 1.0358\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.328 - 0s 874us/step - loss: 0.3404 - val_loss: 1.1525\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.402 - 0s 884us/step - loss: 0.3353 - val_loss: 1.1484\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.287 - 0s 798us/step - loss: 0.3226 - val_loss: 1.2095\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.343 - 0s 826us/step - loss: 0.3163 - val_loss: 1.2658\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.317 - 0s 855us/step - loss: 0.3032 - val_loss: 1.2461\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.319 - 0s 864us/step - loss: 0.3202 - val_loss: 1.2946\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.288 - 0s 883us/step - loss: 0.3139 - val_loss: 1.2733\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.290 - 0s 864us/step - loss: 0.3022 - val_loss: 1.3177\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.371 - 0s 855us/step - loss: 0.3269 - val_loss: 1.5753\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.383 - ETA: 0s - loss: 0.290 - 0s 845us/step - loss: 0.2936 - val_loss: 1.4656\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.326 - 0s 874us/step - loss: 0.2937 - val_loss: 1.5546\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.297 - 0s 2ms/step - loss: 0.2659 - val_loss: 1.4466\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 21\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.02 - ETA: 0s - loss: 1.0401 - 1s 8ms/step - loss: 0.9115 - val_loss: 0.3103\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.187 - ETA: 0s - loss: 0.650 - 0s 855us/step - loss: 0.6448 - val_loss: 0.6708\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.612 - 0s 902us/step - loss: 0.6033 - val_loss: 0.8213\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.560 - ETA: 0s - loss: 0.614 - 0s 874us/step - loss: 0.5920 - val_loss: 0.8014\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.592 - 0s 845us/step - loss: 0.5812 - val_loss: 0.7849\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.591 - 0s 883us/step - loss: 0.5539 - val_loss: 0.6968\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.331 - ETA: 0s - loss: 0.568 - 0s 836us/step - loss: 0.5503 - val_loss: 0.8597\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.515 - 0s 817us/step - loss: 0.5483 - val_loss: 0.8500\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.506 - 0s 874us/step - loss: 0.5190 - val_loss: 0.8101\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.547 - 0s 883us/step - loss: 0.5124 - val_loss: 0.8406\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.511 - 0s 874us/step - loss: 0.5063 - val_loss: 0.9139\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.464 - ETA: 0s - loss: 0.465 - 0s 1ms/step - loss: 0.4955 - val_loss: 0.9337\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.105 - ETA: 0s - loss: 0.553 - 0s 864us/step - loss: 0.4820 - val_loss: 0.8677\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.496 - 0s 826us/step - loss: 0.4688 - val_loss: 0.9267\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.517 - 0s 864us/step - loss: 0.4904 - val_loss: 1.0724\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.189 - ETA: 0s - loss: 0.442 - 0s 826us/step - loss: 0.4660 - val_loss: 0.7099\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.417 - 0s 836us/step - loss: 0.4684 - val_loss: 0.9662\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.509 - 0s 845us/step - loss: 0.4657 - val_loss: 0.8671\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.411 - 0s 779us/step - loss: 0.4738 - val_loss: 0.8325\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.529 - ETA: 0s - loss: 0.504 - 0s 779us/step - loss: 0.4545 - val_loss: 0.7847\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.435 - 0s 817us/step - loss: 0.4486 - val_loss: 0.7249\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.399 - 0s 760us/step - loss: 0.4418 - val_loss: 0.9122\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.312 - ETA: 0s - loss: 0.436 - 0s 769us/step - loss: 0.4319 - val_loss: 0.7372\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.347 - ETA: 0s - loss: 0.406 - 0s 893us/step - loss: 0.4222 - val_loss: 0.7508\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.441 - 0s 883us/step - loss: 0.4171 - val_loss: 0.8772\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.455 - ETA: 0s - loss: 0.346 - 0s 760us/step - loss: 0.4029 - val_loss: 0.8961\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.547 - ETA: 0s - loss: 0.363 - 0s 760us/step - loss: 0.4068 - val_loss: 0.8462\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.828 - ETA: 0s - loss: 0.471 - 0s 788us/step - loss: 0.4070 - val_loss: 0.6449\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.435 - 0s 978us/step - loss: 0.3766 - val_loss: 0.6756\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - ETA: 0s - loss: 0.385 - 0s 817us/step - loss: 0.3802 - val_loss: 0.8908\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.470 - ETA: 0s - loss: 0.399 - 0s 817us/step - loss: 0.3701 - val_loss: 0.7374\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 22\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.26 - ETA: 0s - loss: 0.9532 - 1s 8ms/step - loss: 0.7077 - val_loss: 0.4206\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.261 - 0s 817us/step - loss: 0.6260 - val_loss: 0.5527\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.804 - 0s 836us/step - loss: 0.6207 - val_loss: 0.7731\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.691 - 0s 845us/step - loss: 0.5755 - val_loss: 0.7870\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.891 - ETA: 0s - loss: 0.682 - 0s 836us/step - loss: 0.5875 - val_loss: 0.7155\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.719 - 0s 912us/step - loss: 0.5347 - val_loss: 0.6651\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.551 - 0s 864us/step - loss: 0.5758 - val_loss: 0.6758\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.608 - 0s 836us/step - loss: 0.5130 - val_loss: 0.8449\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.252 - 0s 864us/step - loss: 0.5097 - val_loss: 0.8126\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.482 - 0s 903us/step - loss: 0.4976 - val_loss: 1.1333\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.611 - 0s 969us/step - loss: 0.4697 - val_loss: 1.1160\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.479 - 0s 912us/step - loss: 0.4545 - val_loss: 1.0581\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.465 - 0s 912us/step - loss: 0.4469 - val_loss: 1.0087\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.505 - 0s 893us/step - loss: 0.4588 - val_loss: 1.0625\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.332 - 0s 845us/step - loss: 0.4484 - val_loss: 1.0675\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.461 - 0s 883us/step - loss: 0.4170 - val_loss: 1.3531\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.473 - 0s 940us/step - loss: 0.4205 - val_loss: 1.3259\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.673 - ETA: 0s - loss: 0.415 - 0s 864us/step - loss: 0.4116 - val_loss: 1.3653\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.339 - ETA: 0s - loss: 0.462 - 0s 817us/step - loss: 0.4040 - val_loss: 1.3613\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.448 - 0s 836us/step - loss: 0.3918 - val_loss: 1.2559\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.408 - 0s 845us/step - loss: 0.3959 - val_loss: 1.4598\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.237 - 0s 798us/step - loss: 0.3575 - val_loss: 1.4871\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.594 - ETA: 0s - loss: 0.321 - 0s 845us/step - loss: 0.3457 - val_loss: 1.7411\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.274 - 0s 836us/step - loss: 0.3458 - val_loss: 1.4552\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.373 - 0s 845us/step - loss: 0.3540 - val_loss: 1.6004\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.597 - ETA: 0s - loss: 0.379 - 0s 893us/step - loss: 0.3665 - val_loss: 1.6945\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.279 - 0s 836us/step - loss: 0.3097 - val_loss: 1.4496\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.110 - ETA: 0s - loss: 0.345 - 0s 817us/step - loss: 0.3242 - val_loss: 1.6208\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.240 - 0s 874us/step - loss: 0.3025 - val_loss: 1.7581\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.278 - 0s 836us/step - loss: 0.2966 - val_loss: 1.7934\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.319 - 0s 874us/step - loss: 0.3558 - val_loss: 1.6887\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 23\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.01 - ETA: 0s - loss: 0.8001 - 1s 8ms/step - loss: 0.7953 - val_loss: 0.4148\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.234 - ETA: 0s - loss: 0.502 - 0s 807us/step - loss: 0.5664 - val_loss: 0.8343\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.534 - 0s 817us/step - loss: 0.4854 - val_loss: 1.0264\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.470 - 0s 788us/step - loss: 0.4180 - val_loss: 1.3155\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.217 - 0s 788us/step - loss: 0.3733 - val_loss: 1.4031\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.396 - 0s 817us/step - loss: 0.3270 - val_loss: 1.6177\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.166 - 0s 1ms/step - loss: 0.3076 - val_loss: 1.5293\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.333 - 0s 817us/step - loss: 0.2818 - val_loss: 1.6412\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.250 - 0s 864us/step - loss: 0.2509 - val_loss: 1.7312\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.271 - 0s 855us/step - loss: 0.2573 - val_loss: 1.9016\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.258 - 0s 845us/step - loss: 0.2168 - val_loss: 1.5884\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.246 - 0s 845us/step - loss: 0.2349 - val_loss: 1.8538\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.174 - 0s 807us/step - loss: 0.2287 - val_loss: 1.8612\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.228 - 0s 855us/step - loss: 0.2118 - val_loss: 2.0053\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.256 - 0s 855us/step - loss: 0.2347 - val_loss: 2.2902\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.228 - 0s 769us/step - loss: 0.2088 - val_loss: 1.8407\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.635 - ETA: 0s - loss: 0.255 - 0s 921us/step - loss: 0.2083 - val_loss: 2.0006\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.563 - ETA: 0s - loss: 0.220 - 0s 893us/step - loss: 0.1847 - val_loss: 2.5067\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.197 - 0s 817us/step - loss: 0.2018 - val_loss: 2.2592\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.158 - 0s 836us/step - loss: 0.2002 - val_loss: 2.0640\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.179 - 0s 817us/step - loss: 0.1756 - val_loss: 2.6018\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.181 - 0s 893us/step - loss: 0.1699 - val_loss: 2.2491\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.181 - 0s 931us/step - loss: 0.1721 - val_loss: 2.2864\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.148 - 0s 864us/step - loss: 0.1675 - val_loss: 2.2108\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.153 - 0s 883us/step - loss: 0.1682 - val_loss: 2.1831\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.156 - 0s 865us/step - loss: 0.1540 - val_loss: 2.4293\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.162 - 0s 941us/step - loss: 0.1520 - val_loss: 2.4881\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.136 - 0s 940us/step - loss: 0.1379 - val_loss: 2.3798\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.153 - 0s 874us/step - loss: 0.1424 - val_loss: 2.4814\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.143 - 0s 836us/step - loss: 0.1394 - val_loss: 2.9260\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.137 - 0s 874us/step - loss: 0.1351 - val_loss: 2.3839\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 24\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.38 - ETA: 0s - loss: 0.7053 - 1s 8ms/step - loss: 0.7950 - val_loss: 0.3157\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.285 - 0s 893us/step - loss: 0.5550 - val_loss: 0.6811\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.567 - 0s 883us/step - loss: 0.5255 - val_loss: 1.0685\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.497 - 0s 836us/step - loss: 0.4810 - val_loss: 0.9906\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.526 - 0s 827us/step - loss: 0.4498 - val_loss: 0.9222\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.414 - 0s 836us/step - loss: 0.4182 - val_loss: 0.8405\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.766 - ETA: 0s - loss: 0.451 - 0s 921us/step - loss: 0.3957 - val_loss: 0.9477\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.326 - 0s 988us/step - loss: 0.3601 - val_loss: 0.9985\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.353 - 0s 902us/step - loss: 0.3543 - val_loss: 0.9632\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.586 - ETA: 0s - loss: 0.276 - 0s 864us/step - loss: 0.3281 - val_loss: 0.7411\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.306 - 0s 912us/step - loss: 0.3279 - val_loss: 1.0485\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.283 - 0s 931us/step - loss: 0.2703 - val_loss: 1.0424\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.346 - 0s 893us/step - loss: 0.2936 - val_loss: 1.0650\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.260 - 0s 874us/step - loss: 0.2434 - val_loss: 0.9379\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.329 - 0s 978us/step - loss: 0.2707 - val_loss: 0.9737\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.224 - 0s 912us/step - loss: 0.2570 - val_loss: 1.1215\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.234 - 0s 855us/step - loss: 0.2332 - val_loss: 1.0351\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.217 - 0s 921us/step - loss: 0.2189 - val_loss: 1.0594\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.218 - 0s 903us/step - loss: 0.2168 - val_loss: 1.1183\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.181 - 0s 817us/step - loss: 0.1900 - val_loss: 1.1005\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.199 - 0s 826us/step - loss: 0.1776 - val_loss: 1.1011\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.217 - 0s 855us/step - loss: 0.1885 - val_loss: 1.3236\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.215 - 0s 826us/step - loss: 0.1905 - val_loss: 1.0812\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.156 - 0s 864us/step - loss: 0.1553 - val_loss: 1.1735\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.168 - 0s 846us/step - loss: 0.1664 - val_loss: 1.3119\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.145 - 0s 826us/step - loss: 0.1459 - val_loss: 1.3291\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.190 - 0s 836us/step - loss: 0.1701 - val_loss: 1.2760\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.140 - 0s 826us/step - loss: 0.1432 - val_loss: 1.3271\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.174 - 0s 893us/step - loss: 0.1610 - val_loss: 1.0174\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.139 - 0s 893us/step - loss: 0.1356 - val_loss: 1.2313\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.141 - 0s 826us/step - loss: 0.1397 - val_loss: 1.4393\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 25\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.18 - ETA: 0s - loss: 1.1267 - 1s 8ms/step - loss: 0.8528 - val_loss: 0.2362\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.808 - 0s 950us/step - loss: 0.6719 - val_loss: 0.4267\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.383 - ETA: 0s - loss: 0.861 - 0s 883us/step - loss: 0.5989 - val_loss: 0.5334\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.701 - 0s 893us/step - loss: 0.5476 - val_loss: 0.6043\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.480 - 0s 855us/step - loss: 0.4639 - val_loss: 0.7184\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.485 - 0s 845us/step - loss: 0.4276 - val_loss: 0.7635\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.981 - ETA: 0s - loss: 0.230 - 0s 807us/step - loss: 0.4012 - val_loss: 0.7842\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.350 - 0s 893us/step - loss: 0.3306 - val_loss: 0.8414\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.195 - 0s 1ms/step - loss: 0.2968 - val_loss: 0.9417\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.907 - ETA: 0s - loss: 0.203 - 0s 874us/step - loss: 0.2840 - val_loss: 0.9006\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.332 - 0s 864us/step - loss: 0.2586 - val_loss: 0.8761\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.212 - 0s 893us/step - loss: 0.2484 - val_loss: 0.9186\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.880 - ETA: 0s - loss: 0.279 - 0s 874us/step - loss: 0.2326 - val_loss: 0.9154\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.103 - 0s 893us/step - loss: 0.2311 - val_loss: 0.9307\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.157 - 0s 883us/step - loss: 0.2043 - val_loss: 1.1562\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.123 - 0s 874us/step - loss: 0.1951 - val_loss: 0.9864\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.142 - 0s 864us/step - loss: 0.1979 - val_loss: 1.0110\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.240 - 0s 940us/step - loss: 0.1896 - val_loss: 0.9879\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.145 - 0s 912us/step - loss: 0.2017 - val_loss: 0.9242\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.128 - 0s 883us/step - loss: 0.1802 - val_loss: 1.0064\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.556 - ETA: 0s - loss: 0.212 - 0s 826us/step - loss: 0.1872 - val_loss: 1.1307\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.225 - 0s 864us/step - loss: 0.1825 - val_loss: 1.0243\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.104 - 0s 864us/step - loss: 0.1728 - val_loss: 1.1175\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.122 - 0s 883us/step - loss: 0.1738 - val_loss: 0.9901\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.147 - 0s 874us/step - loss: 0.1596 - val_loss: 1.0959\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.151 - 0s 883us/step - loss: 0.1657 - val_loss: 1.1705\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.170 - 0s 864us/step - loss: 0.1587 - val_loss: 0.9343\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.138 - 0s 912us/step - loss: 0.1650 - val_loss: 1.2977\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.119 - 0s 931us/step - loss: 0.1472 - val_loss: 1.1168\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.154 - 0s 874us/step - loss: 0.1447 - val_loss: 1.1057\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.121 - ETA: 0s - loss: 0.163 - 0s 931us/step - loss: 0.1561 - val_loss: 1.0083\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 26\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.72 - ETA: 0s - loss: 0.8337 - 1s 8ms/step - loss: 0.7106 - val_loss: 0.4900\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.481 - 0s 864us/step - loss: 0.4843 - val_loss: 1.1040\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.952 - ETA: 0s - loss: 0.364 - 0s 855us/step - loss: 0.4201 - val_loss: 1.0785\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.917 - ETA: 0s - loss: 0.354 - 0s 817us/step - loss: 0.3902 - val_loss: 1.1815\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.353 - 0s 798us/step - loss: 0.3744 - val_loss: 1.6096\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.386 - 0s 798us/step - loss: 0.3686 - val_loss: 1.3630\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.379 - 0s 788us/step - loss: 0.3610 - val_loss: 1.6312\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.793 - ETA: 0s - loss: 0.327 - 0s 826us/step - loss: 0.3449 - val_loss: 1.5023\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.228 - 0s 997us/step - loss: 0.3242 - val_loss: 1.6506\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.346 - 0s 779us/step - loss: 0.3433 - val_loss: 1.4724\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.489 - ETA: 0s - loss: 0.252 - 0s 798us/step - loss: 0.3188 - val_loss: 1.8463\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.285 - 0s 769us/step - loss: 0.3144 - val_loss: 2.0231\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.287 - 0s 798us/step - loss: 0.3238 - val_loss: 1.8845\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.272 - 0s 807us/step - loss: 0.3052 - val_loss: 1.9579\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.284 - 0s 770us/step - loss: 0.3038 - val_loss: 2.2236\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.241 - 0s 779us/step - loss: 0.3016 - val_loss: 2.0935\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.086 - ETA: 0s - loss: 0.309 - 0s 807us/step - loss: 0.3096 - val_loss: 2.4183\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.298 - 0s 788us/step - loss: 0.2860 - val_loss: 2.0969\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.388 - ETA: 0s - loss: 0.272 - 0s 845us/step - loss: 0.2873 - val_loss: 2.5292\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.262 - 0s 798us/step - loss: 0.2709 - val_loss: 2.1353\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.265 - 0s 798us/step - loss: 0.2740 - val_loss: 2.6178\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.267 - 0s 836us/step - loss: 0.2627 - val_loss: 2.6117\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.239 - 0s 817us/step - loss: 0.2481 - val_loss: 2.4000\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.237 - 0s 826us/step - loss: 0.2447 - val_loss: 2.5724\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.270 - 0s 883us/step - loss: 0.2539 - val_loss: 2.5790\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.230 - 0s 836us/step - loss: 0.2333 - val_loss: 2.8959\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.247 - 0s 798us/step - loss: 0.2340 - val_loss: 2.8682\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.353 - ETA: 0s - loss: 0.220 - 0s 827us/step - loss: 0.2316 - val_loss: 2.7993\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.217 - 0s 807us/step - loss: 0.2320 - val_loss: 2.7433\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.215 - 0s 864us/step - loss: 0.2304 - val_loss: 3.0102\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.206 - 0s 864us/step - loss: 0.2274 - val_loss: 2.8428\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 27\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.67 - ETA: 0s - loss: 0.9391 - 1s 8ms/step - loss: 0.8377 - val_loss: 0.3334\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.683 - ETA: 0s - loss: 0.611 - 0s 846us/step - loss: 0.6273 - val_loss: 0.5508\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.125 - ETA: 0s - loss: 0.606 - 0s 883us/step - loss: 0.5847 - val_loss: 0.5771\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.476 - 0s 921us/step - loss: 0.5613 - val_loss: 0.7010\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.520 - 0s 893us/step - loss: 0.5041 - val_loss: 0.6296\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.512 - 0s 874us/step - loss: 0.4954 - val_loss: 0.5950\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.172 - ETA: 0s - loss: 0.457 - 0s 902us/step - loss: 0.4610 - val_loss: 0.6395\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.515 - 0s 846us/step - loss: 0.4585 - val_loss: 0.6038\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.422 - ETA: 0s - loss: 0.368 - 0s 893us/step - loss: 0.4317 - val_loss: 0.5978\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.441 - 0s 1ms/step - loss: 0.4087 - val_loss: 0.6634\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.781 - ETA: 0s - loss: 0.479 - 0s 836us/step - loss: 0.4243 - val_loss: 0.5872\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.462 - 0s 865us/step - loss: 0.4083 - val_loss: 0.6319\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.369 - 0s 836us/step - loss: 0.3804 - val_loss: 0.6436\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.355 - 0s 798us/step - loss: 0.3826 - val_loss: 0.5344\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.617 - ETA: 0s - loss: 0.367 - 0s 760us/step - loss: 0.3761 - val_loss: 0.6752\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.083 - ETA: 0s - loss: 0.396 - 0s 788us/step - loss: 0.3556 - val_loss: 0.5836\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.555 - ETA: 0s - loss: 0.393 - 0s 798us/step - loss: 0.3611 - val_loss: 0.5813\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.832 - ETA: 0s - loss: 0.336 - 0s 751us/step - loss: 0.3392 - val_loss: 0.6127\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.371 - 0s 741us/step - loss: 0.3309 - val_loss: 0.6500\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.337 - 0s 779us/step - loss: 0.3247 - val_loss: 0.5587\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.247 - 0s 779us/step - loss: 0.3284 - val_loss: 0.4662\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.348 - 0s 817us/step - loss: 0.3114 - val_loss: 0.5871\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.273 - 0s 1ms/step - loss: 0.3066 - val_loss: 0.5874\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.286 - 0s 874us/step - loss: 0.2929 - val_loss: 0.5839\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.585 - ETA: 0s - loss: 0.312 - 0s 855us/step - loss: 0.2984 - val_loss: 0.5854\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.292 - 0s 865us/step - loss: 0.2865 - val_loss: 0.6960\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.426 - ETA: 0s - loss: 0.275 - 0s 874us/step - loss: 0.2682 - val_loss: 0.5745\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.269 - 0s 798us/step - loss: 0.2830 - val_loss: 0.6543\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.585 - ETA: 0s - loss: 0.262 - 0s 827us/step - loss: 0.2602 - val_loss: 0.6446\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.283 - 0s 826us/step - loss: 0.2542 - val_loss: 0.6370\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.277 - 0s 865us/step - loss: 0.2572 - val_loss: 0.6933\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 28\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 0.37 - ETA: 0s - loss: 0.7899 - 1s 8ms/step - loss: 0.7872 - val_loss: 0.4676\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.759 - ETA: 0s - loss: 0.430 - 0s 845us/step - loss: 0.4830 - val_loss: 1.0369\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.453 - 0s 817us/step - loss: 0.4247 - val_loss: 1.3584\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.393 - ETA: 0s - loss: 0.475 - 0s 912us/step - loss: 0.3860 - val_loss: 1.3723\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.355 - 0s 978us/step - loss: 0.3519 - val_loss: 1.4624\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.724 - ETA: 0s - loss: 0.350 - 0s 864us/step - loss: 0.3418 - val_loss: 1.6566\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.287 - 0s 902us/step - loss: 0.2773 - val_loss: 1.4471\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.241 - 0s 874us/step - loss: 0.2598 - val_loss: 1.7692\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.268 - 0s 864us/step - loss: 0.2575 - val_loss: 1.9135\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.232 - 0s 864us/step - loss: 0.2234 - val_loss: 1.7922\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.205 - 0s 874us/step - loss: 0.2174 - val_loss: 2.2318\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.205 - 0s 931us/step - loss: 0.2116 - val_loss: 1.9996\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.203 - 0s 855us/step - loss: 0.2062 - val_loss: 2.1085\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.179 - 0s 921us/step - loss: 0.1985 - val_loss: 2.2882\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.187 - 0s 921us/step - loss: 0.2084 - val_loss: 2.3455\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.195 - 0s 874us/step - loss: 0.1844 - val_loss: 2.0410\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.149 - 0s 902us/step - loss: 0.1769 - val_loss: 2.2576\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.180 - 0s 864us/step - loss: 0.1808 - val_loss: 2.5566\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.472 - ETA: 0s - loss: 0.201 - 0s 922us/step - loss: 0.1936 - val_loss: 2.6186\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.187 - 0s 1ms/step - loss: 0.1718 - val_loss: 2.4073\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.159 - 0s 864us/step - loss: 0.1591 - val_loss: 2.2578\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.185 - 0s 807us/step - loss: 0.1760 - val_loss: 2.5115\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.163 - 0s 883us/step - loss: 0.1614 - val_loss: 2.1210\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.408 - ETA: 0s - loss: 0.184 - 0s 940us/step - loss: 0.1873 - val_loss: 2.3339\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.149 - 0s 864us/step - loss: 0.1567 - val_loss: 2.1278\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.155 - 0s 817us/step - loss: 0.1532 - val_loss: 2.2034\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.146 - 0s 874us/step - loss: 0.1575 - val_loss: 2.6303\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.148 - 0s 865us/step - loss: 0.1626 - val_loss: 2.5285\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.150 - 0s 912us/step - loss: 0.1548 - val_loss: 2.3658\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.153 - 0s 941us/step - loss: 0.1535 - val_loss: 2.7682\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.153 - 0s 893us/step - loss: 0.1467 - val_loss: 2.2921\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 29\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.50 - ETA: 1s - loss: 1.0427 - ETA: 0s - loss: 0.842 - 1s 9ms/step - loss: 0.7742 - val_loss: 0.4513\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.480 - 0s 760us/step - loss: 0.4601 - val_loss: 1.1792\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.371 - 0s 864us/step - loss: 0.4035 - val_loss: 1.1466\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.356 - 0s 826us/step - loss: 0.3829 - val_loss: 1.1847\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.818 - ETA: 0s - loss: 0.342 - 0s 798us/step - loss: 0.3298 - val_loss: 1.2735\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.291 - 0s 874us/step - loss: 0.3327 - val_loss: 1.3070\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.618 - ETA: 0s - loss: 0.264 - 0s 855us/step - loss: 0.2984 - val_loss: 1.2385\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.850 - ETA: 0s - loss: 0.285 - 0s 836us/step - loss: 0.3048 - val_loss: 1.1479\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.249 - 0s 769us/step - loss: 0.2779 - val_loss: 1.4046\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.356 - ETA: 0s - loss: 0.263 - 0s 845us/step - loss: 0.2703 - val_loss: 1.2431\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.266 - 0s 855us/step - loss: 0.2625 - val_loss: 1.3292\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.231 - 0s 845us/step - loss: 0.2564 - val_loss: 1.3522\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.210 - 0s 807us/step - loss: 0.2535 - val_loss: 1.2961\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.278 - 0s 865us/step - loss: 0.2424 - val_loss: 1.5598\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.232 - 0s 836us/step - loss: 0.2401 - val_loss: 1.3631\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.261 - 0s 779us/step - loss: 0.2460 - val_loss: 1.3320\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.238 - 0s 874us/step - loss: 0.2401 - val_loss: 1.5033\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.254 - 0s 788us/step - loss: 0.2369 - val_loss: 1.3749\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.242 - 0s 846us/step - loss: 0.2222 - val_loss: 1.5251\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.230 - 0s 798us/step - loss: 0.2136 - val_loss: 1.4852\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.186 - 0s 893us/step - loss: 0.2025 - val_loss: 1.6053\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.214 - 0s 940us/step - loss: 0.2082 - val_loss: 1.5095\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.202 - 0s 845us/step - loss: 0.2144 - val_loss: 1.4909\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.195 - 0s 874us/step - loss: 0.1989 - val_loss: 1.6155\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.223 - 0s 817us/step - loss: 0.1914 - val_loss: 1.5508\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.356 - ETA: 0s - loss: 0.211 - 0s 1ms/step - loss: 0.1843 - val_loss: 1.5502\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.184 - 0s 808us/step - loss: 0.2078 - val_loss: 1.5530\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.225 - 0s 864us/step - loss: 0.1922 - val_loss: 1.3034\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.181 - 0s 836us/step - loss: 0.1889 - val_loss: 1.8265\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.212 - 0s 864us/step - loss: 0.1783 - val_loss: 1.6942\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.227 - 0s 845us/step - loss: 0.1953 - val_loss: 1.7158\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 30\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.56 - ETA: 0s - loss: 0.8378 - 1s 8ms/step - loss: 0.7656 - val_loss: 0.3660\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.457 - 0s 855us/step - loss: 0.4489 - val_loss: 0.8712\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.380 - 0s 874us/step - loss: 0.4043 - val_loss: 0.7757\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.358 - 0s 874us/step - loss: 0.3664 - val_loss: 0.7736\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.273 - 0s 950us/step - loss: 0.3314 - val_loss: 0.6707\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.268 - 0s 922us/step - loss: 0.3094 - val_loss: 0.6734\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.335 - ETA: 0s - loss: 0.220 - 0s 902us/step - loss: 0.2832 - val_loss: 0.6755\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.293 - 0s 893us/step - loss: 0.2743 - val_loss: 0.6973\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.214 - 0s 883us/step - loss: 0.2462 - val_loss: 0.8278\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.284 - 0s 826us/step - loss: 0.2467 - val_loss: 0.7993\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.229 - 0s 874us/step - loss: 0.2515 - val_loss: 0.8263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.197 - 0s 779us/step - loss: 0.2452 - val_loss: 0.8191\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.212 - 0s 883us/step - loss: 0.2388 - val_loss: 0.8749\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.881 - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.236 - 0s 1ms/step - loss: 0.2234 - val_loss: 0.9436\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.240 - 0s 1ms/step - loss: 0.2289 - val_loss: 0.9219\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.234 - 0s 950us/step - loss: 0.2226 - val_loss: 1.0531\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.231 - 0s 788us/step - loss: 0.2146 - val_loss: 1.0762\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.837 - ETA: 0s - loss: 0.228 - 0s 798us/step - loss: 0.2123 - val_loss: 1.1046\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.589 - ETA: 0s - loss: 0.224 - 0s 807us/step - loss: 0.1939 - val_loss: 1.1615\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.222 - 0s 817us/step - loss: 0.2008 - val_loss: 1.0994\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.171 - 0s 760us/step - loss: 0.2004 - val_loss: 1.2040\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.227 - 0s 836us/step - loss: 0.1874 - val_loss: 1.1530\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.214 - 0s 798us/step - loss: 0.1939 - val_loss: 1.2414\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.181 - 0s 769us/step - loss: 0.1810 - val_loss: 1.1311\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.191 - 0s 798us/step - loss: 0.1730 - val_loss: 1.2028\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.178 - 0s 921us/step - loss: 0.1754 - val_loss: 1.2233\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.188 - 0s 836us/step - loss: 0.1828 - val_loss: 1.2294\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.164 - 0s 798us/step - loss: 0.1705 - val_loss: 1.2311\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.155 - 0s 826us/step - loss: 0.1652 - val_loss: 1.3766\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.156 - 0s 855us/step - loss: 0.1696 - val_loss: 1.2623\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.708 - ETA: 0s - loss: 0.176 - 0s 817us/step - loss: 0.1763 - val_loss: 1.3212\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 31\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.40 - ETA: 0s - loss: 0.8252 - 1s 8ms/step - loss: 0.6363 - val_loss: 0.6192\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.443 - 0s 874us/step - loss: 0.4714 - val_loss: 0.8162\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.447 - ETA: 0s - loss: 0.466 - 0s 855us/step - loss: 0.4379 - val_loss: 0.9008\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.383 - ETA: 0s - loss: 0.423 - 0s 845us/step - loss: 0.4208 - val_loss: 0.8257\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.436 - 0s 845us/step - loss: 0.3819 - val_loss: 0.6492\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.233 - 0s 1ms/step - loss: 0.3528 - val_loss: 0.6436\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.381 - 0s 912us/step - loss: 0.3536 - val_loss: 0.7720\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.397 - ETA: 0s - loss: 0.313 - 0s 883us/step - loss: 0.3496 - val_loss: 0.5892\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.637 - ETA: 0s - loss: 0.371 - 0s 969us/step - loss: 0.3056 - val_loss: 0.5755\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.075 - ETA: 0s - loss: 0.341 - 0s 864us/step - loss: 0.3232 - val_loss: 0.5603\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.213 - 0s 874us/step - loss: 0.2902 - val_loss: 0.5754\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.330 - 0s 931us/step - loss: 0.2982 - val_loss: 0.5696\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.339 - 0s 864us/step - loss: 0.2882 - val_loss: 0.4870\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.443 - ETA: 0s - loss: 0.247 - 0s 893us/step - loss: 0.2844 - val_loss: 0.5417\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.465 - ETA: 0s - loss: 0.281 - 0s 855us/step - loss: 0.2895 - val_loss: 0.5137\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.309 - 0s 950us/step - loss: 0.2748 - val_loss: 0.5225\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.291 - 0s 997us/step - loss: 0.2841 - val_loss: 0.4826\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.202 - 0s 893us/step - loss: 0.2405 - val_loss: 0.4657\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.274 - 0s 959us/step - loss: 0.2412 - val_loss: 0.4812\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.232 - 0s 931us/step - loss: 0.2354 - val_loss: 0.4502\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.175 - 0s 864us/step - loss: 0.2206 - val_loss: 0.4779\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.175 - 0s 836us/step - loss: 0.2327 - val_loss: 0.4147\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.248 - 0s 1ms/step - loss: 0.2466 - val_loss: 0.4361\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.188 - 0s 922us/step - loss: 0.2306 - val_loss: 0.4213\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.208 - 0s 997us/step - loss: 0.2099 - val_loss: 0.4988\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.262 - 0s 931us/step - loss: 0.2195 - val_loss: 0.5196\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.168 - 0s 902us/step - loss: 0.2231 - val_loss: 0.4502\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.185 - 0s 921us/step - loss: 0.2060 - val_loss: 0.4258\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.201 - 0s 912us/step - loss: 0.2016 - val_loss: 0.3721\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.339 - ETA: 0s - loss: 0.237 - 0s 845us/step - loss: 0.2077 - val_loss: 0.4689\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.157 - 0s 826us/step - loss: 0.1779 - val_loss: 0.4494\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.164 - 0s 836us/step - loss: 0.1947 - val_loss: 0.4449\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.222 - 0s 883us/step - loss: 0.1936 - val_loss: 0.3901\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.175 - 0s 903us/step - loss: 0.1885 - val_loss: 0.4950\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.180 - 0s 1ms/step - loss: 0.1818 - val_loss: 0.4249\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.187 - 0s 864us/step - loss: 0.1801 - val_loss: 0.3759\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.161 - 0s 826us/step - loss: 0.1715 - val_loss: 0.3825\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.163 - 0s 855us/step - loss: 0.1805 - val_loss: 0.4341\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.147 - 0s 893us/step - loss: 0.1706 - val_loss: 0.3769\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.156 - 0s 845us/step - loss: 0.1625 - val_loss: 0.3981\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.144 - 0s 855us/step - loss: 0.1400 - val_loss: 0.4261\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.152 - 0s 826us/step - loss: 0.1526 - val_loss: 0.3995\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.145 - 0s 864us/step - loss: 0.1566 - val_loss: 0.4172\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.165 - 0s 855us/step - loss: 0.1536 - val_loss: 0.5854\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.127 - 0s 883us/step - loss: 0.1541 - val_loss: 0.4748\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.153 - 0s 826us/step - loss: 0.1449 - val_loss: 0.4842\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.154 - 0s 893us/step - loss: 0.1395 - val_loss: 0.4935\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.119 - 0s 883us/step - loss: 0.1336 - val_loss: 0.3850\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.120 - 0s 922us/step - loss: 0.1354 - val_loss: 0.4909\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.118 - 0s 922us/step - loss: 0.1306 - val_loss: 0.3846\n",
      "(105,) (15,)\n",
      "model compiled 32\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 1.04 - ETA: 0s - loss: 0.8821 - 1s 8ms/step - loss: 0.7320 - val_loss: 0.4263\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.553 - 0s 760us/step - loss: 0.5252 - val_loss: 0.8424\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.311 - 0s 807us/step - loss: 0.4600 - val_loss: 0.7919\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.475 - 0s 808us/step - loss: 0.4255 - val_loss: 0.6500\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.861 - ETA: 0s - loss: 0.487 - 0s 836us/step - loss: 0.4318 - val_loss: 0.6262\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.401 - 0s 883us/step - loss: 0.3846 - val_loss: 0.6443\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.352 - 0s 1ms/step - loss: 0.3430 - val_loss: 0.8351\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.438 - ETA: 0s - loss: 0.381 - 0s 940us/step - loss: 0.3308 - val_loss: 0.8744\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.333 - 0s 893us/step - loss: 0.3334 - val_loss: 0.7391\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.196 - 0s 931us/step - loss: 0.3040 - val_loss: 0.6698\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.301 - 0s 912us/step - loss: 0.2870 - val_loss: 0.7868\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.298 - 0s 931us/step - loss: 0.2963 - val_loss: 0.7691\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.209 - 0s 846us/step - loss: 0.2903 - val_loss: 0.7502\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.294 - 0s 845us/step - loss: 0.2807 - val_loss: 0.7818\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.192 - 0s 912us/step - loss: 0.2487 - val_loss: 0.7931\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.266 - 0s 855us/step - loss: 0.2462 - val_loss: 0.9293\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.223 - 0s 893us/step - loss: 0.2513 - val_loss: 0.7775\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.151 - 0s 997us/step - loss: 0.2433 - val_loss: 0.7918\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.199 - 0s 1ms/step - loss: 0.2182 - val_loss: 0.8359\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.576 - ETA: 0s - loss: 0.248 - 0s 712us/step - loss: 0.2309 - val_loss: 0.7673\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.216 - 0s 845us/step - loss: 0.2228 - val_loss: 0.7585\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.221 - 0s 2ms/step - loss: 0.2147 - val_loss: 0.7763\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.223 - 0s 912us/step - loss: 0.2217 - val_loss: 0.8678\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.152 - 0s 817us/step - loss: 0.1995 - val_loss: 0.8221\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.176 - 0s 883us/step - loss: 0.1811 - val_loss: 0.9372\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.204 - 0s 921us/step - loss: 0.1713 - val_loss: 0.9027\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.187 - 0s 960us/step - loss: 0.1785 - val_loss: 0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.165 - 0s 864us/step - loss: 0.1603 - val_loss: 0.8183\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.139 - 0s 855us/step - loss: 0.1742 - val_loss: 0.8753\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.175 - 0s 912us/step - loss: 0.1838 - val_loss: 0.9770\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.147 - 0s 807us/step - loss: 0.1802 - val_loss: 0.9608\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 33\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.13 - ETA: 0s - loss: 0.4662 - 1s 9ms/step - loss: 0.8435 - val_loss: 0.2015\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.968 - 0s 827us/step - loss: 0.7894 - val_loss: 0.2854\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.943 - 0s 817us/step - loss: 0.7378 - val_loss: 0.3148\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.856 - 0s 864us/step - loss: 0.6822 - val_loss: 0.4051\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.099 - 0s 864us/step - loss: 0.6409 - val_loss: 0.3756\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.265 - 0s 845us/step - loss: 0.5928 - val_loss: 0.8993\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.799 - 0s 1ms/step - loss: 0.5337 - val_loss: 0.9090\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.521 - 0s 845us/step - loss: 0.5162 - val_loss: 1.0366\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.086 - 0s 1ms/step - loss: 0.4536 - val_loss: 1.0145\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.201 - 0s 3ms/step - loss: 0.4666 - val_loss: 1.3278\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.479 - 0s 703us/step - loss: 0.4188 - val_loss: 1.6147\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.327 - ETA: 0s - loss: 0.451 - 0s 646us/step - loss: 0.4179 - val_loss: 1.8620\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.417 - 0s 912us/step - loss: 0.3893 - val_loss: 2.0442\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.393 - 0s 2ms/step - loss: 0.3780 - val_loss: 2.7298\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.543 - ETA: 0s - loss: 0.456 - 0s 979us/step - loss: 0.3619 - val_loss: 2.7164\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.400 - 0s 912us/step - loss: 0.3067 - val_loss: 2.8411\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.954 - ETA: 0s - loss: 0.361 - 0s 1ms/step - loss: 0.3040 - val_loss: 3.0455\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.299 - 0s 788us/step - loss: 0.2907 - val_loss: 3.1861\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.277 - 0s 1ms/step - loss: 0.3132 - val_loss: 3.0945\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.733 - ETA: 0s - loss: 0.333 - 0s 836us/step - loss: 0.3396 - val_loss: 4.1093\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.332 - 0s 807us/step - loss: 0.2814 - val_loss: 3.9125\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.277 - 0s 826us/step - loss: 0.2446 - val_loss: 4.0908\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.116 - 0s 836us/step - loss: 0.2321 - val_loss: 3.7723\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.240 - 0s 902us/step - loss: 0.2125 - val_loss: 4.1204\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.154 - 0s 921us/step - loss: 0.2393 - val_loss: 4.3926\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.235 - 0s 836us/step - loss: 0.1977 - val_loss: 4.6172\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.598 - ETA: 0s - loss: 0.234 - 0s 874us/step - loss: 0.2012 - val_loss: 4.5938\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.094 - 0s 874us/step - loss: 0.2469 - val_loss: 4.7400\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.222 - 0s 807us/step - loss: 0.1940 - val_loss: 5.0420\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.233 - 0s 836us/step - loss: 0.2023 - val_loss: 5.2686\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.368 - ETA: 0s - loss: 0.298 - 0s 817us/step - loss: 0.2293 - val_loss: 5.6127\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 34\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.27 - ETA: 0s - loss: 0.7111 - 1s 8ms/step - loss: 0.7474 - val_loss: 0.5519\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.911 - ETA: 0s - loss: 0.629 - 0s 884us/step - loss: 0.5722 - val_loss: 1.0699\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.564 - 0s 845us/step - loss: 0.5396 - val_loss: 1.1732\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.529 - 0s 855us/step - loss: 0.4872 - val_loss: 1.3764\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.818 - ETA: 0s - loss: 0.484 - 0s 922us/step - loss: 0.4954 - val_loss: 1.5527\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.435 - 0s 912us/step - loss: 0.4375 - val_loss: 1.6439\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.476 - 0s 912us/step - loss: 0.4252 - val_loss: 1.8870\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.772 - ETA: 0s - loss: 0.420 - 0s 855us/step - loss: 0.4132 - val_loss: 2.0458\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.021 - ETA: 0s - loss: 0.382 - 0s 817us/step - loss: 0.4091 - val_loss: 2.0691\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.456 - 0s 845us/step - loss: 0.4036 - val_loss: 2.2189\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.788 - ETA: 0s - loss: 0.287 - 0s 864us/step - loss: 0.3856 - val_loss: 2.1731\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.383 - 0s 770us/step - loss: 0.3728 - val_loss: 2.3811\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.598 - ETA: 0s - loss: 0.399 - 0s 798us/step - loss: 0.3712 - val_loss: 2.0883\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.358 - 0s 779us/step - loss: 0.3688 - val_loss: 2.4386\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.399 - 0s 855us/step - loss: 0.3847 - val_loss: 2.6102\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.539 - ETA: 0s - loss: 0.354 - 0s 817us/step - loss: 0.3568 - val_loss: 2.4237\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.399 - 0s 874us/step - loss: 0.3474 - val_loss: 2.7469\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.394 - 0s 836us/step - loss: 0.3394 - val_loss: 2.5117\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.338 - 0s 836us/step - loss: 0.3448 - val_loss: 2.7369\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.249 - 0s 845us/step - loss: 0.3387 - val_loss: 2.7934\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.316 - 0s 845us/step - loss: 0.3490 - val_loss: 2.9829\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.336 - 0s 807us/step - loss: 0.3326 - val_loss: 2.9279\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.340 - ETA: 0s - loss: 0.325 - 0s 807us/step - loss: 0.3496 - val_loss: 2.7066\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.286 - 0s 826us/step - loss: 0.3215 - val_loss: 2.9603\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.366 - 0s 855us/step - loss: 0.3418 - val_loss: 2.7371\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.345 - 0s 845us/step - loss: 0.3147 - val_loss: 2.8164\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.289 - 0s 836us/step - loss: 0.3069 - val_loss: 2.2740\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.229 - 0s 874us/step - loss: 0.3132 - val_loss: 3.0410\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.300 - 0s 788us/step - loss: 0.3089 - val_loss: 2.6863\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.359 - 0s 808us/step - loss: 0.3086 - val_loss: 3.0365\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.271 - 0s 788us/step - loss: 0.3026 - val_loss: 2.9312\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 35\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.49 - ETA: 0s - loss: 0.5784 - 1s 8ms/step - loss: 0.9416 - val_loss: 0.1368\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 1.056 - 0s 883us/step - loss: 0.8667 - val_loss: 0.0764\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.579 - 0s 874us/step - loss: 0.8233 - val_loss: 0.0829\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.647 - 0s 874us/step - loss: 0.7909 - val_loss: 0.0527\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.476 - 0s 864us/step - loss: 0.7563 - val_loss: 0.0875\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.880 - 0s 969us/step - loss: 0.7387 - val_loss: 0.0890\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.443 - 0s 921us/step - loss: 0.7140 - val_loss: 0.1020\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.893 - 0s 874us/step - loss: 0.7051 - val_loss: 0.1121\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 0.872 - 0s 845us/step - loss: 0.6827 - val_loss: 0.1273\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.425 - 0s 912us/step - loss: 0.6451 - val_loss: 0.1448\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.648 - 0s 893us/step - loss: 0.6341 - val_loss: 0.1504\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.590 - 0s 921us/step - loss: 0.6270 - val_loss: 0.1689\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.815 - 0s 912us/step - loss: 0.5884 - val_loss: 0.1618\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.740 - 0s 874us/step - loss: 0.5880 - val_loss: 0.1552\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.520 - 0s 921us/step - loss: 0.5692 - val_loss: 0.1516\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.511 - 0s 969us/step - loss: 0.5666 - val_loss: 0.1430\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.395 - 0s 864us/step - loss: 0.5680 - val_loss: 0.1341\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.418 - 0s 950us/step - loss: 0.5663 - val_loss: 0.1370\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.568 - ETA: 0s - loss: 0.713 - 0s 902us/step - loss: 0.5315 - val_loss: 0.1399\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.187 - ETA: 0s - loss: 0.736 - 0s 931us/step - loss: 0.5179 - val_loss: 0.1434\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.337 - 0s 883us/step - loss: 0.5501 - val_loss: 0.1201\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.686 - 0s 817us/step - loss: 0.5319 - val_loss: 0.1421\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.435 - ETA: 0s - loss: 0.669 - 0s 969us/step - loss: 0.5050 - val_loss: 0.1361\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.666 - ETA: 0s - loss: 0.368 - 0s 874us/step - loss: 0.5021 - val_loss: 0.1176\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.135 - 0s 931us/step - loss: 0.5040 - val_loss: 0.1223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.584 - ETA: 0s - loss: 0.559 - 0s 1ms/step - loss: 0.4864 - val_loss: 0.1297\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.574 - 0s 836us/step - loss: 0.4553 - val_loss: 0.1279\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.514 - 0s 769us/step - loss: 0.4522 - val_loss: 0.1307\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.141 - 0s 883us/step - loss: 0.4430 - val_loss: 0.1272\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.171 - 0s 865us/step - loss: 0.4359 - val_loss: 0.1271\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.174 - 0s 817us/step - loss: 0.4409 - val_loss: 0.1304\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.370 - 0s 883us/step - loss: 0.4239 - val_loss: 0.1460\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.382 - 0s 874us/step - loss: 0.4139 - val_loss: 0.1431\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.517 - 0s 921us/step - loss: 0.4075 - val_loss: 0.1529\n",
      "Epoch 00034: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 36\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.18 - ETA: 0s - loss: 1.0728 - 1s 9ms/step - loss: 0.7900 - val_loss: 0.2606\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.805 - 0s 750us/step - loss: 0.7177 - val_loss: 0.3142\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.808 - 0s 722us/step - loss: 0.7234 - val_loss: 0.4217\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.132 - 0s 826us/step - loss: 0.6462 - val_loss: 0.4331\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.728 - 0s 770us/step - loss: 0.6340 - val_loss: 0.4955\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.683 - 0s 750us/step - loss: 0.5833 - val_loss: 0.7207\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.138 - 0s 826us/step - loss: 0.5292 - val_loss: 0.6213\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.564 - 0s 864us/step - loss: 0.4917 - val_loss: 0.8134\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.135 - 0s 845us/step - loss: 0.4649 - val_loss: 0.7428\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.511 - 0s 845us/step - loss: 0.4247 - val_loss: 0.9478\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.538 - 0s 836us/step - loss: 0.4423 - val_loss: 1.0419\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.466 - 0s 884us/step - loss: 0.3842 - val_loss: 1.0484\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.517 - 0s 893us/step - loss: 0.3840 - val_loss: 1.0622\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.479 - 0s 855us/step - loss: 0.3586 - val_loss: 1.1113\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.417 - 0s 893us/step - loss: 0.3225 - val_loss: 1.1985\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.326 - 0s 931us/step - loss: 0.2710 - val_loss: 1.2237\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.147 - 0s 922us/step - loss: 0.2878 - val_loss: 1.2756\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.324 - 0s 845us/step - loss: 0.2653 - val_loss: 1.4494\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.195 - 0s 864us/step - loss: 0.2223 - val_loss: 1.4194\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.258 - 0s 865us/step - loss: 0.2370 - val_loss: 1.3193\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.482 - ETA: 0s - loss: 0.302 - 0s 864us/step - loss: 0.2557 - val_loss: 1.3545\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.223 - 0s 912us/step - loss: 0.2201 - val_loss: 1.4067\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.240 - 0s 864us/step - loss: 0.2016 - val_loss: 1.4426\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.279 - 0s 902us/step - loss: 0.2177 - val_loss: 1.6101\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.224 - 0s 902us/step - loss: 0.2263 - val_loss: 1.6822\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.225 - 0s 959us/step - loss: 0.2050 - val_loss: 1.6312\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.130 - 0s 855us/step - loss: 0.1848 - val_loss: 1.7231\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.212 - 0s 950us/step - loss: 0.1811 - val_loss: 1.7273\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.142 - 0s 884us/step - loss: 0.1853 - val_loss: 1.6957\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.134 - 0s 874us/step - loss: 0.1486 - val_loss: 1.9837\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.182 - 0s 845us/step - loss: 0.1604 - val_loss: 1.7945\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 37\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               46400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 46,501\n",
      "Trainable params: 46,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.52 - ETA: 0s - loss: 0.8426 - 1s 9ms/step - loss: 0.7804 - val_loss: 0.3079\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.734 - 0s 921us/step - loss: 0.5858 - val_loss: 0.7402\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.509 - 0s 855us/step - loss: 0.5322 - val_loss: 0.7340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.596 - ETA: 0s - loss: 0.592 - 0s 864us/step - loss: 0.5138 - val_loss: 0.7641\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.556 - 0s 855us/step - loss: 0.4837 - val_loss: 0.7249\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.540 - 0s 836us/step - loss: 0.4771 - val_loss: 0.7841\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.700 - ETA: 0s - loss: 0.407 - 0s 826us/step - loss: 0.4351 - val_loss: 0.7534\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.455 - 0s 817us/step - loss: 0.4242 - val_loss: 0.8898\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.433 - 0s 855us/step - loss: 0.4016 - val_loss: 1.0066\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.326 - 0s 836us/step - loss: 0.3768 - val_loss: 0.9226\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.397 - 0s 807us/step - loss: 0.3759 - val_loss: 0.9056\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.403 - 0s 855us/step - loss: 0.3612 - val_loss: 1.0790\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.406 - ETA: 0s - loss: 0.339 - 0s 855us/step - loss: 0.3522 - val_loss: 0.9455\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.752 - ETA: 0s - loss: 0.351 - 0s 893us/step - loss: 0.3522 - val_loss: 1.1431\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.327 - 0s 893us/step - loss: 0.3352 - val_loss: 1.0772\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.275 - 0s 845us/step - loss: 0.3023 - val_loss: 1.4881\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.303 - 0s 826us/step - loss: 0.3203 - val_loss: 1.2077\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.592 - ETA: 0s - loss: 0.315 - 0s 864us/step - loss: 0.2954 - val_loss: 1.1992\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.331 - 0s 864us/step - loss: 0.2995 - val_loss: 1.1443\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.304 - 0s 903us/step - loss: 0.2810 - val_loss: 1.2189\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.285 - 0s 893us/step - loss: 0.2743 - val_loss: 1.4192\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.272 - 0s 845us/step - loss: 0.2555 - val_loss: 1.2468\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.277 - 0s 855us/step - loss: 0.2552 - val_loss: 1.2283\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.235 - 0s 845us/step - loss: 0.2382 - val_loss: 1.0930\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.246 - 0s 940us/step - loss: 0.2482 - val_loss: 1.2374\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.273 - 0s 883us/step - loss: 0.2498 - val_loss: 1.0943\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.241 - 0s 826us/step - loss: 0.2358 - val_loss: 1.2567\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.389 - ETA: 0s - loss: 0.226 - 0s 846us/step - loss: 0.2252 - val_loss: 1.3652\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.223 - 0s 931us/step - loss: 0.2177 - val_loss: 1.3248\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.197 - 0s 836us/step - loss: 0.2174 - val_loss: 1.0557\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.230 - 0s 846us/step - loss: 0.2119 - val_loss: 1.2450\n",
      "Epoch 00031: early stopping\n",
      "MSE: 628.307493\n",
      "RMSE: 25.066063\n",
      "MAE: 19.815779\n",
      "MAPE: 24.492455\n",
      "\n",
      "Quantile 1, between 39.99999999999999 and 77.5\n",
      "MSE: 730.013868\n",
      "RMSE: 27.018769\n",
      "MAE: 22.028897\n",
      "MAPE: 37.364903\n",
      "\n",
      "Quantile 2, between 77.5 and 87.5\n",
      "MSE: 867.460469\n",
      "RMSE: 29.452682\n",
      "MAE: 19.192276\n",
      "MAPE: 23.196354\n",
      "\n",
      "Quantile 3, between 87.5 and 98.00000000000001\n",
      "MSE: 363.997151\n",
      "RMSE: 19.078709\n",
      "MAE: 17.289318\n",
      "MAPE: 18.415128\n",
      "\n",
      "Quantile 4, between 98.00000000000001 and 130.00000000000003\n",
      "MSE: 549.242746\n",
      "RMSE: 23.435929\n",
      "MAE: 20.437627\n",
      "MAPE: 18.256091\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_test = 15\n",
    "n_hexa = 15\n",
    "name_error = 'LSTM predictions - 15 Days - 15 hex - 4q'\n",
    "path_error = \"imgs/ips_predictions/prediction_lstm_15_days_15hex_4q.png\"\n",
    "name_map = '15 days - 15 hex - 4q'\n",
    "path_map = \"imgs/ips_predictions/maps_LSTM_prediction_15_total_data_15_hex_4q.png\"\n",
    "\n",
    "dict_pred,pred_lstm,pred_lstm_cases = lstm_hex_selected(data_days, dict_pred_sel_hexa,n_test,n_hexa,name_error,path_error,name_map,path_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,) (15,)\n",
      "model compiled 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 2.19 - ETA: 0s - loss: 0.8778 - 1s 9ms/step - loss: 0.8367 - val_loss: 0.3301\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.608 - 0s 922us/step - loss: 0.6335 - val_loss: 0.7574\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.664 - 0s 978us/step - loss: 0.5761 - val_loss: 1.2446\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.488 - 0s 864us/step - loss: 0.5517 - val_loss: 1.2193\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.537 - 0s 855us/step - loss: 0.5454 - val_loss: 1.3416\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.516 - ETA: 0s - loss: 0.456 - 0s 865us/step - loss: 0.5214 - val_loss: 1.2455\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.432 - 0s 941us/step - loss: 0.5075 - val_loss: 1.3015\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.454 - 0s 950us/step - loss: 0.4883 - val_loss: 1.4139\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.514 - 0s 864us/step - loss: 0.5043 - val_loss: 1.2048\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.743 - ETA: 0s - loss: 0.565 - 0s 893us/step - loss: 0.4784 - val_loss: 1.4468\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.983 - ETA: 0s - loss: 0.558 - 0s 883us/step - loss: 0.4653 - val_loss: 1.4149\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.537 - 0s 998us/step - loss: 0.4543 - val_loss: 1.3325\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.463 - ETA: 0s - loss: 0.418 - 0s 922us/step - loss: 0.4473 - val_loss: 1.3663\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.355 - 0s 959us/step - loss: 0.4253 - val_loss: 1.2839\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.373 - 0s 1ms/step - loss: 0.4479 - val_loss: 1.6032\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.409 - 0s 988us/step - loss: 0.4274 - val_loss: 1.8521\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.901 - ETA: 0s - loss: 0.406 - 0s 950us/step - loss: 0.4196 - val_loss: 1.5026\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.171 - ETA: 0s - loss: 0.506 - 0s 1ms/step - loss: 0.4302 - val_loss: 1.5550\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.321 - 0s 998us/step - loss: 0.4092 - val_loss: 1.5912\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.482 - 0s 988us/step - loss: 0.4173 - val_loss: 1.6419\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.386 - 0s 1ms/step - loss: 0.4040 - val_loss: 1.8156\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.398 - 0s 997us/step - loss: 0.3940 - val_loss: 1.8618\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.378 - ETA: 0s - loss: 0.472 - 0s 969us/step - loss: 0.3754 - val_loss: 1.7529\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.597 - ETA: 0s - loss: 0.438 - 0s 893us/step - loss: 0.4123 - val_loss: 1.7200\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.356 - 0s 950us/step - loss: 0.3900 - val_loss: 1.8332\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.373 - 0s 959us/step - loss: 0.3805 - val_loss: 1.7640\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.473 - 0s 940us/step - loss: 0.3865 - val_loss: 1.7893\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.366 - 0s 912us/step - loss: 0.3937 - val_loss: 2.0146\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.396 - 0s 922us/step - loss: 0.3697 - val_loss: 1.8825\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.031 - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.380 - 0s 1ms/step - loss: 0.3789 - val_loss: 1.9152\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.388 - 0s 874us/step - loss: 0.3555 - val_loss: 2.2263\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.96 - ETA: 0s - loss: 0.8723 - 1s 8ms/step - loss: 0.8293 - val_loss: 0.3212\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.638 - 0s 874us/step - loss: 0.5648 - val_loss: 0.6386\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.599 - ETA: 0s - loss: 0.425 - 0s 864us/step - loss: 0.4992 - val_loss: 0.7534\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.435 - ETA: 0s - loss: 0.492 - 0s 826us/step - loss: 0.4483 - val_loss: 1.1600\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.426 - 0s 912us/step - loss: 0.4390 - val_loss: 1.0426\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.341 - 0s 902us/step - loss: 0.4099 - val_loss: 0.9710\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.387 - 0s 864us/step - loss: 0.4019 - val_loss: 0.9846\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.434 - 0s 864us/step - loss: 0.3857 - val_loss: 0.8824\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.352 - 0s 940us/step - loss: 0.3823 - val_loss: 0.9999\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.790 - ETA: 0s - loss: 0.404 - 0s 912us/step - loss: 0.3494 - val_loss: 0.9890\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.312 - 0s 874us/step - loss: 0.3467 - val_loss: 0.9828\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.315 - 0s 921us/step - loss: 0.3412 - val_loss: 1.0279\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.305 - 0s 817us/step - loss: 0.3327 - val_loss: 0.8610\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.316 - 0s 769us/step - loss: 0.3227 - val_loss: 0.9121\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.289 - 0s 779us/step - loss: 0.3150 - val_loss: 1.0514\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.316 - 0s 731us/step - loss: 0.3254 - val_loss: 0.8510\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.334 - 0s 788us/step - loss: 0.3181 - val_loss: 0.8543\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.364 - 0s 807us/step - loss: 0.3154 - val_loss: 0.9703\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.327 - ETA: 0s - loss: 0.333 - 0s 779us/step - loss: 0.3097 - val_loss: 0.9080\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.319 - 0s 750us/step - loss: 0.3103 - val_loss: 1.0141\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.211 - 0s 1ms/step - loss: 0.3022 - val_loss: 0.9339\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.287 - 0s 798us/step - loss: 0.3012 - val_loss: 1.0099\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.279 - 0s 969us/step - loss: 0.2967 - val_loss: 1.0320\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.309 - 0s 770us/step - loss: 0.2913 - val_loss: 0.9976\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.272 - 0s 779us/step - loss: 0.2964 - val_loss: 1.0143\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.289 - 0s 779us/step - loss: 0.2936 - val_loss: 0.9551\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.318 - 0s 807us/step - loss: 0.2906 - val_loss: 1.0254\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.311 - 0s 798us/step - loss: 0.2943 - val_loss: 0.9894\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.244 - 0s 798us/step - loss: 0.2818 - val_loss: 0.8814\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.315 - 0s 798us/step - loss: 0.3096 - val_loss: 0.9165\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.314 - 0s 874us/step - loss: 0.2716 - val_loss: 1.0306\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 16s - loss: 1.69 - ETA: 0s - loss: 0.8077 - 1s 8ms/step - loss: 0.8109 - val_loss: 0.2805\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.644 - 0s 807us/step - loss: 0.5986 - val_loss: 0.5551\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.941 - ETA: 0s - loss: 0.533 - 0s 864us/step - loss: 0.5334 - val_loss: 0.5702\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - ETA: 0s - loss: 0.519 - 0s 741us/step - loss: 0.4899 - val_loss: 0.5984\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.564 - 0s 741us/step - loss: 0.4716 - val_loss: 0.5540\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.602 - ETA: 0s - loss: 0.447 - 0s 788us/step - loss: 0.4590 - val_loss: 0.6393\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.433 - 0s 788us/step - loss: 0.4507 - val_loss: 0.5307\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.424 - 0s 760us/step - loss: 0.4112 - val_loss: 0.6587\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.466 - 0s 836us/step - loss: 0.4118 - val_loss: 0.6031\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.346 - 0s 798us/step - loss: 0.3889 - val_loss: 0.6089\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.346 - 0s 817us/step - loss: 0.3844 - val_loss: 0.5913\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.418 - 0s 788us/step - loss: 0.3738 - val_loss: 0.6505\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.601 - ETA: 0s - loss: 0.290 - 0s 855us/step - loss: 0.3740 - val_loss: 0.6247\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.338 - 0s 751us/step - loss: 0.3561 - val_loss: 0.6453\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.345 - 0s 807us/step - loss: 0.3626 - val_loss: 0.6792\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.038 - ETA: 0s - loss: 0.311 - 0s 836us/step - loss: 0.3530 - val_loss: 0.6800\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.790 - ETA: 0s - loss: 0.434 - 0s 779us/step - loss: 0.3626 - val_loss: 0.7612\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.346 - 0s 826us/step - loss: 0.3418 - val_loss: 0.8356\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.361 - ETA: 0s - loss: 0.231 - 0s 864us/step - loss: 0.3313 - val_loss: 0.7475\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.294 - 0s 807us/step - loss: 0.3404 - val_loss: 0.7521\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.372 - 0s 827us/step - loss: 0.3339 - val_loss: 0.7836\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.640 - ETA: 0s - loss: 0.339 - 0s 779us/step - loss: 0.3512 - val_loss: 0.8133\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.683 - ETA: 0s - loss: 0.397 - 0s 807us/step - loss: 0.3406 - val_loss: 0.8150\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.315 - 0s 921us/step - loss: 0.3396 - val_loss: 0.8178\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.870 - ETA: 0s - loss: 0.302 - 0s 836us/step - loss: 0.3229 - val_loss: 0.8859\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.339 - ETA: 0s - loss: 0.347 - 0s 855us/step - loss: 0.3119 - val_loss: 0.8529\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.321 - 0s 817us/step - loss: 0.3296 - val_loss: 0.9452\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.327 - 0s 798us/step - loss: 0.3158 - val_loss: 0.9729\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.327 - 0s 883us/step - loss: 0.3142 - val_loss: 0.8343\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.301 - 0s 874us/step - loss: 0.3062 - val_loss: 0.9089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.100 - ETA: 0s - loss: 0.300 - 0s 788us/step - loss: 0.3025 - val_loss: 0.9155\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 3.06 - ETA: 0s - loss: 0.8696 - 1s 8ms/step - loss: 0.7988 - val_loss: 0.2999\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.703 - ETA: 0s - loss: 0.405 - 0s 807us/step - loss: 0.3959 - val_loss: 1.0477\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.274 - 0s 826us/step - loss: 0.2726 - val_loss: 1.7251\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.525 - ETA: 0s - loss: 0.272 - 0s 893us/step - loss: 0.2514 - val_loss: 2.0633\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.231 - 0s 836us/step - loss: 0.2439 - val_loss: 2.2058\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.234 - 0s 826us/step - loss: 0.2254 - val_loss: 2.2012\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.218 - 0s 817us/step - loss: 0.2209 - val_loss: 2.1601\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.206 - 0s 845us/step - loss: 0.2023 - val_loss: 2.4719\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.207 - 0s 855us/step - loss: 0.2211 - val_loss: 2.1947\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.202 - 0s 817us/step - loss: 0.2020 - val_loss: 2.3836\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.174 - 0s 874us/step - loss: 0.1915 - val_loss: 2.2096\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.212 - 0s 836us/step - loss: 0.1982 - val_loss: 2.4594\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.220 - 0s 865us/step - loss: 0.1975 - val_loss: 2.3562\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.178 - 0s 931us/step - loss: 0.1858 - val_loss: 2.3716\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.157 - 0s 902us/step - loss: 0.1851 - val_loss: 2.2996\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.195 - 0s 978us/step - loss: 0.1950 - val_loss: 2.4577\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.180 - 0s 855us/step - loss: 0.1835 - val_loss: 2.2655\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.158 - 0s 845us/step - loss: 0.1766 - val_loss: 2.4355\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.192 - 0s 836us/step - loss: 0.1881 - val_loss: 2.5376\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.183 - 0s 883us/step - loss: 0.1894 - val_loss: 2.5420\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.158 - 0s 950us/step - loss: 0.1703 - val_loss: 2.6033\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.181 - 0s 864us/step - loss: 0.1726 - val_loss: 2.4815\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.152 - 0s 845us/step - loss: 0.1763 - val_loss: 2.5792\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.174 - 0s 921us/step - loss: 0.1754 - val_loss: 2.2321\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.169 - 0s 808us/step - loss: 0.1642 - val_loss: 2.6326\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.194 - 0s 902us/step - loss: 0.1693 - val_loss: 2.4743\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.140 - 0s 827us/step - loss: 0.1615 - val_loss: 2.5332\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.194 - 0s 883us/step - loss: 0.1545 - val_loss: 2.2475\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.163 - 0s 884us/step - loss: 0.1608 - val_loss: 2.3200\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.133 - 0s 845us/step - loss: 0.1523 - val_loss: 2.5127\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.156 - 0s 903us/step - loss: 0.1515 - val_loss: 2.3380\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.18 - ETA: 0s - loss: 0.8603 - 1s 9ms/step - loss: 0.9076 - val_loss: 0.1086\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.569 - ETA: 0s - loss: 0.703 - 0s 760us/step - loss: 0.6757 - val_loss: 0.3782\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.662 - ETA: 0s - loss: 0.585 - 0s 845us/step - loss: 0.6329 - val_loss: 0.5646\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.710 - ETA: 0s - loss: 0.509 - 0s 836us/step - loss: 0.5825 - val_loss: 0.5230\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.506 - ETA: 0s - loss: 0.587 - 0s 959us/step - loss: 0.5703 - val_loss: 0.5775\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.632 - ETA: 0s - loss: 0.508 - 0s 893us/step - loss: 0.5447 - val_loss: 0.5877\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.993 - ETA: 0s - loss: 0.560 - 0s 969us/step - loss: 0.5438 - val_loss: 0.6421\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.515 - 0s 931us/step - loss: 0.5345 - val_loss: 0.5965\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.753 - ETA: 0s - loss: 0.486 - 0s 1ms/step - loss: 0.5149 - val_loss: 0.6483\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.486 - 0s 864us/step - loss: 0.5299 - val_loss: 0.5444\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.530 - ETA: 0s - loss: 0.617 - 0s 883us/step - loss: 0.5020 - val_loss: 0.6897\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.441 - 0s 893us/step - loss: 0.4828 - val_loss: 0.7590\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.546 - 0s 855us/step - loss: 0.4772 - val_loss: 0.7383\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.526 - ETA: 0s - loss: 0.323 - 0s 845us/step - loss: 0.4646 - val_loss: 0.6907\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.547 - ETA: 0s - loss: 0.436 - 0s 893us/step - loss: 0.4796 - val_loss: 0.8951\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.408 - 0s 845us/step - loss: 0.4411 - val_loss: 0.9782\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.373 - 0s 864us/step - loss: 0.4377 - val_loss: 1.0052\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.784 - ETA: 0s - loss: 0.329 - 0s 817us/step - loss: 0.4430 - val_loss: 0.8777\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.334 - 0s 1ms/step - loss: 0.4289 - val_loss: 0.7476\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.465 - 0s 836us/step - loss: 0.4354 - val_loss: 0.8699\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.437 - 0s 827us/step - loss: 0.4354 - val_loss: 0.9461\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.396 - 0s 845us/step - loss: 0.4478 - val_loss: 1.0180\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.418 - 0s 826us/step - loss: 0.4256 - val_loss: 0.9089\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.486 - 0s 836us/step - loss: 0.4363 - val_loss: 0.9753\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.413 - 0s 950us/step - loss: 0.4265 - val_loss: 1.1232\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.382 - 0s 855us/step - loss: 0.4019 - val_loss: 0.9720\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.428 - 0s 912us/step - loss: 0.4046 - val_loss: 1.0066\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.368 - 0s 864us/step - loss: 0.4046 - val_loss: 1.0010\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.399 - 0s 902us/step - loss: 0.4081 - val_loss: 1.0252\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.369 - 0s 845us/step - loss: 0.4005 - val_loss: 1.0684\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.388 - 0s 997us/step - loss: 0.4123 - val_loss: 0.9474\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.38 - ETA: 0s - loss: 0.7354 - 1s 8ms/step - loss: 0.8587 - val_loss: 0.0830\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.965 - ETA: 0s - loss: 0.629 - 0s 817us/step - loss: 0.6953 - val_loss: 0.3225\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.724 - ETA: 0s - loss: 0.657 - 0s 836us/step - loss: 0.6387 - val_loss: 0.4833\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.679 - ETA: 0s - loss: 0.633 - 0s 855us/step - loss: 0.5940 - val_loss: 0.5453\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.725 - 0s 969us/step - loss: 0.5673 - val_loss: 0.6034\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.498 - 0s 893us/step - loss: 0.5438 - val_loss: 0.6870\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.555 - 0s 893us/step - loss: 0.5253 - val_loss: 0.7279\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.730 - ETA: 0s - loss: 0.609 - 0s 874us/step - loss: 0.5016 - val_loss: 0.9115\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.485 - 0s 865us/step - loss: 0.4743 - val_loss: 0.8404\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.587 - ETA: 0s - loss: 0.598 - 0s 893us/step - loss: 0.4608 - val_loss: 0.9758\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.520 - 0s 893us/step - loss: 0.4435 - val_loss: 0.9659\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.459 - 0s 855us/step - loss: 0.4348 - val_loss: 1.0537\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.534 - 0s 883us/step - loss: 0.4468 - val_loss: 1.1726\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.424 - 0s 893us/step - loss: 0.4060 - val_loss: 1.1267\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.682 - ETA: 0s - loss: 0.451 - 0s 960us/step - loss: 0.4201 - val_loss: 1.0775\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.457 - 0s 865us/step - loss: 0.4049 - val_loss: 1.1671\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.319 - 0s 864us/step - loss: 0.3998 - val_loss: 1.1710\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.574 - ETA: 0s - loss: 0.480 - 0s 902us/step - loss: 0.4156 - val_loss: 1.2273\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.652 - ETA: 0s - loss: 0.483 - 0s 902us/step - loss: 0.3967 - val_loss: 1.1687\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.355 - 0s 845us/step - loss: 0.3905 - val_loss: 1.2485\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.373 - 0s 921us/step - loss: 0.3991 - val_loss: 1.1213\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.450 - 0s 902us/step - loss: 0.3873 - val_loss: 1.2580\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.379 - 0s 874us/step - loss: 0.3742 - val_loss: 1.2811\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.383 - 0s 864us/step - loss: 0.3653 - val_loss: 1.2107\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.422 - ETA: 0s - loss: 0.419 - 0s 921us/step - loss: 0.3667 - val_loss: 1.3032\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.364 - 0s 950us/step - loss: 0.3881 - val_loss: 1.2596\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.392 - 0s 940us/step - loss: 0.3705 - val_loss: 1.2892\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.338 - 0s 902us/step - loss: 0.3746 - val_loss: 1.3499\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.338 - 0s 855us/step - loss: 0.3591 - val_loss: 1.3551\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.350 - 0s 826us/step - loss: 0.3582 - val_loss: 1.4238\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.365 - 0s 788us/step - loss: 0.3558 - val_loss: 1.4208\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.22 - ETA: 0s - loss: 0.5717 - 1s 9ms/step - loss: 0.8129 - val_loss: 0.2311\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.628 - ETA: 0s - loss: 0.718 - 0s 760us/step - loss: 0.7337 - val_loss: 0.4904\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.380 - ETA: 0s - loss: 0.797 - 0s 827us/step - loss: 0.6917 - val_loss: 0.5777\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.743 - 0s 836us/step - loss: 0.6730 - val_loss: 0.5449\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.504 - 0s 874us/step - loss: 0.6539 - val_loss: 0.5811\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.965 - ETA: 0s - loss: 0.528 - 0s 921us/step - loss: 0.6201 - val_loss: 0.6373\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.727 - 0s 817us/step - loss: 0.6084 - val_loss: 0.7333\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.377 - ETA: 0s - loss: 0.604 - 0s 779us/step - loss: 0.5799 - val_loss: 0.7388\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.597 - 0s 798us/step - loss: 0.5682 - val_loss: 0.8005\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.576 - 0s 817us/step - loss: 0.5412 - val_loss: 0.7546\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.989 - ETA: 0s - loss: 0.545 - 0s 864us/step - loss: 0.5245 - val_loss: 0.7443\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.641 - 0s 836us/step - loss: 0.5279 - val_loss: 0.7618\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.630 - ETA: 0s - loss: 0.410 - 0s 845us/step - loss: 0.5122 - val_loss: 0.8066\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.438 - 0s 845us/step - loss: 0.4975 - val_loss: 0.7534\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.465 - 0s 788us/step - loss: 0.4733 - val_loss: 0.9913\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.405 - 0s 874us/step - loss: 0.4788 - val_loss: 0.7731\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.551 - 0s 903us/step - loss: 0.4812 - val_loss: 0.9801\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.439 - 0s 845us/step - loss: 0.4902 - val_loss: 0.8770\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.503 - 0s 807us/step - loss: 0.4675 - val_loss: 0.8855\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.456 - 0s 836us/step - loss: 0.4465 - val_loss: 0.9166\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.053 - ETA: 0s - loss: 0.477 - 0s 855us/step - loss: 0.4596 - val_loss: 0.9368\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.320 - 0s 817us/step - loss: 0.4366 - val_loss: 0.7215\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.510 - 0s 845us/step - loss: 0.4458 - val_loss: 0.9611\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.373 - 0s 912us/step - loss: 0.4236 - val_loss: 0.8521\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.415 - 0s 807us/step - loss: 0.4151 - val_loss: 0.9239\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.393 - ETA: 0s - loss: 0.440 - 0s 807us/step - loss: 0.4383 - val_loss: 0.8705\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.035 - ETA: 0s - loss: 0.482 - 0s 874us/step - loss: 0.4286 - val_loss: 1.0290\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.415 - 0s 864us/step - loss: 0.3993 - val_loss: 0.9750\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.402 - 0s 922us/step - loss: 0.3994 - val_loss: 0.9600\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.241 - 0s 864us/step - loss: 0.4063 - val_loss: 1.0982\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.428 - 0s 817us/step - loss: 0.4029 - val_loss: 1.0756\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 7\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.67 - ETA: 0s - loss: 0.8836 - 1s 9ms/step - loss: 0.8873 - val_loss: 0.1788\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.645 - 0s 845us/step - loss: 0.5891 - val_loss: 0.7045\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.581 - 0s 893us/step - loss: 0.5078 - val_loss: 1.0776\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.431 - 0s 855us/step - loss: 0.4804 - val_loss: 1.0156\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.434 - 0s 902us/step - loss: 0.4789 - val_loss: 1.0089\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.378 - 0s 912us/step - loss: 0.4586 - val_loss: 1.1447\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.455 - 0s 864us/step - loss: 0.4562 - val_loss: 1.2499\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.498 - ETA: 0s - loss: 0.412 - 0s 874us/step - loss: 0.4402 - val_loss: 1.0825\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.380 - ETA: 0s - loss: 0.388 - 0s 874us/step - loss: 0.4277 - val_loss: 1.1433\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.431 - ETA: 0s - loss: 0.409 - 0s 788us/step - loss: 0.4179 - val_loss: 1.2098\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.425 - 0s 788us/step - loss: 0.4248 - val_loss: 1.0779\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.384 - 0s 807us/step - loss: 0.3912 - val_loss: 1.2053\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.362 - 0s 769us/step - loss: 0.4020 - val_loss: 1.2208\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.471 - 0s 836us/step - loss: 0.3940 - val_loss: 1.0691\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.423 - 0s 817us/step - loss: 0.3754 - val_loss: 1.2549\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.295 - 0s 817us/step - loss: 0.3699 - val_loss: 1.3083\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.408 - 0s 826us/step - loss: 0.3779 - val_loss: 1.2146\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.465 - ETA: 0s - loss: 0.439 - 0s 902us/step - loss: 0.3669 - val_loss: 1.2648\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.896 - ETA: 0s - loss: 0.373 - 0s 836us/step - loss: 0.3579 - val_loss: 1.2978\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.381 - 0s 779us/step - loss: 0.3586 - val_loss: 1.3037\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.360 - 0s 760us/step - loss: 0.3744 - val_loss: 1.2483\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.367 - 0s 750us/step - loss: 0.3587 - val_loss: 1.3548\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.339 - 0s 817us/step - loss: 0.3553 - val_loss: 1.4015\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.352 - 0s 855us/step - loss: 0.3409 - val_loss: 1.1882\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.356 - 0s 817us/step - loss: 0.3329 - val_loss: 1.3339\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.017 - ETA: 0s - loss: 0.386 - 0s 817us/step - loss: 0.3406 - val_loss: 1.3529\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.450 - ETA: 0s - loss: 0.361 - 0s 826us/step - loss: 0.3292 - val_loss: 1.4635\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.346 - 0s 808us/step - loss: 0.3243 - val_loss: 1.3277\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.362 - 0s 874us/step - loss: 0.3322 - val_loss: 1.3478\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.319 - 0s 1ms/step - loss: 0.3091 - val_loss: 1.4280\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.302 - 0s 845us/step - loss: 0.3293 - val_loss: 1.3114\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 8\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.38 - ETA: 0s - loss: 0.9709 - 1s 9ms/step - loss: 0.6933 - val_loss: 0.2875\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.354 - 0s 845us/step - loss: 0.5190 - val_loss: 0.4412\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.469 - 0s 836us/step - loss: 0.4300 - val_loss: 0.5712\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.439 - 0s 779us/step - loss: 0.3946 - val_loss: 0.6103\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.448 - 0s 798us/step - loss: 0.3867 - val_loss: 0.4995\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.338 - 0s 807us/step - loss: 0.3150 - val_loss: 0.4710\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.183 - 0s 845us/step - loss: 0.3021 - val_loss: 0.3360\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.177 - 0s 845us/step - loss: 0.2957 - val_loss: 0.4108\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.312 - 0s 826us/step - loss: 0.2778 - val_loss: 0.3465\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.231 - 0s 1ms/step - loss: 0.2374 - val_loss: 0.3021\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.152 - 0s 893us/step - loss: 0.2388 - val_loss: 0.3237\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.139 - 0s 855us/step - loss: 0.2233 - val_loss: 0.2914\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.237 - 0s 826us/step - loss: 0.2063 - val_loss: 0.2950\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.249 - 0s 864us/step - loss: 0.2071 - val_loss: 0.3001\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.190 - 0s 874us/step - loss: 0.1705 - val_loss: 0.3230\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.113 - 0s 836us/step - loss: 0.2041 - val_loss: 0.3205\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.218 - 0s 893us/step - loss: 0.2053 - val_loss: 0.3366\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.419 - ETA: 0s - loss: 0.197 - 0s 845us/step - loss: 0.1893 - val_loss: 0.3324\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.153 - 0s 1ms/step - loss: 0.1714 - val_loss: 0.3363\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.199 - 0s 940us/step - loss: 0.1686 - val_loss: 0.3452\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.186 - 0s 912us/step - loss: 0.1526 - val_loss: 0.3200\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.190 - 0s 893us/step - loss: 0.1730 - val_loss: 0.3663\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.152 - 0s 864us/step - loss: 0.1641 - val_loss: 0.2984\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.158 - 0s 883us/step - loss: 0.1465 - val_loss: 0.4040\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.140 - 0s 864us/step - loss: 0.1556 - val_loss: 0.3577\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.166 - 0s 893us/step - loss: 0.1640 - val_loss: 0.3799\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.127 - 0s 912us/step - loss: 0.1386 - val_loss: 0.3944\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.138 - 0s 845us/step - loss: 0.1578 - val_loss: 0.4025\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.132 - 0s 902us/step - loss: 0.1466 - val_loss: 0.3482\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.427 - ETA: 0s - loss: 0.154 - 0s 817us/step - loss: 0.1563 - val_loss: 0.3803\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.123 - 0s 817us/step - loss: 0.1364 - val_loss: 0.3068\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 9\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 1.61 - ETA: 0s - loss: 0.8304 - 1s 9ms/step - loss: 0.7798 - val_loss: 0.2487\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.601 - 0s 836us/step - loss: 0.5527 - val_loss: 0.6505\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.529 - 0s 836us/step - loss: 0.5414 - val_loss: 0.8041\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.544 - 0s 769us/step - loss: 0.5055 - val_loss: 0.7458\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.481 - 0s 836us/step - loss: 0.4836 - val_loss: 0.7611\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.468 - 0s 846us/step - loss: 0.4219 - val_loss: 0.9454\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.408 - 0s 864us/step - loss: 0.3769 - val_loss: 0.9405\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.030 - ETA: 0s - loss: 0.339 - 0s 826us/step - loss: 0.3616 - val_loss: 0.6743\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.334 - 0s 940us/step - loss: 0.3248 - val_loss: 0.8502\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.327 - 0s 836us/step - loss: 0.3328 - val_loss: 0.7939\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.668 - ETA: 0s - loss: 0.316 - 0s 855us/step - loss: 0.2882 - val_loss: 0.6385\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.256 - 0s 855us/step - loss: 0.2441 - val_loss: 0.6597\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.298 - 0s 874us/step - loss: 0.2666 - val_loss: 0.6682\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.447 - ETA: 0s - loss: 0.261 - 0s 827us/step - loss: 0.2374 - val_loss: 0.6495\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.270 - 0s 864us/step - loss: 0.2443 - val_loss: 0.6241\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.201 - 0s 845us/step - loss: 0.2245 - val_loss: 0.7093\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.242 - 0s 893us/step - loss: 0.2362 - val_loss: 0.6428\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.190 - 0s 893us/step - loss: 0.2163 - val_loss: 0.6036\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.078 - ETA: 0s - loss: 0.197 - 0s 921us/step - loss: 0.2086 - val_loss: 0.7289\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.192 - 0s 893us/step - loss: 0.2079 - val_loss: 0.5768\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.168 - 0s 836us/step - loss: 0.1870 - val_loss: 0.6041\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.781 - ETA: 0s - loss: 0.197 - 0s 855us/step - loss: 0.1981 - val_loss: 0.6263\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.251 - 0s 921us/step - loss: 0.2001 - val_loss: 0.5533\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.216 - 0s 884us/step - loss: 0.2038 - val_loss: 0.5437\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.192 - 0s 874us/step - loss: 0.1821 - val_loss: 0.5626\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.200 - 0s 931us/step - loss: 0.1737 - val_loss: 0.5710\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.194 - 0s 893us/step - loss: 0.1873 - val_loss: 0.5309\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.222 - 0s 779us/step - loss: 0.1992 - val_loss: 0.6941\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.553 - ETA: 0s - loss: 0.202 - 0s 807us/step - loss: 0.1958 - val_loss: 0.5983\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.190 - 0s 845us/step - loss: 0.1837 - val_loss: 0.5867\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.230 - 0s 855us/step - loss: 0.2078 - val_loss: 0.5866\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.36 - ETA: 0s - loss: 0.5922 - 1s 9ms/step - loss: 0.8266 - val_loss: 0.2209\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.667 - 0s 826us/step - loss: 0.6308 - val_loss: 0.5288\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.508 - 0s 883us/step - loss: 0.5355 - val_loss: 0.7034\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.575 - 0s 855us/step - loss: 0.4821 - val_loss: 0.9323\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.503 - 0s 864us/step - loss: 0.4255 - val_loss: 1.0332\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.417 - 0s 836us/step - loss: 0.4349 - val_loss: 0.9950\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.281 - 0s 836us/step - loss: 0.4082 - val_loss: 1.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.470 - ETA: 0s - loss: 0.446 - 0s 826us/step - loss: 0.3847 - val_loss: 1.2665\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.436 - ETA: 0s - loss: 0.407 - 0s 798us/step - loss: 0.3472 - val_loss: 1.0237\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.216 - 0s 836us/step - loss: 0.3077 - val_loss: 0.9497\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.377 - 0s 826us/step - loss: 0.3110 - val_loss: 1.5161\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.259 - 0s 817us/step - loss: 0.2854 - val_loss: 1.3535\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.273 - 0s 874us/step - loss: 0.2486 - val_loss: 1.1820\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.254 - 0s 817us/step - loss: 0.2504 - val_loss: 1.2300\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.255 - 0s 807us/step - loss: 0.2464 - val_loss: 1.1686\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.292 - 0s 817us/step - loss: 0.2461 - val_loss: 1.1603\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.214 - 0s 845us/step - loss: 0.2592 - val_loss: 1.2719\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.210 - 0s 798us/step - loss: 0.2209 - val_loss: 1.0891\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.224 - 0s 836us/step - loss: 0.2381 - val_loss: 1.2504\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.637 - ETA: 0s - loss: 0.284 - 0s 864us/step - loss: 0.2248 - val_loss: 1.1266\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.627 - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.211 - 0s 1ms/step - loss: 0.2245 - val_loss: 1.1009\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.752 - ETA: 0s - loss: 0.187 - 0s 845us/step - loss: 0.2038 - val_loss: 1.0931\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.235 - 0s 846us/step - loss: 0.2090 - val_loss: 1.2061\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.180 - 0s 864us/step - loss: 0.1909 - val_loss: 1.1230\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.271 - 0s 865us/step - loss: 0.2148 - val_loss: 1.2055\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.198 - 0s 855us/step - loss: 0.2095 - val_loss: 1.2542\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.263 - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.206 - 1s 7ms/step - loss: 0.1907 - val_loss: 1.2856\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.194 - 1s 7ms/step - loss: 0.1930 - val_loss: 1.3309\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.193 - 1s 7ms/step - loss: 0.1921 - val_loss: 1.1234\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.182 - 1s 6ms/step - loss: 0.2063 - val_loss: 1.2527\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.378 - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.266 - 0s 5ms/step - loss: 0.2073 - val_loss: 1.0834\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 11\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.20 - ETA: 0s - loss: 0.8663 - 1s 9ms/step - loss: 0.7611 - val_loss: 0.3155\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.388 - 0s 845us/step - loss: 0.5120 - val_loss: 0.7512\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.483 - 0s 855us/step - loss: 0.4302 - val_loss: 1.1237\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.541 - ETA: 0s - loss: 0.501 - 0s 931us/step - loss: 0.4129 - val_loss: 1.2903\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.493 - ETA: 0s - loss: 0.475 - 0s 921us/step - loss: 0.3866 - val_loss: 1.2542\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.406 - 0s 893us/step - loss: 0.3704 - val_loss: 1.3998\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.388 - 0s 921us/step - loss: 0.3556 - val_loss: 1.3316\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.622 - ETA: 0s - loss: 0.401 - 0s 883us/step - loss: 0.3544 - val_loss: 1.2916\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.481 - ETA: 0s - loss: 0.323 - 0s 855us/step - loss: 0.3334 - val_loss: 1.3406\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.304 - 0s 883us/step - loss: 0.3307 - val_loss: 1.3832\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.355 - 0s 864us/step - loss: 0.3429 - val_loss: 1.6117\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.366 - 0s 883us/step - loss: 0.3259 - val_loss: 1.4225\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.323 - 0s 883us/step - loss: 0.3086 - val_loss: 1.5771\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.198 - 0s 940us/step - loss: 0.3068 - val_loss: 1.5380\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.397 - ETA: 0s - loss: 0.345 - 0s 903us/step - loss: 0.3055 - val_loss: 1.6500\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.319 - 0s 874us/step - loss: 0.2873 - val_loss: 1.5508\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.477 - ETA: 0s - loss: 0.366 - 0s 921us/step - loss: 0.2870 - val_loss: 1.7822\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.303 - 0s 883us/step - loss: 0.2933 - val_loss: 1.7087\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.353 - ETA: 0s - loss: 0.279 - 0s 874us/step - loss: 0.2893 - val_loss: 1.4952\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.316 - 0s 884us/step - loss: 0.2914 - val_loss: 1.5324\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.255 - 0s 817us/step - loss: 0.2804 - val_loss: 1.8396\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.653 - ETA: 0s - loss: 0.260 - 0s 788us/step - loss: 0.2857 - val_loss: 1.4475\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.227 - 0s 807us/step - loss: 0.2683 - val_loss: 1.7023\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.259 - 0s 817us/step - loss: 0.2671 - val_loss: 2.0081\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.245 - 0s 788us/step - loss: 0.2894 - val_loss: 1.4538\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.269 - 0s 769us/step - loss: 0.2577 - val_loss: 1.8180\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.205 - 0s 769us/step - loss: 0.2692 - val_loss: 1.7171\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.263 - 0s 807us/step - loss: 0.2677 - val_loss: 2.1559\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.191 - 0s 845us/step - loss: 0.2401 - val_loss: 1.7629\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.257 - 0s 826us/step - loss: 0.2538 - val_loss: 1.9512\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.260 - 0s 788us/step - loss: 0.2518 - val_loss: 2.0035\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 12\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 1.21 - ETA: 0s - loss: 0.8006 - 1s 9ms/step - loss: 0.7430 - val_loss: 0.3806\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.576 - 0s 826us/step - loss: 0.4746 - val_loss: 0.9892\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.345 - 0s 817us/step - loss: 0.3777 - val_loss: 1.3590\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - ETA: 0s - loss: 0.346 - 0s 788us/step - loss: 0.3492 - val_loss: 1.6049\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.356 - 0s 1ms/step - loss: 0.3402 - val_loss: 1.7512\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.354 - 0s 826us/step - loss: 0.3238 - val_loss: 1.6030\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.326 - 0s 807us/step - loss: 0.3096 - val_loss: 1.4929\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.248 - 0s 903us/step - loss: 0.2910 - val_loss: 1.7607\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.309 - 0s 855us/step - loss: 0.2879 - val_loss: 1.6211\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.254 - 0s 817us/step - loss: 0.3004 - val_loss: 1.6619\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.259 - 0s 864us/step - loss: 0.2847 - val_loss: 1.5723\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.295 - 0s 807us/step - loss: 0.2784 - val_loss: 1.7171\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.236 - 0s 836us/step - loss: 0.2764 - val_loss: 1.5822\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.248 - 0s 855us/step - loss: 0.2590 - val_loss: 1.7349\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.361 - ETA: 0s - loss: 0.296 - 0s 903us/step - loss: 0.2623 - val_loss: 1.7300\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.284 - 0s 864us/step - loss: 0.2648 - val_loss: 1.7252\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.255 - 0s 893us/step - loss: 0.2554 - val_loss: 1.6273\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.416 - ETA: 0s - loss: 0.249 - 0s 940us/step - loss: 0.2446 - val_loss: 1.7998\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.241 - 0s 893us/step - loss: 0.2413 - val_loss: 1.5218\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.234 - 0s 836us/step - loss: 0.2474 - val_loss: 1.5845\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.416 - ETA: 0s - loss: 0.267 - 0s 864us/step - loss: 0.2315 - val_loss: 1.8835\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.212 - 0s 893us/step - loss: 0.2231 - val_loss: 2.0078\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.252 - 0s 836us/step - loss: 0.2190 - val_loss: 1.8628\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.246 - 0s 912us/step - loss: 0.2278 - val_loss: 1.8798\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.217 - 0s 950us/step - loss: 0.2124 - val_loss: 1.7800\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.202 - 0s 931us/step - loss: 0.2077 - val_loss: 2.1274\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.213 - 0s 912us/step - loss: 0.2063 - val_loss: 1.9840\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.225 - 0s 1ms/step - loss: 0.2252 - val_loss: 2.2563\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.220 - 0s 883us/step - loss: 0.2159 - val_loss: 1.9145\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.209 - 0s 902us/step - loss: 0.2129 - val_loss: 2.1744\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.186 - 0s 912us/step - loss: 0.2051 - val_loss: 2.1905\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 13\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.96 - ETA: 0s - loss: 0.6363 - 1s 8ms/step - loss: 0.8086 - val_loss: 0.2590\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.615 - 0s 798us/step - loss: 0.5797 - val_loss: 0.6861\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.411 - 0s 798us/step - loss: 0.5089 - val_loss: 0.8917\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.967 - ETA: 0s - loss: 0.508 - 0s 760us/step - loss: 0.4787 - val_loss: 0.9203\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.673 - ETA: 0s - loss: 0.429 - 0s 893us/step - loss: 0.4595 - val_loss: 0.9176\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.491 - 0s 902us/step - loss: 0.4427 - val_loss: 0.9897\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.447 - ETA: 0s - loss: 0.387 - 0s 817us/step - loss: 0.4326 - val_loss: 0.9663\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.345 - 0s 798us/step - loss: 0.3955 - val_loss: 1.0410\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.479 - ETA: 0s - loss: 0.361 - 0s 836us/step - loss: 0.3753 - val_loss: 1.0837\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.374 - 0s 779us/step - loss: 0.3845 - val_loss: 1.1559\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.972 - ETA: 0s - loss: 0.405 - 0s 769us/step - loss: 0.3673 - val_loss: 1.0671\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.463 - ETA: 0s - loss: 0.328 - 0s 798us/step - loss: 0.3661 - val_loss: 1.0469\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.415 - 0s 808us/step - loss: 0.3697 - val_loss: 1.2765\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.357 - 0s 760us/step - loss: 0.3448 - val_loss: 1.2782\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.393 - 0s 760us/step - loss: 0.3573 - val_loss: 1.3347\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.964 - ETA: 0s - loss: 0.374 - 0s 864us/step - loss: 0.3393 - val_loss: 1.3650\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.280 - 0s 883us/step - loss: 0.3403 - val_loss: 1.5392\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.356 - 0s 788us/step - loss: 0.3282 - val_loss: 1.2903\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.342 - 0s 817us/step - loss: 0.3354 - val_loss: 1.3686\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.378 - 0s 817us/step - loss: 0.3357 - val_loss: 1.2895\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.312 - 0s 836us/step - loss: 0.3239 - val_loss: 1.6099\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.235 - 0s 1ms/step - loss: 0.3135 - val_loss: 1.3357\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.387 - ETA: 0s - loss: 0.324 - 0s 826us/step - loss: 0.2969 - val_loss: 1.4892\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.296 - 0s 807us/step - loss: 0.3163 - val_loss: 1.6428\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.270 - 0s 817us/step - loss: 0.2870 - val_loss: 1.6795\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.307 - 0s 836us/step - loss: 0.2990 - val_loss: 1.7132\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.240 - 0s 874us/step - loss: 0.2834 - val_loss: 1.7759\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.439 - ETA: 0s - loss: 0.287 - 0s 855us/step - loss: 0.3042 - val_loss: 1.6182\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.276 - 0s 836us/step - loss: 0.2746 - val_loss: 1.7797\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.568 - ETA: 0s - loss: 0.260 - 0s 855us/step - loss: 0.2778 - val_loss: 1.6928\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.498 - ETA: 0s - loss: 0.290 - 0s 845us/step - loss: 0.2771 - val_loss: 1.7400\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 14\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.38 - ETA: 0s - loss: 0.9976 - 1s 9ms/step - loss: 0.8688 - val_loss: 0.1919\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.740 - 0s 931us/step - loss: 0.6621 - val_loss: 0.5666\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.562 - 0s 855us/step - loss: 0.5756 - val_loss: 0.8184\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.540 - 0s 874us/step - loss: 0.5215 - val_loss: 1.0518\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.484 - ETA: 0s - loss: 0.460 - 0s 836us/step - loss: 0.4775 - val_loss: 1.1256\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.427 - 0s 969us/step - loss: 0.4414 - val_loss: 1.3405\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.020 - ETA: 0s - loss: 0.417 - 0s 855us/step - loss: 0.4119 - val_loss: 1.6095\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.615 - ETA: 0s - loss: 0.406 - 0s 921us/step - loss: 0.4080 - val_loss: 1.9467\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.359 - 0s 845us/step - loss: 0.4107 - val_loss: 1.9443\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.495 - ETA: 0s - loss: 0.346 - 0s 978us/step - loss: 0.3779 - val_loss: 2.0499\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.734 - ETA: 0s - loss: 0.444 - 0s 874us/step - loss: 0.3871 - val_loss: 2.1333\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.311 - 0s 874us/step - loss: 0.3526 - val_loss: 2.2656\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.398 - 0s 931us/step - loss: 0.3702 - val_loss: 2.4251\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.619 - ETA: 0s - loss: 0.325 - 0s 845us/step - loss: 0.3585 - val_loss: 2.2446\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.469 - ETA: 0s - loss: 0.398 - 0s 836us/step - loss: 0.3515 - val_loss: 2.7425\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.504 - ETA: 0s - loss: 0.399 - 0s 1ms/step - loss: 0.3574 - val_loss: 2.8528\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.456 - ETA: 0s - loss: 0.379 - 0s 883us/step - loss: 0.3457 - val_loss: 2.8037\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.336 - 0s 731us/step - loss: 0.3456 - val_loss: 2.7569\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.363 - 0s 836us/step - loss: 0.3354 - val_loss: 2.8829\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.689 - ETA: 0s - loss: 0.326 - 0s 760us/step - loss: 0.3252 - val_loss: 3.4390\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.335 - 0s 769us/step - loss: 0.3208 - val_loss: 2.9691\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.352 - 0s 788us/step - loss: 0.3301 - val_loss: 3.0613\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.307 - 0s 798us/step - loss: 0.3139 - val_loss: 3.2386\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.306 - 0s 703us/step - loss: 0.3118 - val_loss: 3.2400\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.304 - 0s 760us/step - loss: 0.3139 - val_loss: 3.6224\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.293 - 0s 826us/step - loss: 0.3078 - val_loss: 3.1006\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.311 - 0s 712us/step - loss: 0.3005 - val_loss: 3.1266\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.282 - 0s 741us/step - loss: 0.3027 - val_loss: 3.0581\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.294 - 0s 769us/step - loss: 0.2943 - val_loss: 3.5492\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.295 - 0s 769us/step - loss: 0.2927 - val_loss: 3.2464\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.453 - ETA: 0s - loss: 0.271 - 0s 779us/step - loss: 0.2888 - val_loss: 3.8302\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 15\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.67 - ETA: 0s - loss: 0.6421 - 1s 9ms/step - loss: 0.8014 - val_loss: 0.2483\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.692 - 0s 912us/step - loss: 0.6084 - val_loss: 0.5779\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.463 - 0s 855us/step - loss: 0.5513 - val_loss: 0.6247\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.715 - ETA: 0s - loss: 0.569 - 0s 855us/step - loss: 0.5056 - val_loss: 0.6751\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.424 - 0s 855us/step - loss: 0.4825 - val_loss: 0.6874\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.424 - 0s 874us/step - loss: 0.4452 - val_loss: 0.6001\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.365 - ETA: 0s - loss: 0.385 - 0s 874us/step - loss: 0.4294 - val_loss: 0.6020\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.518 - 0s 988us/step - loss: 0.4326 - val_loss: 0.6034\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.495 - 0s 1ms/step - loss: 0.4130 - val_loss: 0.5881\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.340 - 0s 1ms/step - loss: 0.3856 - val_loss: 0.6906\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.379 - 0s 997us/step - loss: 0.3886 - val_loss: 0.6390\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.391 - 0s 1ms/step - loss: 0.3847 - val_loss: 0.6902\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.352 - 0s 959us/step - loss: 0.3696 - val_loss: 0.5571\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.356 - 0s 1ms/step - loss: 0.3855 - val_loss: 0.6547\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.118 - ETA: 0s - loss: 0.405 - 0s 959us/step - loss: 0.3764 - val_loss: 0.4065\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.337 - 0s 731us/step - loss: 0.3499 - val_loss: 0.4941\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.297 - 0s 845us/step - loss: 0.3450 - val_loss: 0.5224\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.620 - ETA: 0s - loss: 0.364 - 0s 693us/step - loss: 0.3498 - val_loss: 0.5191\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.546 - ETA: 0s - loss: 0.361 - 0s 788us/step - loss: 0.3491 - val_loss: 0.4847\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.330 - 0s 807us/step - loss: 0.3423 - val_loss: 0.4832\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.347 - 0s 883us/step - loss: 0.3262 - val_loss: 0.5577\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.388 - 0s 855us/step - loss: 0.3563 - val_loss: 0.4824\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.398 - ETA: 0s - loss: 0.255 - 0s 903us/step - loss: 0.3131 - val_loss: 0.5986\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.295 - 0s 931us/step - loss: 0.3343 - val_loss: 0.4298\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.301 - 0s 912us/step - loss: 0.3343 - val_loss: 0.4565\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.373 - 0s 883us/step - loss: 0.3298 - val_loss: 0.4513\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.288 - 0s 931us/step - loss: 0.2950 - val_loss: 0.4556\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.278 - 0s 836us/step - loss: 0.2950 - val_loss: 0.4538\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.330 - 0s 855us/step - loss: 0.3103 - val_loss: 0.4940\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.285 - 0s 874us/step - loss: 0.2962 - val_loss: 0.4768\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.324 - 0s 921us/step - loss: 0.2892 - val_loss: 0.4496\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 16\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.26 - ETA: 0s - loss: 0.7739 - 1s 9ms/step - loss: 0.7457 - val_loss: 0.4930\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.602 - 0s 798us/step - loss: 0.5816 - val_loss: 0.9804\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.018 - ETA: 0s - loss: 0.494 - 0s 836us/step - loss: 0.5221 - val_loss: 0.9430\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.469 - ETA: 0s - loss: 0.456 - 0s 798us/step - loss: 0.5260 - val_loss: 1.1475\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.424 - 0s 817us/step - loss: 0.4856 - val_loss: 1.0563\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.498 - ETA: 0s - loss: 0.506 - 0s 826us/step - loss: 0.4943 - val_loss: 1.2040\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.502 - 0s 940us/step - loss: 0.4692 - val_loss: 1.2257\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.453 - 0s 893us/step - loss: 0.4601 - val_loss: 1.2051\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.479 - 0s 893us/step - loss: 0.4445 - val_loss: 1.3458\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.908 - ETA: 0s - loss: 0.532 - 0s 864us/step - loss: 0.4345 - val_loss: 1.4714\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.981 - ETA: 0s - loss: 0.430 - 0s 1ms/step - loss: 0.4352 - val_loss: 1.3669\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.623 - ETA: 0s - loss: 0.387 - 0s 836us/step - loss: 0.4260 - val_loss: 1.2676\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.521 - ETA: 0s - loss: 0.394 - 0s 865us/step - loss: 0.4174 - val_loss: 1.6578\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.751 - ETA: 0s - loss: 0.432 - 0s 855us/step - loss: 0.4167 - val_loss: 1.5172\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.445 - 0s 865us/step - loss: 0.4113 - val_loss: 1.5846\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.417 - 0s 864us/step - loss: 0.4093 - val_loss: 1.7248\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.969 - ETA: 0s - loss: 0.322 - 0s 874us/step - loss: 0.3952 - val_loss: 1.6449\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.387 - 0s 902us/step - loss: 0.4133 - val_loss: 1.7868\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.529 - ETA: 0s - loss: 0.423 - 0s 864us/step - loss: 0.3887 - val_loss: 1.8854\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.350 - 0s 845us/step - loss: 0.3872 - val_loss: 1.7409\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.361 - 0s 864us/step - loss: 0.3947 - val_loss: 1.5537\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.499 - ETA: 0s - loss: 0.386 - 0s 845us/step - loss: 0.3929 - val_loss: 1.8082\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.819 - ETA: 0s - loss: 0.427 - 0s 912us/step - loss: 0.3815 - val_loss: 1.8540\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.336 - 0s 855us/step - loss: 0.3798 - val_loss: 1.7491\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.249 - ETA: 0s - loss: 0.367 - 0s 817us/step - loss: 0.3745 - val_loss: 1.8529\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.371 - 0s 798us/step - loss: 0.3698 - val_loss: 1.9430\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.598 - ETA: 0s - loss: 0.487 - 0s 1ms/step - loss: 0.3762 - val_loss: 1.8656\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.662 - ETA: 0s - loss: 0.346 - 0s 817us/step - loss: 0.3674 - val_loss: 2.0593\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.406 - ETA: 0s - loss: 0.333 - 0s 874us/step - loss: 0.3663 - val_loss: 1.8574\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.369 - 0s 912us/step - loss: 0.3594 - val_loss: 1.9620\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.540 - ETA: 0s - loss: 0.306 - 0s 883us/step - loss: 0.3579 - val_loss: 1.9983\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 17\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.23 - ETA: 0s - loss: 1.0436 - 1s 9ms/step - loss: 0.8440 - val_loss: 0.2393\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.805 - ETA: 0s - loss: 0.853 - 0s 874us/step - loss: 0.7248 - val_loss: 0.4875\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.576 - 0s 874us/step - loss: 0.7028 - val_loss: 0.5881\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.645 - 0s 893us/step - loss: 0.6494 - val_loss: 0.6136\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.492 - 0s 874us/step - loss: 0.6217 - val_loss: 0.6189\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.099 - ETA: 0s - loss: 0.644 - 0s 912us/step - loss: 0.6123 - val_loss: 0.7246\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.639 - 0s 855us/step - loss: 0.6007 - val_loss: 0.8035\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.658 - 0s 997us/step - loss: 0.5684 - val_loss: 0.7401\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.505 - ETA: 0s - loss: 0.608 - 0s 874us/step - loss: 0.5510 - val_loss: 0.8590\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.790 - ETA: 0s - loss: 0.545 - 0s 884us/step - loss: 0.5223 - val_loss: 0.9027\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.647 - ETA: 0s - loss: 0.567 - 0s 855us/step - loss: 0.5312 - val_loss: 0.9434\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.908 - ETA: 0s - loss: 0.532 - 0s 855us/step - loss: 0.5201 - val_loss: 0.9320\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.468 - 0s 893us/step - loss: 0.5202 - val_loss: 1.0968\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.515 - ETA: 0s - loss: 0.550 - 0s 940us/step - loss: 0.5023 - val_loss: 1.0439\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.542 - 0s 912us/step - loss: 0.5032 - val_loss: 1.0764\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.500 - 0s 846us/step - loss: 0.5060 - val_loss: 1.0492\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.510 - 0s 855us/step - loss: 0.4784 - val_loss: 1.1867\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.590 - 0s 884us/step - loss: 0.4841 - val_loss: 1.1390\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.045 - ETA: 0s - loss: 0.461 - 0s 912us/step - loss: 0.4765 - val_loss: 1.1292\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.356 - ETA: 0s - loss: 0.551 - 0s 836us/step - loss: 0.4762 - val_loss: 1.3578\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.410 - 0s 902us/step - loss: 0.4760 - val_loss: 1.1770\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.356 - ETA: 0s - loss: 0.438 - 0s 836us/step - loss: 0.4687 - val_loss: 1.1620\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.495 - 0s 874us/step - loss: 0.4386 - val_loss: 1.3365\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.640 - ETA: 0s - loss: 0.447 - 0s 826us/step - loss: 0.4458 - val_loss: 1.3239\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.481 - 0s 855us/step - loss: 0.4420 - val_loss: 1.3455\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.447 - ETA: 0s - loss: 0.395 - 0s 817us/step - loss: 0.4566 - val_loss: 1.2636\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.459 - 0s 864us/step - loss: 0.4386 - val_loss: 1.4624\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.741 - ETA: 0s - loss: 0.362 - 0s 950us/step - loss: 0.4190 - val_loss: 1.4498\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.522 - 0s 836us/step - loss: 0.4492 - val_loss: 1.3301\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.553 - ETA: 0s - loss: 0.463 - 0s 2ms/step - loss: 0.4430 - val_loss: 1.4244\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.416 - 0s 817us/step - loss: 0.4245 - val_loss: 1.4384\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 18\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.25 - ETA: 0s - loss: 0.8730 - 1s 9ms/step - loss: 0.8775 - val_loss: 0.2001\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.731 - 0s 931us/step - loss: 0.6540 - val_loss: 0.4996\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.539 - 0s 883us/step - loss: 0.5817 - val_loss: 0.5208\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.479 - 0s 864us/step - loss: 0.5284 - val_loss: 0.6013\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.543 - 0s 864us/step - loss: 0.4777 - val_loss: 0.7410\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.221 - ETA: 0s - loss: 0.436 - 0s 931us/step - loss: 0.4605 - val_loss: 0.6788\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.396 - ETA: 0s - loss: 0.418 - 0s 855us/step - loss: 0.4493 - val_loss: 0.7474\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.468 - 0s 846us/step - loss: 0.4076 - val_loss: 0.7382\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.242 - 0s 902us/step - loss: 0.4150 - val_loss: 0.7044\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.453 - 0s 902us/step - loss: 0.3903 - val_loss: 0.7942\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.268 - 0s 864us/step - loss: 0.3636 - val_loss: 0.8167\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.317 - 0s 883us/step - loss: 0.3598 - val_loss: 0.8431\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.257 - 0s 978us/step - loss: 0.3602 - val_loss: 0.9739\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.377 - 0s 902us/step - loss: 0.3362 - val_loss: 0.8286\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.430 - ETA: 0s - loss: 0.355 - 0s 902us/step - loss: 0.3331 - val_loss: 0.8126\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.737 - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.329 - 0s 3ms/step - loss: 0.3420 - val_loss: 0.8650\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.267 - ETA: 0s - loss: 0.313 - 0s 1ms/step - loss: 0.3296 - val_loss: 0.8816\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.381 - 0s 826us/step - loss: 0.3228 - val_loss: 0.8824\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.243 - 0s 712us/step - loss: 0.3096 - val_loss: 0.8646\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.625 - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.307 - 0s 2ms/step - loss: 0.3122 - val_loss: 0.8815\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.560 - ETA: 0s - loss: 0.219 - 0s 1ms/step - loss: 0.3029 - val_loss: 0.9527\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.323 - 0s 940us/step - loss: 0.2993 - val_loss: 0.9149\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.339 - 0s 921us/step - loss: 0.3078 - val_loss: 0.9230\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.365 - 0s 864us/step - loss: 0.3037 - val_loss: 0.9086\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.486 - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.277 - 0s 2ms/step - loss: 0.3090 - val_loss: 0.9013\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.340 - 0s 940us/step - loss: 0.2970 - val_loss: 0.9231\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.288 - 0s 846us/step - loss: 0.2912 - val_loss: 1.0072\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.335 - ETA: 0s - loss: 0.352 - 0s 846us/step - loss: 0.2981 - val_loss: 0.9503\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.298 - 0s 845us/step - loss: 0.2796 - val_loss: 1.0526\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.474 - ETA: 0s - loss: 0.238 - 0s 845us/step - loss: 0.2846 - val_loss: 0.9446\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.232 - 0s 874us/step - loss: 0.2677 - val_loss: 0.9185\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 19\n",
      "Model: \"sequential_1\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.57 - ETA: 0s - loss: 0.7332 - 1s 9ms/step - loss: 0.7630 - val_loss: 0.4169\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.788 - ETA: 0s - loss: 0.556 - 0s 826us/step - loss: 0.4958 - val_loss: 1.0881\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.487 - 0s 903us/step - loss: 0.4418 - val_loss: 1.4887\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.768 - ETA: 0s - loss: 0.335 - 0s 836us/step - loss: 0.4160 - val_loss: 1.2662\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.391 - 0s 883us/step - loss: 0.3729 - val_loss: 1.5151\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.541 - ETA: 0s - loss: 0.372 - 0s 826us/step - loss: 0.3581 - val_loss: 1.5413\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.363 - 0s 817us/step - loss: 0.3546 - val_loss: 1.3573\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.926 - ETA: 0s - loss: 0.403 - 0s 893us/step - loss: 0.3612 - val_loss: 1.5621\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.373 - 0s 874us/step - loss: 0.3307 - val_loss: 1.3872\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.330 - 0s 883us/step - loss: 0.3205 - val_loss: 1.4737\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.298 - 0s 874us/step - loss: 0.3004 - val_loss: 1.6739\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.724 - ETA: 0s - loss: 0.356 - 0s 893us/step - loss: 0.3300 - val_loss: 1.3631\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.288 - 0s 988us/step - loss: 0.2999 - val_loss: 1.5017\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.284 - 0s 864us/step - loss: 0.2991 - val_loss: 1.3897\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.293 - 0s 864us/step - loss: 0.2891 - val_loss: 1.4918\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.324 - 0s 817us/step - loss: 0.2925 - val_loss: 1.4263\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.455 - ETA: 0s - loss: 0.291 - 0s 864us/step - loss: 0.2895 - val_loss: 1.5457\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.326 - 0s 864us/step - loss: 0.3032 - val_loss: 1.5145\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.290 - 0s 884us/step - loss: 0.2900 - val_loss: 1.3869\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.222 - 0s 893us/step - loss: 0.2808 - val_loss: 1.4145\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.210 - 0s 883us/step - loss: 0.2846 - val_loss: 1.4649\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.237 - 0s 883us/step - loss: 0.2936 - val_loss: 1.5352\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.285 - 0s 959us/step - loss: 0.2672 - val_loss: 1.2083\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.434 - ETA: 0s - loss: 0.246 - 0s 855us/step - loss: 0.2774 - val_loss: 1.3461\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.462 - ETA: 0s - loss: 0.286 - 0s 883us/step - loss: 0.2725 - val_loss: 1.5144\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.256 - 0s 874us/step - loss: 0.2859 - val_loss: 1.4026\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.260 - 0s 855us/step - loss: 0.2738 - val_loss: 1.6147\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.222 - 0s 921us/step - loss: 0.2637 - val_loss: 1.4284\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.223 - 0s 845us/step - loss: 0.2684 - val_loss: 1.4058\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.305 - 0s 941us/step - loss: 0.2874 - val_loss: 1.1739\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.526 - ETA: 0s - loss: 0.272 - 0s 864us/step - loss: 0.2575 - val_loss: 1.5474\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 20\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.39 - ETA: 0s - loss: 0.5552 - 1s 9ms/step - loss: 0.8686 - val_loss: 0.1756\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.644 - 0s 855us/step - loss: 0.6857 - val_loss: 0.4347\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.860 - ETA: 0s - loss: 0.734 - 0s 817us/step - loss: 0.6376 - val_loss: 0.6017\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.435 - 0s 864us/step - loss: 0.6270 - val_loss: 0.5260\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.437 - 0s 874us/step - loss: 0.5979 - val_loss: 0.5097\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.730 - 0s 931us/step - loss: 0.5740 - val_loss: 0.6125\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.380 - 0s 845us/step - loss: 0.5513 - val_loss: 0.5654\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.495 - ETA: 0s - loss: 0.605 - 0s 950us/step - loss: 0.5219 - val_loss: 0.6567\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.523 - 0s 969us/step - loss: 0.5252 - val_loss: 0.6110\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.530 - 0s 893us/step - loss: 0.5085 - val_loss: 0.6480\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.362 - 0s 959us/step - loss: 0.4902 - val_loss: 0.6112\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.462 - 0s 883us/step - loss: 0.4858 - val_loss: 0.6948\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.480 - 0s 884us/step - loss: 0.4880 - val_loss: 0.8265\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.475 - 0s 788us/step - loss: 0.4634 - val_loss: 0.8370\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.371 - 0s 883us/step - loss: 0.4661 - val_loss: 0.8991\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.076 - ETA: 0s - loss: 0.350 - 0s 817us/step - loss: 0.4405 - val_loss: 0.9676\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.372 - 0s 760us/step - loss: 0.4350 - val_loss: 1.0592\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.410 - 0s 779us/step - loss: 0.4642 - val_loss: 1.3861\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.527 - ETA: 0s - loss: 0.428 - 0s 817us/step - loss: 0.4294 - val_loss: 1.3870\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.493 - 0s 789us/step - loss: 0.4443 - val_loss: 1.4225\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.458 - ETA: 0s - loss: 0.471 - 0s 789us/step - loss: 0.4376 - val_loss: 1.4526\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.363 - 0s 940us/step - loss: 0.4498 - val_loss: 1.4127\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.612 - ETA: 0s - loss: 0.418 - 0s 817us/step - loss: 0.4290 - val_loss: 1.4850\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.461 - 0s 817us/step - loss: 0.4322 - val_loss: 1.4119\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.812 - ETA: 0s - loss: 0.289 - 0s 855us/step - loss: 0.4187 - val_loss: 1.3829\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.450 - 0s 845us/step - loss: 0.4212 - val_loss: 1.5431\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.361 - ETA: 0s - loss: 0.400 - 0s 864us/step - loss: 0.4094 - val_loss: 1.5272\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.822 - ETA: 0s - loss: 0.435 - 0s 817us/step - loss: 0.3971 - val_loss: 1.5987\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.400 - 0s 826us/step - loss: 0.4079 - val_loss: 1.6802\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.337 - 0s 826us/step - loss: 0.4031 - val_loss: 1.5974\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.507 - ETA: 0s - loss: 0.432 - 0s 836us/step - loss: 0.4113 - val_loss: 1.7453\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 21\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.70 - ETA: 0s - loss: 0.6905 - 1s 9ms/step - loss: 0.7427 - val_loss: 0.3903\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.871 - ETA: 0s - loss: 0.543 - 0s 931us/step - loss: 0.6064 - val_loss: 0.8041\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.583 - 0s 836us/step - loss: 0.5929 - val_loss: 0.9489\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.606 - 0s 817us/step - loss: 0.5782 - val_loss: 0.9951\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.538 - ETA: 0s - loss: 0.573 - 0s 893us/step - loss: 0.5661 - val_loss: 0.8800\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.411 - ETA: 0s - loss: 0.460 - 0s 883us/step - loss: 0.5623 - val_loss: 0.8895\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.486 - ETA: 0s - loss: 0.518 - 0s 893us/step - loss: 0.5599 - val_loss: 0.8201\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.614 - ETA: 0s - loss: 0.559 - 0s 855us/step - loss: 0.5481 - val_loss: 0.8907\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.014 - ETA: 0s - loss: 0.507 - 0s 864us/step - loss: 0.5406 - val_loss: 0.9601\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.782 - ETA: 0s - loss: 0.495 - 0s 893us/step - loss: 0.5343 - val_loss: 0.8565\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.486 - 0s 883us/step - loss: 0.5258 - val_loss: 0.9918\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.356 - 0s 912us/step - loss: 0.5278 - val_loss: 0.9089\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.492 - 0s 931us/step - loss: 0.5237 - val_loss: 0.8192\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.445 - ETA: 0s - loss: 0.582 - 0s 864us/step - loss: 0.5303 - val_loss: 0.8991\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.416 - ETA: 0s - loss: 0.569 - 0s 836us/step - loss: 0.5107 - val_loss: 0.8300\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.592 - 0s 921us/step - loss: 0.5051 - val_loss: 0.8511\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.456 - 0s 845us/step - loss: 0.5025 - val_loss: 0.8199\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.924 - ETA: 0s - loss: 0.612 - 0s 903us/step - loss: 0.4995 - val_loss: 0.9009\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.492 - ETA: 0s - loss: 0.520 - 0s 864us/step - loss: 0.4877 - val_loss: 0.7739\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - ETA: 0s - loss: 0.479 - 0s 864us/step - loss: 0.4653 - val_loss: 0.8244\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.484 - 0s 921us/step - loss: 0.4854 - val_loss: 0.9513\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.521 - 0s 912us/step - loss: 0.4733 - val_loss: 0.8487\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.388 - ETA: 0s - loss: 0.474 - 0s 845us/step - loss: 0.4596 - val_loss: 0.8836\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.533 - 0s 807us/step - loss: 0.4646 - val_loss: 0.7739\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.559 - 0s 836us/step - loss: 0.4755 - val_loss: 0.8268\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.423 - 0s 884us/step - loss: 0.4448 - val_loss: 0.8926\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.454 - 0s 884us/step - loss: 0.4579 - val_loss: 0.8239\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.842 - ETA: 0s - loss: 0.429 - 0s 846us/step - loss: 0.4454 - val_loss: 0.8817\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.463 - 0s 845us/step - loss: 0.4405 - val_loss: 0.7815\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.379 - 0s 874us/step - loss: 0.4266 - val_loss: 0.8703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.466 - 0s 845us/step - loss: 0.4308 - val_loss: 0.8477\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 22\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.30 - ETA: 0s - loss: 0.7937 - 1s 9ms/step - loss: 0.8339 - val_loss: 0.2038\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 1.154 - 0s 1ms/step - loss: 0.6740 - val_loss: 0.5138\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.647 - ETA: 0s - loss: 1.120 - ETA: 0s - loss: 0.695 - 0s 1ms/step - loss: 0.6273 - val_loss: 0.5736\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.283 - 0s 950us/step - loss: 0.6212 - val_loss: 0.5985\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.725 - 0s 1ms/step - loss: 0.6013 - val_loss: 0.6623\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.636 - 0s 1ms/step - loss: 0.5918 - val_loss: 0.5929\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.249 - ETA: 0s - loss: 0.858 - 0s 1ms/step - loss: 0.5705 - val_loss: 0.6421\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.305 - ETA: 0s - loss: 0.619 - 0s 1ms/step - loss: 0.5763 - val_loss: 0.6021\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.348 - 0s 912us/step - loss: 0.5513 - val_loss: 0.5767\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.706 - 0s 1ms/step - loss: 0.5515 - val_loss: 0.8198\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.482 - ETA: 0s - loss: 0.640 - 0s 1ms/step - loss: 0.5203 - val_loss: 0.5848\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.327 - 0s 1ms/step - loss: 0.5088 - val_loss: 0.6281\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.234 - ETA: 0s - loss: 0.553 - 0s 912us/step - loss: 0.5097 - val_loss: 0.8447\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.562 - 0s 864us/step - loss: 0.5041 - val_loss: 0.6191\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.228 - 0s 1ms/step - loss: 0.4653 - val_loss: 0.7955\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.588 - 0s 864us/step - loss: 0.4893 - val_loss: 0.7674\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.295 - 0s 883us/step - loss: 0.4710 - val_loss: 0.7314\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.614 - ETA: 0s - loss: 0.491 - 0s 827us/step - loss: 0.4677 - val_loss: 1.0235\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.509 - 0s 902us/step - loss: 0.4402 - val_loss: 0.9488\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.345 - 0s 826us/step - loss: 0.4787 - val_loss: 0.9844\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.452 - 0s 893us/step - loss: 0.4312 - val_loss: 1.0492\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.432 - 0s 883us/step - loss: 0.4005 - val_loss: 1.0364\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.681 - ETA: 0s - loss: 0.518 - 0s 874us/step - loss: 0.4116 - val_loss: 1.0656\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.511 - 0s 864us/step - loss: 0.4219 - val_loss: 1.3218\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.475 - 0s 874us/step - loss: 0.4190 - val_loss: 1.2525\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.407 - 0s 884us/step - loss: 0.3979 - val_loss: 1.1894\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.512 - 0s 912us/step - loss: 0.4134 - val_loss: 1.2121\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.363 - 0s 893us/step - loss: 0.3995 - val_loss: 1.2371\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.472 - ETA: 0s - loss: 0.360 - 0s 836us/step - loss: 0.3745 - val_loss: 1.4003\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.458 - 0s 883us/step - loss: 0.3865 - val_loss: 1.5390\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.367 - 0s 874us/step - loss: 0.3646 - val_loss: 1.5438\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 23\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.34 - ETA: 0s - loss: 0.7835 - 1s 9ms/step - loss: 0.8109 - val_loss: 0.2888\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.483 - 0s 893us/step - loss: 0.5076 - val_loss: 0.8223\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.508 - ETA: 0s - loss: 0.369 - 0s 893us/step - loss: 0.4407 - val_loss: 1.4525\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.328 - 0s 893us/step - loss: 0.4079 - val_loss: 1.6454\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.401 - 0s 855us/step - loss: 0.3847 - val_loss: 2.1182\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.768 - ETA: 0s - loss: 0.394 - 0s 940us/step - loss: 0.3554 - val_loss: 1.8719\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.780 - ETA: 0s - loss: 0.282 - 0s 902us/step - loss: 0.3257 - val_loss: 2.1084\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.247 - 0s 883us/step - loss: 0.3225 - val_loss: 2.1475\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.310 - 0s 950us/step - loss: 0.2838 - val_loss: 2.3480\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.301 - 0s 902us/step - loss: 0.2854 - val_loss: 2.4532\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.940 - ETA: 0s - loss: 0.281 - 0s 940us/step - loss: 0.2688 - val_loss: 2.4373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.277 - 0s 845us/step - loss: 0.2715 - val_loss: 2.4858\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.296 - 0s 845us/step - loss: 0.2635 - val_loss: 2.4984\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.231 - 0s 845us/step - loss: 0.2664 - val_loss: 2.6918\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.417 - ETA: 0s - loss: 0.253 - 0s 798us/step - loss: 0.2596 - val_loss: 2.5281\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.222 - 0s 817us/step - loss: 0.2477 - val_loss: 2.4775\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.227 - 0s 798us/step - loss: 0.2537 - val_loss: 2.8414\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.483 - ETA: 0s - loss: 0.225 - 0s 817us/step - loss: 0.2605 - val_loss: 2.2687\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.231 - 0s 864us/step - loss: 0.2516 - val_loss: 2.4833\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.219 - 0s 912us/step - loss: 0.2502 - val_loss: 2.4595\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.272 - 0s 817us/step - loss: 0.2425 - val_loss: 2.7586\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.335 - ETA: 0s - loss: 0.184 - 0s 836us/step - loss: 0.2312 - val_loss: 2.7324\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.205 - 0s 826us/step - loss: 0.2147 - val_loss: 2.6141\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.195 - 0s 807us/step - loss: 0.2199 - val_loss: 2.6500\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.193 - 0s 826us/step - loss: 0.2361 - val_loss: 3.0300\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.215 - 0s 817us/step - loss: 0.2188 - val_loss: 3.0045\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.215 - 0s 855us/step - loss: 0.2127 - val_loss: 2.2628\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.247 - 0s 826us/step - loss: 0.2353 - val_loss: 2.9561\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.200 - 0s 893us/step - loss: 0.1936 - val_loss: 2.7409\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.207 - 0s 940us/step - loss: 0.1927 - val_loss: 2.7171\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.210 - 0s 855us/step - loss: 0.1940 - val_loss: 2.7423\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 24\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.28 - ETA: 0s - loss: 0.6711 - 1s 9ms/step - loss: 0.7953 - val_loss: 0.3774\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.419 - ETA: 0s - loss: 0.742 - 0s 836us/step - loss: 0.6093 - val_loss: 1.0666\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.635 - ETA: 0s - loss: 0.477 - 0s 836us/step - loss: 0.5417 - val_loss: 1.1221\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.604 - 0s 826us/step - loss: 0.5318 - val_loss: 1.3900\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.525 - 0s 808us/step - loss: 0.4711 - val_loss: 1.3835\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.564 - 0s 827us/step - loss: 0.4521 - val_loss: 1.3604\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.456 - 0s 807us/step - loss: 0.4208 - val_loss: 1.3500\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.467 - ETA: 0s - loss: 0.312 - 0s 836us/step - loss: 0.3975 - val_loss: 1.4954\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.461 - 0s 893us/step - loss: 0.3832 - val_loss: 1.4028\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.529 - ETA: 0s - loss: 0.421 - 0s 921us/step - loss: 0.3791 - val_loss: 1.4343\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.030 - ETA: 0s - loss: 0.410 - 0s 931us/step - loss: 0.3981 - val_loss: 1.5324\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.320 - 0s 883us/step - loss: 0.3531 - val_loss: 1.3636\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.401 - 0s 874us/step - loss: 0.3648 - val_loss: 1.6258\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.390 - 0s 836us/step - loss: 0.3337 - val_loss: 1.3763\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.295 - 0s 922us/step - loss: 0.3293 - val_loss: 1.5313\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.274 - 0s 921us/step - loss: 0.3241 - val_loss: 1.3884\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.200 - 0s 893us/step - loss: 0.3115 - val_loss: 1.5145\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.246 - 0s 959us/step - loss: 0.3172 - val_loss: 1.1628\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.350 - 0s 902us/step - loss: 0.3172 - val_loss: 1.4776\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.331 - ETA: 0s - loss: 0.342 - 0s 940us/step - loss: 0.2922 - val_loss: 1.4655\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.240 - 0s 940us/step - loss: 0.2894 - val_loss: 1.4340\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.283 - 0s 921us/step - loss: 0.2789 - val_loss: 1.3267\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.189 - 0s 893us/step - loss: 0.2621 - val_loss: 1.3187\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.493 - ETA: 0s - loss: 0.257 - 0s 893us/step - loss: 0.2592 - val_loss: 1.5757\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.282 - 0s 893us/step - loss: 0.2593 - val_loss: 1.3544\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.258 - 0s 940us/step - loss: 0.2442 - val_loss: 1.3946\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.233 - 0s 950us/step - loss: 0.2293 - val_loss: 1.5930\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.167 - 0s 874us/step - loss: 0.2510 - val_loss: 1.6422\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.165 - 0s 912us/step - loss: 0.2368 - val_loss: 1.5537\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.238 - 0s 845us/step - loss: 0.2184 - val_loss: 1.4754\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.157 - 0s 912us/step - loss: 0.2221 - val_loss: 1.7487\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 25\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.33 - ETA: 0s - loss: 1.0815 - 1s 9ms/step - loss: 0.9396 - val_loss: 0.1441\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.352 - ETA: 0s - loss: 0.578 - 0s 864us/step - loss: 0.7422 - val_loss: 0.2416\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.804 - 0s 808us/step - loss: 0.6778 - val_loss: 0.3659\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.456 - 0s 855us/step - loss: 0.5997 - val_loss: 0.4113\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.748 - 0s 864us/step - loss: 0.5751 - val_loss: 0.4893\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.424 - 0s 845us/step - loss: 0.5082 - val_loss: 0.4064\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.456 - ETA: 0s - loss: 0.581 - 0s 836us/step - loss: 0.4801 - val_loss: 0.5602\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.453 - 0s 817us/step - loss: 0.4549 - val_loss: 0.6037\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.490 - ETA: 0s - loss: 0.429 - 0s 902us/step - loss: 0.4322 - val_loss: 0.5804\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.273 - 0s 1ms/step - loss: 0.3805 - val_loss: 0.6459\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.335 - 0s 826us/step - loss: 0.3362 - val_loss: 0.7732\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.363 - 0s 808us/step - loss: 0.3280 - val_loss: 0.8190\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.039 - ETA: 0s - loss: 0.325 - 0s 845us/step - loss: 0.2931 - val_loss: 0.8660\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.349 - ETA: 0s - loss: 0.261 - 0s 826us/step - loss: 0.3139 - val_loss: 0.8531\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - ETA: 0s - loss: 0.255 - 0s 845us/step - loss: 0.3056 - val_loss: 0.8342\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.586 - ETA: 0s - loss: 0.273 - 0s 874us/step - loss: 0.2982 - val_loss: 0.8570\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.417 - ETA: 0s - loss: 0.375 - 0s 921us/step - loss: 0.2938 - val_loss: 0.9615\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.244 - 0s 864us/step - loss: 0.2775 - val_loss: 1.0242\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.227 - 0s 912us/step - loss: 0.2695 - val_loss: 1.0142\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.617 - ETA: 0s - loss: 0.270 - 0s 978us/step - loss: 0.2892 - val_loss: 0.9520\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.250 - 0s 912us/step - loss: 0.2327 - val_loss: 1.0377\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.597 - ETA: 0s - loss: 0.257 - 0s 912us/step - loss: 0.2697 - val_loss: 1.1289\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.186 - 0s 883us/step - loss: 0.2499 - val_loss: 1.2618\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.254 - 0s 845us/step - loss: 0.2380 - val_loss: 1.0964\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.202 - 0s 845us/step - loss: 0.2608 - val_loss: 1.0145\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.487 - ETA: 0s - loss: 0.275 - 0s 874us/step - loss: 0.2331 - val_loss: 1.0967\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.256 - 0s 874us/step - loss: 0.2497 - val_loss: 1.1675\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.208 - 0s 883us/step - loss: 0.2102 - val_loss: 1.0891\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.241 - 0s 864us/step - loss: 0.2269 - val_loss: 0.9529\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.145 - 0s 931us/step - loss: 0.2146 - val_loss: 1.0201\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.152 - 0s 940us/step - loss: 0.2012 - val_loss: 1.1180\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 26\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.72 - ETA: 0s - loss: 0.7137 - 1s 9ms/step - loss: 0.7375 - val_loss: 0.2724\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.711 - ETA: 0s - loss: 0.940 - 0s 1ms/step - loss: 0.4914 - val_loss: 0.9853\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.387 - ETA: 0s - loss: 0.405 - 0s 893us/step - loss: 0.4017 - val_loss: 1.4082\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.440 - 0s 846us/step - loss: 0.4029 - val_loss: 1.3178\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.370 - 0s 864us/step - loss: 0.3843 - val_loss: 1.3001\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.350 - 0s 874us/step - loss: 0.3802 - val_loss: 1.6354\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.453 - ETA: 0s - loss: 0.398 - 0s 855us/step - loss: 0.3592 - val_loss: 1.8348\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.342 - 0s 893us/step - loss: 0.3713 - val_loss: 1.9015\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.404 - 0s 940us/step - loss: 0.3582 - val_loss: 1.8592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.778 - ETA: 0s - loss: 0.351 - 0s 884us/step - loss: 0.3521 - val_loss: 1.8997\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 0.343 - 0s 836us/step - loss: 0.3238 - val_loss: 2.1262\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.622 - ETA: 0s - loss: 0.341 - 0s 826us/step - loss: 0.3297 - val_loss: 2.3003\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.768 - ETA: 0s - loss: 0.303 - 0s 826us/step - loss: 0.3160 - val_loss: 2.3152\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.355 - 0s 826us/step - loss: 0.3376 - val_loss: 2.5342\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.328 - 0s 845us/step - loss: 0.3178 - val_loss: 2.4936\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.280 - ETA: 0s - loss: 0.301 - 0s 817us/step - loss: 0.3057 - val_loss: 2.4910\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.481 - ETA: 0s - loss: 0.282 - 0s 827us/step - loss: 0.2975 - val_loss: 2.6961\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.269 - 0s 864us/step - loss: 0.3061 - val_loss: 2.9028\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.594 - ETA: 0s - loss: 0.302 - 0s 883us/step - loss: 0.2979 - val_loss: 2.9695\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.223 - 0s 921us/step - loss: 0.2865 - val_loss: 3.1022\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.289 - 0s 817us/step - loss: 0.2830 - val_loss: 3.0047\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.490 - ETA: 0s - loss: 0.275 - 0s 817us/step - loss: 0.2882 - val_loss: 3.0925\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.277 - 0s 826us/step - loss: 0.2781 - val_loss: 3.4395\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.246 - 0s 798us/step - loss: 0.2552 - val_loss: 3.4009\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.255 - 0s 826us/step - loss: 0.2577 - val_loss: 3.7126\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.292 - 0s 836us/step - loss: 0.2597 - val_loss: 3.3365\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.406 - ETA: 0s - loss: 0.253 - 0s 864us/step - loss: 0.2592 - val_loss: 3.3531\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.369 - ETA: 0s - loss: 0.285 - 0s 836us/step - loss: 0.2624 - val_loss: 3.6115\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.261 - 0s 817us/step - loss: 0.2515 - val_loss: 3.8600\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.238 - 0s 883us/step - loss: 0.2491 - val_loss: 3.8409\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.250 - 0s 921us/step - loss: 0.2377 - val_loss: 3.6840\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 27\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.25 - ETA: 0s - loss: 0.8275 - 1s 9ms/step - loss: 0.8067 - val_loss: 0.2799\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.662 - 0s 864us/step - loss: 0.6552 - val_loss: 0.6680\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.445 - ETA: 0s - loss: 0.545 - 0s 826us/step - loss: 0.6110 - val_loss: 0.7682\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.475 - ETA: 0s - loss: 0.572 - 0s 902us/step - loss: 0.5904 - val_loss: 0.6902\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.513 - 0s 855us/step - loss: 0.5755 - val_loss: 0.6786\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.259 - ETA: 0s - loss: 0.566 - 0s 921us/step - loss: 0.5445 - val_loss: 0.7514\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.633 - ETA: 0s - loss: 0.585 - 0s 959us/step - loss: 0.5404 - val_loss: 0.7107\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.437 - 0s 893us/step - loss: 0.5220 - val_loss: 0.7365\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.892 - ETA: 0s - loss: 0.431 - 0s 855us/step - loss: 0.5397 - val_loss: 0.5699\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.502 - 0s 969us/step - loss: 0.5085 - val_loss: 0.7095\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.622 - ETA: 0s - loss: 0.488 - 0s 931us/step - loss: 0.4988 - val_loss: 0.6547\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.567 - ETA: 0s - loss: 0.591 - 0s 826us/step - loss: 0.5126 - val_loss: 0.7341\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.409 - ETA: 0s - loss: 0.548 - 0s 855us/step - loss: 0.4982 - val_loss: 0.8039\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.504 - 0s 864us/step - loss: 0.4738 - val_loss: 0.7218\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.445 - 0s 845us/step - loss: 0.4816 - val_loss: 0.7768\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.307 - ETA: 0s - loss: 0.489 - 0s 902us/step - loss: 0.4780 - val_loss: 0.6785\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.445 - 0s 855us/step - loss: 0.4680 - val_loss: 0.6486\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.592 - ETA: 0s - loss: 0.335 - 0s 893us/step - loss: 0.4611 - val_loss: 0.6458\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.747 - ETA: 0s - loss: 0.507 - 0s 912us/step - loss: 0.4558 - val_loss: 0.7503\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.444 - ETA: 0s - loss: 0.413 - 0s 1ms/step - loss: 0.4574 - val_loss: 0.7756\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.388 - 0s 921us/step - loss: 0.4543 - val_loss: 0.8267\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.480 - 0s 902us/step - loss: 0.4456 - val_loss: 0.7004\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.383 - 0s 902us/step - loss: 0.4398 - val_loss: 0.6633\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.408 - ETA: 0s - loss: 0.365 - 0s 988us/step - loss: 0.4227 - val_loss: 0.7570\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.414 - 0s 883us/step - loss: 0.4439 - val_loss: 0.7688\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.437 - 0s 959us/step - loss: 0.4253 - val_loss: 0.9543\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.388 - 0s 921us/step - loss: 0.4246 - val_loss: 0.7493\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.499 - ETA: 0s - loss: 0.459 - 0s 921us/step - loss: 0.4306 - val_loss: 0.8666\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.498 - ETA: 0s - loss: 0.370 - 0s 855us/step - loss: 0.4013 - val_loss: 0.9797\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.469 - 0s 883us/step - loss: 0.4061 - val_loss: 0.9594\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.436 - 0s 874us/step - loss: 0.4133 - val_loss: 1.0539\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 28\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 3.86 - ETA: 0s - loss: 0.6768 - 1s 9ms/step - loss: 0.7230 - val_loss: 0.3655\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.386 - 0s 1ms/step - loss: 0.5206 - val_loss: 0.7721\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.397 - ETA: 0s - loss: 0.484 - 0s 817us/step - loss: 0.4748 - val_loss: 1.3604\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.449 - ETA: 0s - loss: 0.391 - 0s 1ms/step - loss: 0.4041 - val_loss: 1.3377\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.306 - 0s 874us/step - loss: 0.4051 - val_loss: 1.5403\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.379 - 0s 893us/step - loss: 0.3749 - val_loss: 1.6606\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.632 - ETA: 0s - loss: 0.383 - 0s 883us/step - loss: 0.3297 - val_loss: 1.8187\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.493 - ETA: 0s - loss: 0.326 - 0s 902us/step - loss: 0.3204 - val_loss: 2.0380\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.323 - 0s 893us/step - loss: 0.2857 - val_loss: 1.8955\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.567 - ETA: 0s - loss: 0.272 - 0s 940us/step - loss: 0.3010 - val_loss: 1.6730\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.323 - 0s 883us/step - loss: 0.2973 - val_loss: 2.6346\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.329 - 0s 912us/step - loss: 0.2880 - val_loss: 2.0275\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.268 - 0s 893us/step - loss: 0.2578 - val_loss: 2.1103\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.255 - ETA: 0s - loss: 0.270 - 0s 893us/step - loss: 0.2509 - val_loss: 2.3676\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.202 - 0s 864us/step - loss: 0.2443 - val_loss: 2.2546\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.279 - 0s 855us/step - loss: 0.2418 - val_loss: 2.5486\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.215 - 0s 874us/step - loss: 0.2469 - val_loss: 2.5453\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.223 - 0s 883us/step - loss: 0.2297 - val_loss: 2.4095\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.235 - 0s 864us/step - loss: 0.2451 - val_loss: 2.4988\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.221 - 0s 921us/step - loss: 0.2490 - val_loss: 2.3591\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.594 - ETA: 0s - loss: 0.232 - 0s 836us/step - loss: 0.2388 - val_loss: 2.1179\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.200 - 0s 912us/step - loss: 0.2300 - val_loss: 2.6416\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.217 - 0s 902us/step - loss: 0.2212 - val_loss: 2.3156\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.231 - 0s 912us/step - loss: 0.2315 - val_loss: 2.5820\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.230 - 0s 893us/step - loss: 0.2108 - val_loss: 2.5369\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.221 - 0s 874us/step - loss: 0.2226 - val_loss: 2.5671\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.213 - 0s 836us/step - loss: 0.2114 - val_loss: 2.5503\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.177 - 0s 997us/step - loss: 0.2224 - val_loss: 2.4360\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.221 - 0s 874us/step - loss: 0.2123 - val_loss: 2.6191\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.165 - 0s 950us/step - loss: 0.2122 - val_loss: 2.6577\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.241 - 0s 969us/step - loss: 0.2112 - val_loss: 2.2927\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 29\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.48 - ETA: 0s - loss: 0.8565 - 1s 9ms/step - loss: 0.6996 - val_loss: 0.5052\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.781 - ETA: 0s - loss: 0.586 - 0s 921us/step - loss: 0.5004 - val_loss: 1.0230\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.675 - ETA: 0s - loss: 0.542 - 0s 1ms/step - loss: 0.4484 - val_loss: 1.3305\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.665 - ETA: 0s - loss: 0.440 - 0s 864us/step - loss: 0.3890 - val_loss: 1.6712\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.313 - 0s 978us/step - loss: 0.3713 - val_loss: 1.6489\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.367 - 0s 950us/step - loss: 0.3600 - val_loss: 1.7039\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.383 - 0s 902us/step - loss: 0.3460 - val_loss: 1.7729\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.647 - ETA: 0s - loss: 0.311 - 0s 902us/step - loss: 0.3351 - val_loss: 1.9598\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.676 - ETA: 0s - loss: 0.427 - 0s 874us/step - loss: 0.3495 - val_loss: 1.6439\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.346 - 0s 826us/step - loss: 0.3128 - val_loss: 1.7800\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.317 - 0s 826us/step - loss: 0.3057 - val_loss: 1.8546\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.321 - 0s 873us/step - loss: 0.3181 - val_loss: 1.9344\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.305 - 0s 826us/step - loss: 0.3199 - val_loss: 2.3603\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.243 - 0s 864us/step - loss: 0.2993 - val_loss: 1.9149\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.312 - 0s 845us/step - loss: 0.2979 - val_loss: 2.3110\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.293 - 0s 883us/step - loss: 0.2872 - val_loss: 2.0220\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.271 - 0s 912us/step - loss: 0.2997 - val_loss: 2.2242\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.292 - 0s 2ms/step - loss: 0.3058 - val_loss: 2.0747\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.304 - 0s 826us/step - loss: 0.2903 - val_loss: 2.1210\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.248 - 0s 817us/step - loss: 0.2909 - val_loss: 2.0159\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.260 - 0s 836us/step - loss: 0.2825 - val_loss: 2.1322\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.638 - ETA: 0s - loss: 0.290 - 0s 826us/step - loss: 0.2749 - val_loss: 2.0154\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.274 - 0s 845us/step - loss: 0.2760 - val_loss: 2.1919\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.329 - 0s 807us/step - loss: 0.2823 - val_loss: 2.1141\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.305 - 0s 788us/step - loss: 0.2777 - val_loss: 2.1226\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.227 - 0s 883us/step - loss: 0.2809 - val_loss: 2.0412\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.284 - 0s 788us/step - loss: 0.2637 - val_loss: 2.1481\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.289 - 0s 931us/step - loss: 0.2911 - val_loss: 1.9490\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.259 - 0s 1ms/step - loss: 0.2563 - val_loss: 2.1916\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.453 - ETA: 0s - loss: 0.261 - 0s 845us/step - loss: 0.2553 - val_loss: 2.0672\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.244 - 0s 883us/step - loss: 0.2524 - val_loss: 2.2154\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 30\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.90 - ETA: 0s - loss: 0.5957 - 1s 9ms/step - loss: 0.7079 - val_loss: 0.4038\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.507 - 0s 836us/step - loss: 0.5153 - val_loss: 0.8594\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.460 - 0s 836us/step - loss: 0.4588 - val_loss: 0.9580\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.353 - 0s 893us/step - loss: 0.4334 - val_loss: 0.9668\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.399 - 0s 817us/step - loss: 0.4036 - val_loss: 0.8885\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.434 - 0s 874us/step - loss: 0.3797 - val_loss: 0.8528\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.412 - ETA: 0s - loss: 0.386 - 0s 912us/step - loss: 0.3557 - val_loss: 0.8456\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.482 - ETA: 0s - loss: 0.226 - 0s 902us/step - loss: 0.3528 - val_loss: 0.7773\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.350 - 0s 959us/step - loss: 0.3292 - val_loss: 0.9162\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.280 - 0s 865us/step - loss: 0.3147 - val_loss: 0.9042\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.336 - 0s 855us/step - loss: 0.3168 - val_loss: 0.8608\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.331 - 0s 864us/step - loss: 0.3013 - val_loss: 0.8582\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.327 - 0s 903us/step - loss: 0.3041 - val_loss: 0.9625\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.248 - 0s 883us/step - loss: 0.2951 - val_loss: 0.9609\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.253 - 0s 893us/step - loss: 0.2714 - val_loss: 1.0485\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.185 - 0s 940us/step - loss: 0.2743 - val_loss: 1.0686\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.225 - 0s 855us/step - loss: 0.2701 - val_loss: 1.0528\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.294 - 0s 874us/step - loss: 0.2811 - val_loss: 1.0915\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.200 - 0s 940us/step - loss: 0.2878 - val_loss: 1.1162\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.318 - 0s 874us/step - loss: 0.2556 - val_loss: 1.1887\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.195 - 0s 931us/step - loss: 0.2486 - val_loss: 1.2098\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.282 - 0s 893us/step - loss: 0.2545 - val_loss: 1.1963\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.465 - ETA: 0s - loss: 0.275 - 0s 845us/step - loss: 0.2532 - val_loss: 1.2391\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.238 - 0s 988us/step - loss: 0.2432 - val_loss: 1.2635\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.225 - 0s 921us/step - loss: 0.2386 - val_loss: 1.3008\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.254 - 0s 893us/step - loss: 0.2404 - val_loss: 1.2829\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.634 - ETA: 0s - loss: 0.280 - 0s 826us/step - loss: 0.2374 - val_loss: 1.3041\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.246 - 0s 836us/step - loss: 0.2555 - val_loss: 1.4047\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.248 - 0s 921us/step - loss: 0.2424 - val_loss: 1.4009\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.224 - 0s 865us/step - loss: 0.2291 - val_loss: 1.3771\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.262 - 0s 808us/step - loss: 0.2441 - val_loss: 1.4337\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 31\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.37 - ETA: 0s - loss: 0.9449 - 1s 9ms/step - loss: 0.7862 - val_loss: 0.2663\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.667 - 0s 827us/step - loss: 0.5218 - val_loss: 0.7033\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.236 - 0s 855us/step - loss: 0.4437 - val_loss: 0.9005\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.442 - 0s 798us/step - loss: 0.4185 - val_loss: 1.0053\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.423 - 0s 893us/step - loss: 0.3925 - val_loss: 0.9661\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.424 - 0s 864us/step - loss: 0.3738 - val_loss: 0.8720\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.383 - 0s 1ms/step - loss: 0.3656 - val_loss: 0.7760\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.549 - ETA: 0s - loss: 0.406 - 0s 893us/step - loss: 0.3511 - val_loss: 0.7248\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.335 - 0s 903us/step - loss: 0.3507 - val_loss: 0.7865\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.508 - ETA: 0s - loss: 0.415 - 0s 855us/step - loss: 0.3501 - val_loss: 0.7178\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.412 - ETA: 0s - loss: 0.359 - 0s 1ms/step - loss: 0.3475 - val_loss: 0.5878\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.372 - 0s 760us/step - loss: 0.3408 - val_loss: 0.5930\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.356 - 0s 817us/step - loss: 0.3226 - val_loss: 0.5260\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.290 - ETA: 0s - loss: 0.279 - 0s 807us/step - loss: 0.3142 - val_loss: 0.4916\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.711 - ETA: 0s - loss: 0.371 - 0s 865us/step - loss: 0.3266 - val_loss: 0.5167\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.273 - 0s 817us/step - loss: 0.3043 - val_loss: 0.5031\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.320 - 0s 798us/step - loss: 0.2948 - val_loss: 0.5259\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.362 - 0s 874us/step - loss: 0.3123 - val_loss: 0.5157\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.330 - 0s 817us/step - loss: 0.3029 - val_loss: 0.4651\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.267 - 0s 836us/step - loss: 0.3104 - val_loss: 0.3456\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.528 - ETA: 0s - loss: 0.251 - 0s 846us/step - loss: 0.2884 - val_loss: 0.4022\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.409 - ETA: 0s - loss: 0.309 - 0s 883us/step - loss: 0.2931 - val_loss: 0.3994\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.151 - ETA: 0s - loss: 0.289 - 0s 940us/step - loss: 0.3054 - val_loss: 0.3513\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.290 - 0s 826us/step - loss: 0.2926 - val_loss: 0.3902\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.313 - 0s 931us/step - loss: 0.2876 - val_loss: 0.3359\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.263 - 0s 826us/step - loss: 0.2744 - val_loss: 0.3876\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.312 - 0s 883us/step - loss: 0.2892 - val_loss: 0.3886\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.284 - 0s 969us/step - loss: 0.2888 - val_loss: 0.3927\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.251 - 0s 1ms/step - loss: 0.2612 - val_loss: 0.4809\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.260 - 0s 941us/step - loss: 0.2867 - val_loss: 0.3715\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.288 - 0s 921us/step - loss: 0.2728 - val_loss: 0.3125\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 32\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.69 - ETA: 0s - loss: 1.2227 - 1s 9ms/step - loss: 0.9694 - val_loss: 0.1912\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.439 - 0s 864us/step - loss: 0.6657 - val_loss: 0.5231\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.951 - ETA: 0s - loss: 0.691 - 0s 893us/step - loss: 0.6040 - val_loss: 0.8781\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.509 - 0s 846us/step - loss: 0.5428 - val_loss: 0.8124\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.588 - 0s 912us/step - loss: 0.5121 - val_loss: 0.8059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.352 - 0s 836us/step - loss: 0.4709 - val_loss: 0.7062\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.539 - 0s 855us/step - loss: 0.4676 - val_loss: 0.8795\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.370 - 0s 845us/step - loss: 0.4240 - val_loss: 0.8951\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.344 - 0s 893us/step - loss: 0.4245 - val_loss: 0.9543\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.339 - 0s 836us/step - loss: 0.4411 - val_loss: 0.8334\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.304 - 0s 845us/step - loss: 0.4277 - val_loss: 0.7976\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.322 - 0s 855us/step - loss: 0.3940 - val_loss: 0.8848\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.301 - 0s 874us/step - loss: 0.3912 - val_loss: 0.9408\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.408 - 0s 855us/step - loss: 0.3786 - val_loss: 0.8755\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.374 - 0s 883us/step - loss: 0.4008 - val_loss: 0.7455\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.398 - 0s 807us/step - loss: 0.3686 - val_loss: 0.7248\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.209 - ETA: 0s - loss: 0.386 - 0s 893us/step - loss: 0.3576 - val_loss: 0.7650\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.436 - 0s 893us/step - loss: 0.3610 - val_loss: 0.8484\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 0.337 - 0s 845us/step - loss: 0.3683 - val_loss: 0.7497\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.528 - ETA: 0s - loss: 0.394 - 0s 807us/step - loss: 0.3481 - val_loss: 0.9785\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.333 - 0s 836us/step - loss: 0.3391 - val_loss: 0.9999\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.354 - 0s 836us/step - loss: 0.3448 - val_loss: 0.9478\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.387 - 0s 893us/step - loss: 0.3447 - val_loss: 0.8358\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.382 - 0s 817us/step - loss: 0.3356 - val_loss: 0.9330\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.376 - 0s 798us/step - loss: 0.3401 - val_loss: 1.0095\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.341 - 0s 855us/step - loss: 0.3362 - val_loss: 0.8856\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.421 - ETA: 0s - loss: 0.337 - 0s 798us/step - loss: 0.3135 - val_loss: 0.8374\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.800 - ETA: 0s - loss: 0.365 - 0s 912us/step - loss: 0.3333 - val_loss: 0.8263\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.960 - ETA: 0s - loss: 0.334 - 0s 845us/step - loss: 0.3130 - val_loss: 0.8365\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.256 - 0s 789us/step - loss: 0.2929 - val_loss: 0.8954\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.272 - 0s 845us/step - loss: 0.2924 - val_loss: 0.8767\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 33\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.12 - ETA: 0s - loss: 0.1586 - 1s 9ms/step - loss: 0.9314 - val_loss: 0.1583\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.441 - 0s 912us/step - loss: 0.8351 - val_loss: 0.2473\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 1.106 - 0s 883us/step - loss: 0.7840 - val_loss: 0.3207\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.826 - 0s 940us/step - loss: 0.7691 - val_loss: 0.3231\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.926 - 0s 1ms/step - loss: 0.7061 - val_loss: 0.3572\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.664 - 0s 874us/step - loss: 0.6653 - val_loss: 0.4865\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.290 - ETA: 0s - loss: 0.891 - 0s 922us/step - loss: 0.6679 - val_loss: 0.6090\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.819 - 0s 855us/step - loss: 0.6116 - val_loss: 0.6521\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.290 - 0s 845us/step - loss: 0.5722 - val_loss: 0.6966\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.785 - 0s 912us/step - loss: 0.5662 - val_loss: 0.9910\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.718 - 0s 893us/step - loss: 0.5513 - val_loss: 1.1665\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.587 - 0s 902us/step - loss: 0.4780 - val_loss: 1.3779\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.498 - 0s 1ms/step - loss: 0.4842 - val_loss: 1.5199\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.587 - 0s 912us/step - loss: 0.5084 - val_loss: 1.7336\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.594 - 0s 921us/step - loss: 0.4399 - val_loss: 1.9553\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.556 - 0s 902us/step - loss: 0.4223 - val_loss: 2.1435\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.549 - 0s 969us/step - loss: 0.4343 - val_loss: 2.4276\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.500 - 0s 940us/step - loss: 0.3703 - val_loss: 2.6311\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.457 - 0s 826us/step - loss: 0.4014 - val_loss: 2.7842\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.748 - ETA: 0s - loss: 0.481 - 0s 921us/step - loss: 0.3734 - val_loss: 3.0077\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.251 - 0s 902us/step - loss: 0.4023 - val_loss: 2.8748\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.178 - 0s 922us/step - loss: 0.3532 - val_loss: 3.0461\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.375 - 0s 893us/step - loss: 0.3538 - val_loss: 3.5953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.218 - 0s 912us/step - loss: 0.3301 - val_loss: 3.3364\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.286 - 0s 883us/step - loss: 0.2884 - val_loss: 3.9462\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.307 - 0s 826us/step - loss: 0.2940 - val_loss: 3.8215\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.391 - 0s 931us/step - loss: 0.2891 - val_loss: 4.0774\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.404 - 0s 836us/step - loss: 0.3602 - val_loss: 4.3135\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.280 - 0s 836us/step - loss: 0.2655 - val_loss: 4.3722\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.592 - ETA: 0s - loss: 0.313 - 0s 855us/step - loss: 0.2726 - val_loss: 4.5281\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.267 - 0s 874us/step - loss: 0.2551 - val_loss: 4.7893\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 34\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 1.78 - ETA: 0s - loss: 1.1557 - 1s 9ms/step - loss: 0.8900 - val_loss: 0.2235\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.577 - 0s 826us/step - loss: 0.6728 - val_loss: 0.5423\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.541 - 0s 807us/step - loss: 0.5831 - val_loss: 0.9974\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.526 - 0s 845us/step - loss: 0.5299 - val_loss: 1.2456\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.459 - 0s 807us/step - loss: 0.5276 - val_loss: 1.3133\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.509 - 0s 826us/step - loss: 0.5031 - val_loss: 1.6039\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.721 - ETA: 0s - loss: 0.462 - 0s 817us/step - loss: 0.4746 - val_loss: 1.7064\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.799 - ETA: 0s - loss: 0.510 - 0s 836us/step - loss: 0.4641 - val_loss: 1.5694\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.927 - ETA: 0s - loss: 0.462 - 0s 845us/step - loss: 0.4453 - val_loss: 2.0758\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.353 - ETA: 0s - loss: 0.395 - 0s 826us/step - loss: 0.4181 - val_loss: 1.9237\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.429 - 0s 846us/step - loss: 0.4341 - val_loss: 2.2506\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.445 - ETA: 0s - loss: 0.379 - 0s 826us/step - loss: 0.4202 - val_loss: 2.2730\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.415 - 0s 893us/step - loss: 0.4096 - val_loss: 2.3133\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.528 - ETA: 0s - loss: 0.406 - 0s 912us/step - loss: 0.4221 - val_loss: 2.5186\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.569 - ETA: 0s - loss: 0.433 - 0s 921us/step - loss: 0.3916 - val_loss: 2.4886\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.533 - ETA: 0s - loss: 0.401 - 0s 874us/step - loss: 0.4073 - val_loss: 2.4252\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.380 - 0s 864us/step - loss: 0.3887 - val_loss: 2.7818\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.321 - 0s 997us/step - loss: 0.3856 - val_loss: 2.3889\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.361 - 0s 912us/step - loss: 0.3947 - val_loss: 2.7413\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.381 - 0s 902us/step - loss: 0.3759 - val_loss: 2.9609\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.340 - 0s 883us/step - loss: 0.3547 - val_loss: 2.9262\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.384 - 0s 912us/step - loss: 0.3890 - val_loss: 2.9791\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.371 - 0s 912us/step - loss: 0.3825 - val_loss: 2.8284\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.688 - ETA: 0s - loss: 0.382 - 0s 864us/step - loss: 0.3742 - val_loss: 3.0291\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.298 - 0s 893us/step - loss: 0.3728 - val_loss: 3.1749\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.386 - 0s 950us/step - loss: 0.3644 - val_loss: 3.2324\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.394 - ETA: 0s - loss: 0.442 - 0s 921us/step - loss: 0.3694 - val_loss: 2.9702\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.394 - 0s 893us/step - loss: 0.3532 - val_loss: 2.9373\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 0.316 - 0s 893us/step - loss: 0.3628 - val_loss: 3.1567\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.306 - 0s 893us/step - loss: 0.3497 - val_loss: 3.2187\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.354 - 0s 921us/step - loss: 0.3514 - val_loss: 3.3380\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 35\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.21 - ETA: 1s - loss: 0.8526 - 1s 9ms/step - loss: 0.9608 - val_loss: 0.0632\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.306 - ETA: 0s - loss: 1.166 - 0s 921us/step - loss: 0.8886 - val_loss: 0.1209\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 10.67 - ETA: 0s - loss: 1.2586 - 0s 921us/step - loss: 0.8644 - val_loss: 0.1626\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 1.117 - 0s 836us/step - loss: 0.8436 - val_loss: 0.0907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.867 - ETA: 0s - loss: 1.051 - 0s 864us/step - loss: 0.8271 - val_loss: 0.0789\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 1.021 - 0s 826us/step - loss: 0.8138 - val_loss: 0.1123\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 1.028 - 0s 826us/step - loss: 0.7999 - val_loss: 0.0920\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.808 - 0s 903us/step - loss: 0.7875 - val_loss: 0.1019\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.382 - ETA: 0s - loss: 0.509 - 0s 817us/step - loss: 0.7739 - val_loss: 0.1145\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.647 - 0s 836us/step - loss: 0.7539 - val_loss: 0.1115\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.829 - 0s 836us/step - loss: 0.7384 - val_loss: 0.1441\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.892 - 0s 826us/step - loss: 0.7274 - val_loss: 0.1562\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.476 - 0s 893us/step - loss: 0.7252 - val_loss: 0.1701\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.961 - 0s 893us/step - loss: 0.7393 - val_loss: 0.1801\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.515 - 0s 874us/step - loss: 0.7173 - val_loss: 0.1793\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.889 - 0s 798us/step - loss: 0.7280 - val_loss: 0.1984\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.671 - ETA: 0s - loss: 0.988 - 0s 883us/step - loss: 0.6996 - val_loss: 0.2058\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.871 - 0s 912us/step - loss: 0.7000 - val_loss: 0.2147\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.911 - 0s 922us/step - loss: 0.6893 - val_loss: 0.2160\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.867 - 0s 855us/step - loss: 0.6957 - val_loss: 0.2309\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.870 - 0s 883us/step - loss: 0.6607 - val_loss: 0.2392\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.331 - ETA: 0s - loss: 0.809 - 0s 855us/step - loss: 0.6512 - val_loss: 0.2830\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.745 - 0s 836us/step - loss: 0.6585 - val_loss: 0.3005\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.842 - 0s 845us/step - loss: 0.6608 - val_loss: 0.2749\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.407 - ETA: 0s - loss: 0.585 - 0s 855us/step - loss: 0.6406 - val_loss: 0.2832\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.578 - 0s 874us/step - loss: 0.6532 - val_loss: 0.2919\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.755 - 0s 845us/step - loss: 0.6273 - val_loss: 0.2939\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.422 - 0s 883us/step - loss: 0.6281 - val_loss: 0.2832\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.224 - 0s 883us/step - loss: 0.6408 - val_loss: 0.2816\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.537 - 0s 874us/step - loss: 0.6177 - val_loss: 0.3038\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.625 - ETA: 0s - loss: 0.423 - 0s 902us/step - loss: 0.6159 - val_loss: 0.3005\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 36\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.89 - ETA: 0s - loss: 1.3106 - 1s 9ms/step - loss: 0.9156 - val_loss: 0.1129\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.166 - 0s 922us/step - loss: 0.7827 - val_loss: 0.2357\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.897 - 0s 788us/step - loss: 0.7355 - val_loss: 0.3768\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.170 - 0s 845us/step - loss: 0.6919 - val_loss: 0.3828\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.841 - 0s 807us/step - loss: 0.6750 - val_loss: 0.5281\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.762 - 0s 874us/step - loss: 0.6320 - val_loss: 0.6304\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.148 - 0s 874us/step - loss: 0.5769 - val_loss: 0.5242\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.681 - 0s 769us/step - loss: 0.5735 - val_loss: 0.7725\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.331 - ETA: 0s - loss: 0.646 - 0s 817us/step - loss: 0.5474 - val_loss: 0.8232\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.168 - 0s 912us/step - loss: 0.5641 - val_loss: 0.6781\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.190 - 0s 807us/step - loss: 0.4908 - val_loss: 0.9543\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.584 - 0s 969us/step - loss: 0.4692 - val_loss: 1.1820\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.544 - 0s 836us/step - loss: 0.4522 - val_loss: 1.2690\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.194 - 0s 950us/step - loss: 0.4605 - val_loss: 1.2159\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.541 - 0s 940us/step - loss: 0.4431 - val_loss: 1.4672\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.192 - 0s 845us/step - loss: 0.4254 - val_loss: 1.2687\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.113 - ETA: 0s - loss: 0.603 - 0s 855us/step - loss: 0.4721 - val_loss: 1.4680\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.463 - ETA: 0s - loss: 0.570 - 0s 921us/step - loss: 0.4368 - val_loss: 1.4317\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.193 - ETA: 0s - loss: 0.472 - 0s 931us/step - loss: 0.4061 - val_loss: 1.5203\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.653 - ETA: 0s - loss: 0.568 - 0s 940us/step - loss: 0.4331 - val_loss: 1.5249\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.132 - 0s 931us/step - loss: 0.3782 - val_loss: 1.3393\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.477 - 0s 902us/step - loss: 0.4205 - val_loss: 2.0999\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.420 - 0s 902us/step - loss: 0.3633 - val_loss: 1.6492\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.489 - 0s 912us/step - loss: 0.3708 - val_loss: 1.6690\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.379 - 0s 874us/step - loss: 0.3341 - val_loss: 1.6910\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.431 - 0s 874us/step - loss: 0.3388 - val_loss: 1.6993\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.387 - 0s 855us/step - loss: 0.3299 - val_loss: 1.8747\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.168 - 0s 893us/step - loss: 0.3437 - val_loss: 1.6833\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.353 - 0s 874us/step - loss: 0.3366 - val_loss: 2.0759\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.349 - 0s 864us/step - loss: 0.3492 - val_loss: 1.8004\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.416 - ETA: 0s - loss: 0.326 - 0s 845us/step - loss: 0.2848 - val_loss: 1.8362\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 37\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.20 - ETA: 0s - loss: 0.7355 - 1s 9ms/step - loss: 0.8227 - val_loss: 0.2017\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.700 - ETA: 0s - loss: 0.427 - 0s 798us/step - loss: 0.6066 - val_loss: 0.5997\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.566 - 0s 836us/step - loss: 0.5382 - val_loss: 0.9529\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.460 - 0s 912us/step - loss: 0.5111 - val_loss: 0.7597\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.554 - 0s 883us/step - loss: 0.4870 - val_loss: 1.0582\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.524 - 0s 1ms/step - loss: 0.4628 - val_loss: 0.8869\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.346 - ETA: 0s - loss: 0.484 - 0s 817us/step - loss: 0.4555 - val_loss: 0.9035\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.209 - ETA: 0s - loss: 0.428 - 0s 893us/step - loss: 0.4388 - val_loss: 0.9559\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.742 - ETA: 0s - loss: 0.418 - 0s 950us/step - loss: 0.4206 - val_loss: 0.9403\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.436 - 0s 864us/step - loss: 0.4244 - val_loss: 1.0369\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.109 - ETA: 0s - loss: 0.440 - 0s 817us/step - loss: 0.4109 - val_loss: 1.0624\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.345 - 0s 798us/step - loss: 0.4127 - val_loss: 1.0328\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.363 - 0s 836us/step - loss: 0.3814 - val_loss: 0.9931\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.384 - 0s 817us/step - loss: 0.3891 - val_loss: 1.1248\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.383 - 0s 902us/step - loss: 0.3880 - val_loss: 0.9858\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.734 - ETA: 0s - loss: 0.349 - 0s 855us/step - loss: 0.3695 - val_loss: 1.0575\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.387 - 0s 817us/step - loss: 0.3646 - val_loss: 0.9814\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.347 - 0s 912us/step - loss: 0.3655 - val_loss: 1.0259\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.325 - 0s 902us/step - loss: 0.3565 - val_loss: 0.9998\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.347 - 0s 883us/step - loss: 0.3608 - val_loss: 1.0636\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.316 - 0s 855us/step - loss: 0.3613 - val_loss: 0.8842\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.323 - 0s 845us/step - loss: 0.3579 - val_loss: 0.8328\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.334 - 0s 874us/step - loss: 0.3503 - val_loss: 0.9830\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.769 - ETA: 0s - loss: 0.350 - 0s 884us/step - loss: 0.3701 - val_loss: 0.9473\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.304 - 0s 912us/step - loss: 0.3271 - val_loss: 0.8473\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.261 - 0s 874us/step - loss: 0.3194 - val_loss: 1.0234\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.356 - 0s 921us/step - loss: 0.3265 - val_loss: 0.8800\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.467 - ETA: 0s - loss: 0.365 - 0s 950us/step - loss: 0.3294 - val_loss: 0.9107\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.278 - 0s 940us/step - loss: 0.3188 - val_loss: 0.9070\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.337 - 0s 902us/step - loss: 0.3293 - val_loss: 1.2861\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.553 - ETA: 0s - loss: 0.323 - 0s 855us/step - loss: 0.3257 - val_loss: 0.9902\n",
      "Epoch 00031: early stopping\n",
      "MSE: 603.477344\n",
      "RMSE: 24.565776\n",
      "MAE: 19.643537\n",
      "MAPE: 23.800854\n",
      "\n",
      "Quantile 1, between 39.99999999999999 and 77.5\n",
      "MSE: 658.622571\n",
      "RMSE: 25.663643\n",
      "MAE: 20.187494\n",
      "MAPE: 34.605493\n",
      "\n",
      "Quantile 2, between 77.5 and 87.5\n",
      "MSE: 580.880916\n",
      "RMSE: 24.101471\n",
      "MAE: 16.145712\n",
      "MAPE: 19.561259\n",
      "\n",
      "Quantile 3, between 87.5 and 98.00000000000001\n",
      "MSE: 318.214424\n",
      "RMSE: 17.838566\n",
      "MAE: 16.063421\n",
      "MAPE: 17.070304\n",
      "\n",
      "Quantile 4, between 98.00000000000001 and 130.00000000000003\n",
      "MSE: 825.405531\n",
      "RMSE: 28.729872\n",
      "MAE: 25.469728\n",
      "MAPE: 22.869347\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_test = 15\n",
    "n_hexa = 10\n",
    "name_error = 'LSTM predictions - 15 Days - 10 hex -4q '\n",
    "path_error = \"imgs/ips_predictions/prediction_lstm_15_days_10hex_4q.png\"\n",
    "name_map = '15 days - 10 hex - 4q'\n",
    "path_map = \"imgs/ips_predictions/maps_LSTM_prediction_15_total_data_10_hex_4q.png\"\n",
    "\n",
    "dict_pred,pred_lstm,pred_lstm_cases = lstm_hex_selected(data_days, dict_pred_sel_hexa,n_test,n_hexa,name_error,path_error,name_map,path_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,) (15,)\n",
      "model compiled 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.66 - ETA: 0s - loss: 0.8717 - 1s 9ms/step - loss: 0.8228 - val_loss: 0.1937\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.551 - ETA: 0s - loss: 0.733 - 0s 883us/step - loss: 0.6686 - val_loss: 0.5334\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.644 - 0s 874us/step - loss: 0.6025 - val_loss: 0.8648\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.073 - ETA: 0s - loss: 0.580 - 0s 950us/step - loss: 0.5672 - val_loss: 1.0965\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.500 - 0s 864us/step - loss: 0.5428 - val_loss: 1.1219\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.644 - 0s 988us/step - loss: 0.5529 - val_loss: 1.1391\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.366 - ETA: 0s - loss: 0.637 - 0s 902us/step - loss: 0.5370 - val_loss: 1.3150\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.850 - ETA: 0s - loss: 0.621 - 0s 950us/step - loss: 0.5381 - val_loss: 1.1305\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.861 - ETA: 0s - loss: 0.491 - 0s 912us/step - loss: 0.5209 - val_loss: 1.2208\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.580 - 0s 997us/step - loss: 0.5188 - val_loss: 1.3804\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.572 - 0s 912us/step - loss: 0.5252 - val_loss: 1.3163\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.542 - ETA: 0s - loss: 0.579 - 0s 931us/step - loss: 0.5234 - val_loss: 1.2806\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.272 - ETA: 0s - loss: 0.605 - 0s 931us/step - loss: 0.5162 - val_loss: 1.2139\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.634 - ETA: 0s - loss: 0.575 - 0s 959us/step - loss: 0.5078 - val_loss: 1.4700\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.569 - ETA: 0s - loss: 0.501 - 0s 1ms/step - loss: 0.5155 - val_loss: 1.4473\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.411 - ETA: 0s - loss: 0.474 - 0s 884us/step - loss: 0.5046 - val_loss: 1.3644\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.569 - ETA: 0s - loss: 0.508 - 0s 912us/step - loss: 0.4972 - val_loss: 1.3727\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.501 - 0s 883us/step - loss: 0.4981 - val_loss: 1.5021\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.432 - 0s 1ms/step - loss: 0.5075 - val_loss: 1.5041\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.758 - ETA: 0s - loss: 0.437 - 0s 836us/step - loss: 0.4944 - val_loss: 1.3223\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.429 - 0s 864us/step - loss: 0.4800 - val_loss: 1.5354\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.401 - 0s 1ms/step - loss: 0.4844 - val_loss: 1.3458\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.487 - ETA: 0s - loss: 0.582 - 0s 978us/step - loss: 0.5104 - val_loss: 1.5550\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.475 - 0s 864us/step - loss: 0.4742 - val_loss: 1.4649\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.578 - 0s 893us/step - loss: 0.4982 - val_loss: 1.4130\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.525 - ETA: 0s - loss: 0.502 - 0s 893us/step - loss: 0.4899 - val_loss: 1.6778\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.383 - ETA: 0s - loss: 0.544 - 0s 940us/step - loss: 0.4848 - val_loss: 1.6536\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.571 - 0s 941us/step - loss: 0.4696 - val_loss: 1.6848\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.433 - 0s 1ms/step - loss: 0.4862 - val_loss: 1.6159\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.709 - ETA: 0s - loss: 0.542 - 0s 921us/step - loss: 0.5023 - val_loss: 1.7759\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.484 - ETA: 0s - loss: 0.429 - 0s 912us/step - loss: 0.4925 - val_loss: 1.5606\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.31 - ETA: 0s - loss: 0.8151 - 1s 9ms/step - loss: 0.8101 - val_loss: 0.0698\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.075 - ETA: 0s - loss: 0.600 - 0s 827us/step - loss: 0.5946 - val_loss: 0.2310\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.447 - ETA: 0s - loss: 0.495 - 0s 988us/step - loss: 0.5145 - val_loss: 0.4118\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.390 - 0s 845us/step - loss: 0.4674 - val_loss: 0.5128\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.525 - 0s 864us/step - loss: 0.4352 - val_loss: 0.5235\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.174 - ETA: 0s - loss: 0.425 - 0s 817us/step - loss: 0.4361 - val_loss: 0.5127\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.397 - 0s 903us/step - loss: 0.4200 - val_loss: 0.5431\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.455 - ETA: 0s - loss: 0.371 - 0s 836us/step - loss: 0.4038 - val_loss: 0.5207\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.383 - 0s 931us/step - loss: 0.4082 - val_loss: 0.5580\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.421 - 0s 855us/step - loss: 0.3922 - val_loss: 0.4952\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.474 - 0s 921us/step - loss: 0.3920 - val_loss: 0.5455\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.407 - 0s 883us/step - loss: 0.3918 - val_loss: 0.5675\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.808 - ETA: 0s - loss: 0.367 - 0s 883us/step - loss: 0.3745 - val_loss: 0.5560\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.368 - ETA: 0s - loss: 0.426 - 0s 826us/step - loss: 0.3803 - val_loss: 0.5352\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.578 - ETA: 0s - loss: 0.426 - 0s 826us/step - loss: 0.3887 - val_loss: 0.5610\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.326 - 0s 817us/step - loss: 0.3575 - val_loss: 0.5925\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.346 - 0s 779us/step - loss: 0.3675 - val_loss: 0.5676\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.352 - ETA: 0s - loss: 0.376 - 0s 788us/step - loss: 0.3861 - val_loss: 0.5286\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.338 - 0s 845us/step - loss: 0.3601 - val_loss: 0.5717\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.437 - 0s 874us/step - loss: 0.3628 - val_loss: 0.5875\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.323 - 0s 807us/step - loss: 0.3587 - val_loss: 0.5824\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.978 - ETA: 0s - loss: 0.370 - 0s 817us/step - loss: 0.3552 - val_loss: 0.6214\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.377 - 0s 1ms/step - loss: 0.3562 - val_loss: 0.5561\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - ETA: 0s - loss: 0.322 - 0s 864us/step - loss: 0.3498 - val_loss: 0.5578\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.405 - 0s 922us/step - loss: 0.3512 - val_loss: 0.6138\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.400 - 0s 845us/step - loss: 0.3637 - val_loss: 0.5782\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.338 - 0s 855us/step - loss: 0.3616 - val_loss: 0.6102\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.353 - 0s 836us/step - loss: 0.3447 - val_loss: 0.5902\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.354 - 0s 845us/step - loss: 0.3558 - val_loss: 0.5523\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.360 - 0s 798us/step - loss: 0.3365 - val_loss: 0.5882\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.359 - 0s 769us/step - loss: 0.3520 - val_loss: 0.5904\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.68 - ETA: 0s - loss: 0.8815 - 1s 9ms/step - loss: 0.8731 - val_loss: 0.1667\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.713 - 0s 855us/step - loss: 0.7617 - val_loss: 0.4382\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.631 - ETA: 0s - loss: 0.859 - 0s 864us/step - loss: 0.7136 - val_loss: 0.6788\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.625 - 0s 826us/step - loss: 0.6954 - val_loss: 0.7069\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.663 - 0s 874us/step - loss: 0.6746 - val_loss: 0.8124\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.378 - ETA: 0s - loss: 0.622 - 0s 836us/step - loss: 0.6651 - val_loss: 0.7671\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.697 - 0s 855us/step - loss: 0.6527 - val_loss: 0.8788\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.597 - 0s 893us/step - loss: 0.6500 - val_loss: 0.8534\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.669 - ETA: 0s - loss: 0.714 - 0s 874us/step - loss: 0.6397 - val_loss: 0.7858\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.879 - ETA: 0s - loss: 0.733 - 0s 836us/step - loss: 0.6348 - val_loss: 0.8455\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.370 - ETA: 0s - loss: 0.652 - 0s 845us/step - loss: 0.6321 - val_loss: 0.8421\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.665 - 0s 883us/step - loss: 0.6434 - val_loss: 0.8715\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.574 - 0s 998us/step - loss: 0.6279 - val_loss: 0.8670\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.632 - 0s 902us/step - loss: 0.6240 - val_loss: 0.8222\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.585 - 0s 940us/step - loss: 0.6228 - val_loss: 0.7943\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.622 - 0s 921us/step - loss: 0.6190 - val_loss: 0.7841\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.541 - ETA: 0s - loss: 0.574 - 0s 874us/step - loss: 0.6120 - val_loss: 0.9186\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.573 - 0s 845us/step - loss: 0.6129 - val_loss: 0.8388\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.269 - ETA: 0s - loss: 0.580 - 0s 874us/step - loss: 0.6182 - val_loss: 0.8856\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.661 - 0s 855us/step - loss: 0.6122 - val_loss: 1.1194\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.645 - 0s 855us/step - loss: 0.5980 - val_loss: 1.0210\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.651 - 0s 883us/step - loss: 0.6137 - val_loss: 0.9176\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.741 - 0s 950us/step - loss: 0.6015 - val_loss: 1.0186\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.575 - 0s 883us/step - loss: 0.5875 - val_loss: 0.9216\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.150 - ETA: 0s - loss: 0.475 - 0s 893us/step - loss: 0.5916 - val_loss: 0.8956\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.756 - ETA: 0s - loss: 0.660 - 0s 845us/step - loss: 0.5814 - val_loss: 0.9132\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.600 - 0s 912us/step - loss: 0.5959 - val_loss: 0.9947\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.629 - ETA: 0s - loss: 0.671 - 0s 940us/step - loss: 0.6011 - val_loss: 1.0456\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.635 - 0s 864us/step - loss: 0.5794 - val_loss: 0.8017\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.318 - 0s 931us/step - loss: 0.5763 - val_loss: 0.8705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.419 - ETA: 0s - loss: 0.662 - 0s 817us/step - loss: 0.5699 - val_loss: 0.9630\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.44 - ETA: 0s - loss: 0.9220 - 1s 9ms/step - loss: 0.8394 - val_loss: 0.1889\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.457 - ETA: 0s - loss: 0.557 - 0s 817us/step - loss: 0.5546 - val_loss: 0.7470\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.424 - 0s 912us/step - loss: 0.3932 - val_loss: 1.5442\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.322 - 0s 855us/step - loss: 0.3060 - val_loss: 2.3451\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.424 - ETA: 0s - loss: 0.283 - 0s 912us/step - loss: 0.2750 - val_loss: 2.6504\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.265 - 0s 836us/step - loss: 0.2752 - val_loss: 2.8390\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.361 - ETA: 0s - loss: 0.261 - 0s 874us/step - loss: 0.2697 - val_loss: 2.9347\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.477 - ETA: 0s - loss: 0.259 - 0s 1ms/step - loss: 0.2594 - val_loss: 2.8625\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.253 - 0s 874us/step - loss: 0.2631 - val_loss: 3.0014\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.226 - 0s 864us/step - loss: 0.2606 - val_loss: 3.0373\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.252 - 0s 893us/step - loss: 0.2508 - val_loss: 2.9544\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.256 - 0s 893us/step - loss: 0.2534 - val_loss: 3.4730\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.256 - 0s 921us/step - loss: 0.2510 - val_loss: 2.8325\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.273 - 0s 921us/step - loss: 0.2526 - val_loss: 3.0432\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.241 - 0s 884us/step - loss: 0.2462 - val_loss: 3.1247\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.225 - 0s 845us/step - loss: 0.2438 - val_loss: 3.2247\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.237 - 0s 864us/step - loss: 0.2646 - val_loss: 2.8861\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.433 - ETA: 0s - loss: 0.244 - 0s 883us/step - loss: 0.2378 - val_loss: 3.4947\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.258 - 0s 921us/step - loss: 0.2427 - val_loss: 3.1690\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.200 - 0s 836us/step - loss: 0.2235 - val_loss: 3.3722\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.256 - 0s 807us/step - loss: 0.2372 - val_loss: 2.9665\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.241 - 0s 893us/step - loss: 0.2319 - val_loss: 3.1260\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.253 - 0s 931us/step - loss: 0.2225 - val_loss: 3.1703\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.249 - 0s 864us/step - loss: 0.2317 - val_loss: 2.9436\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.237 - 0s 969us/step - loss: 0.2384 - val_loss: 3.2367\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.266 - 0s 826us/step - loss: 0.2368 - val_loss: 2.7795\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.231 - 0s 846us/step - loss: 0.2230 - val_loss: 2.7690\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.228 - 0s 1ms/step - loss: 0.2227 - val_loss: 3.1007\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.219 - 0s 3ms/step - loss: 0.2183 - val_loss: 3.3027\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.223 - 0s 1ms/step - loss: 0.2204 - val_loss: 3.2107\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.240 - 0s 779us/step - loss: 0.2250 - val_loss: 2.9530\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 4.22 - ETA: 0s - loss: 0.8977 - 1s 9ms/step - loss: 0.9697 - val_loss: 0.0551\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.462 - ETA: 0s - loss: 0.829 - 0s 959us/step - loss: 0.8343 - val_loss: 0.2242\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.778 - 0s 864us/step - loss: 0.7636 - val_loss: 0.4269\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 0.841 - 0s 921us/step - loss: 0.7205 - val_loss: 0.5248\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.699 - 0s 912us/step - loss: 0.7237 - val_loss: 0.5781\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.629 - 0s 874us/step - loss: 0.6969 - val_loss: 0.6865\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.762 - 0s 893us/step - loss: 0.6739 - val_loss: 0.7289\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.701 - ETA: 0s - loss: 0.750 - 0s 855us/step - loss: 0.6558 - val_loss: 0.7431\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.738 - 0s 959us/step - loss: 0.6466 - val_loss: 0.8522\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.512 - 0s 893us/step - loss: 0.6421 - val_loss: 0.7463\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.368 - ETA: 0s - loss: 0.541 - 0s 883us/step - loss: 0.6176 - val_loss: 0.8140\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.863 - ETA: 0s - loss: 0.544 - 0s 817us/step - loss: 0.6247 - val_loss: 0.8896\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.532 - 0s 883us/step - loss: 0.6388 - val_loss: 0.6998\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.304 - ETA: 0s - loss: 0.593 - 0s 912us/step - loss: 0.6380 - val_loss: 0.8511\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.704 - 0s 836us/step - loss: 0.6271 - val_loss: 0.7608\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.739 - ETA: 0s - loss: 0.625 - 0s 855us/step - loss: 0.6274 - val_loss: 0.7817\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.595 - 0s 836us/step - loss: 0.6090 - val_loss: 0.8913\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.618 - 0s 798us/step - loss: 0.6213 - val_loss: 0.9238\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.501 - 0s 874us/step - loss: 0.5948 - val_loss: 1.0948\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.263 - ETA: 0s - loss: 0.622 - 0s 941us/step - loss: 0.6040 - val_loss: 1.0941\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.653 - ETA: 0s - loss: 0.648 - 0s 883us/step - loss: 0.6035 - val_loss: 1.0733\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.332 - ETA: 0s - loss: 0.650 - 0s 826us/step - loss: 0.5963 - val_loss: 1.1647\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.744 - ETA: 0s - loss: 0.699 - 0s 874us/step - loss: 0.5987 - val_loss: 1.0987\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.571 - 0s 912us/step - loss: 0.6054 - val_loss: 1.1799\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.719 - 0s 864us/step - loss: 0.5849 - val_loss: 1.0831\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.910 - ETA: 0s - loss: 0.582 - 0s 883us/step - loss: 0.5852 - val_loss: 1.1756\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.615 - ETA: 0s - loss: 0.538 - 0s 931us/step - loss: 0.6122 - val_loss: 1.2571\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.458 - ETA: 0s - loss: 0.589 - 0s 855us/step - loss: 0.5823 - val_loss: 1.0915\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.405 - ETA: 0s - loss: 0.665 - 0s 1ms/step - loss: 0.5830 - val_loss: 1.1560\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.644 - 0s 874us/step - loss: 0.5776 - val_loss: 1.2391\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.629 - ETA: 0s - loss: 0.611 - 0s 845us/step - loss: 0.5697 - val_loss: 1.3189\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.15 - ETA: 0s - loss: 0.6041 - 1s 9ms/step - loss: 0.8980 - val_loss: 0.0519\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.530 - 0s 864us/step - loss: 0.7394 - val_loss: 0.2126\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.728 - ETA: 0s - loss: 0.629 - 0s 827us/step - loss: 0.6594 - val_loss: 0.3926\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.451 - 0s 893us/step - loss: 0.5836 - val_loss: 0.4622\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.414 - 0s 845us/step - loss: 0.5619 - val_loss: 0.5937\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.662 - 0s 1ms/step - loss: 0.5515 - val_loss: 0.6867\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.602 - 0s 826us/step - loss: 0.5149 - val_loss: 0.6473\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.551 - 0s 893us/step - loss: 0.5066 - val_loss: 0.7883\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.470 - ETA: 0s - loss: 0.436 - 0s 940us/step - loss: 0.4911 - val_loss: 0.8247\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.270 - 0s 864us/step - loss: 0.4883 - val_loss: 0.8431\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.723 - ETA: 0s - loss: 0.471 - 0s 883us/step - loss: 0.4825 - val_loss: 0.8914\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.447 - ETA: 0s - loss: 0.442 - 0s 874us/step - loss: 0.4682 - val_loss: 1.0358\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.443 - ETA: 0s - loss: 0.377 - 0s 1ms/step - loss: 0.4844 - val_loss: 0.8663\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.501 - 0s 845us/step - loss: 0.4535 - val_loss: 1.1020\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.379 - 0s 874us/step - loss: 0.4358 - val_loss: 1.1341\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.503 - 0s 893us/step - loss: 0.4402 - val_loss: 1.2075\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.400 - 0s 921us/step - loss: 0.4483 - val_loss: 1.2147\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.477 - 0s 874us/step - loss: 0.4346 - val_loss: 1.4385\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.067 - ETA: 0s - loss: 0.566 - 0s 959us/step - loss: 0.4279 - val_loss: 1.3447\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.406 - 0s 950us/step - loss: 0.4302 - val_loss: 1.2420\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.494 - 0s 855us/step - loss: 0.4465 - val_loss: 1.2098\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.658 - ETA: 0s - loss: 0.457 - 0s 864us/step - loss: 0.4348 - val_loss: 1.2691\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.480 - 0s 826us/step - loss: 0.4310 - val_loss: 1.3865\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.546 - 0s 912us/step - loss: 0.4478 - val_loss: 1.2679\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.353 - 0s 941us/step - loss: 0.4276 - val_loss: 1.3813\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.601 - ETA: 0s - loss: 0.530 - 0s 940us/step - loss: 0.4267 - val_loss: 1.3460\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.472 - 0s 902us/step - loss: 0.4386 - val_loss: 1.6619\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.370 - 0s 864us/step - loss: 0.4208 - val_loss: 1.4592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.502 - 0s 864us/step - loss: 0.4362 - val_loss: 1.3942\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.453 - ETA: 0s - loss: 0.461 - 0s 845us/step - loss: 0.4396 - val_loss: 1.5083\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.398 - 0s 827us/step - loss: 0.4206 - val_loss: 1.4770\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.38 - ETA: 0s - loss: 1.1300 - 1s 9ms/step - loss: 0.8959 - val_loss: 0.1247\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.074 - ETA: 0s - loss: 0.746 - 0s 845us/step - loss: 0.8124 - val_loss: 0.2916\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.741 - 0s 883us/step - loss: 0.7424 - val_loss: 0.4836\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.625 - 0s 836us/step - loss: 0.7283 - val_loss: 0.6379\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.644 - 0s 826us/step - loss: 0.7178 - val_loss: 0.7086\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.450 - ETA: 0s - loss: 0.680 - 0s 883us/step - loss: 0.7063 - val_loss: 0.7517\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.680 - 0s 912us/step - loss: 0.6819 - val_loss: 0.8620\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.065 - ETA: 0s - loss: 0.828 - 0s 845us/step - loss: 0.6969 - val_loss: 0.9942\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.661 - 0s 855us/step - loss: 0.6667 - val_loss: 0.9992\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.492 - ETA: 0s - loss: 0.756 - 0s 883us/step - loss: 0.6676 - val_loss: 1.1008\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.864 - ETA: 0s - loss: 0.807 - 0s 874us/step - loss: 0.6678 - val_loss: 1.1572\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.774 - ETA: 0s - loss: 0.746 - 0s 959us/step - loss: 0.6540 - val_loss: 1.1803\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.335 - ETA: 0s - loss: 0.391 - 0s 959us/step - loss: 0.6408 - val_loss: 1.2239\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.752 - 0s 940us/step - loss: 0.6438 - val_loss: 1.3467\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.692 - ETA: 0s - loss: 0.701 - 0s 940us/step - loss: 0.6459 - val_loss: 1.3426\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.524 - ETA: 0s - loss: 0.481 - 0s 845us/step - loss: 0.6462 - val_loss: 1.3401\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.471 - ETA: 0s - loss: 0.536 - 0s 950us/step - loss: 0.6254 - val_loss: 1.3606\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.894 - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.878 - ETA: 0s - loss: 0.820 - 0s 2ms/step - loss: 0.6445 - val_loss: 1.4742\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.571 - ETA: 0s - loss: 0.625 - 0s 893us/step - loss: 0.6232 - val_loss: 1.3709\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.793 - 0s 912us/step - loss: 0.6276 - val_loss: 1.8547\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.095 - ETA: 0s - loss: 0.870 - 0s 902us/step - loss: 0.6393 - val_loss: 1.8636\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.641 - 0s 893us/step - loss: 0.6244 - val_loss: 1.7266\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.493 - 0s 912us/step - loss: 0.6265 - val_loss: 1.5856\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.411 - ETA: 0s - loss: 0.640 - 0s 874us/step - loss: 0.6167 - val_loss: 1.5865\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.565 - 0s 864us/step - loss: 0.6225 - val_loss: 1.7213\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.483 - 0s 988us/step - loss: 0.5972 - val_loss: 1.7877\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.555 - 0s 912us/step - loss: 0.6109 - val_loss: 1.5354\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.432 - 0s 874us/step - loss: 0.6062 - val_loss: 1.6397\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.392 - 0s 893us/step - loss: 0.6050 - val_loss: 1.6334\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.476 - 0s 893us/step - loss: 0.6191 - val_loss: 1.7347\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.816 - ETA: 0s - loss: 0.755 - 0s 940us/step - loss: 0.6112 - val_loss: 1.8339\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 7\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 2.88 - ETA: 0s - loss: 1.0913 - 1s 9ms/step - loss: 0.9709 - val_loss: 0.1047\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.523 - ETA: 0s - loss: 0.853 - 0s 874us/step - loss: 0.7371 - val_loss: 0.4501\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.711 - 0s 864us/step - loss: 0.6113 - val_loss: 0.8641\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.397 - 0s 864us/step - loss: 0.5314 - val_loss: 1.1864\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.572 - 0s 940us/step - loss: 0.5339 - val_loss: 1.2496\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.573 - ETA: 0s - loss: 0.511 - 0s 902us/step - loss: 0.4922 - val_loss: 1.2691\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.459 - ETA: 0s - loss: 0.515 - 0s 959us/step - loss: 0.4722 - val_loss: 1.2690\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.525 - ETA: 0s - loss: 0.567 - 0s 931us/step - loss: 0.4694 - val_loss: 1.3145\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.309 - ETA: 0s - loss: 0.458 - 0s 912us/step - loss: 0.4355 - val_loss: 1.2663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.412 - 0s 902us/step - loss: 0.4513 - val_loss: 1.2195\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.431 - 0s 855us/step - loss: 0.4456 - val_loss: 1.1435\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.520 - ETA: 0s - loss: 0.400 - 0s 864us/step - loss: 0.4231 - val_loss: 1.0885\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.368 - 0s 807us/step - loss: 0.4388 - val_loss: 1.1015\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.308 - ETA: 0s - loss: 0.409 - 0s 845us/step - loss: 0.4321 - val_loss: 1.1019\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.664 - ETA: 0s - loss: 0.424 - 0s 836us/step - loss: 0.4133 - val_loss: 1.0704\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.365 - 0s 902us/step - loss: 0.4168 - val_loss: 1.0356\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.422 - 0s 921us/step - loss: 0.4142 - val_loss: 1.0723\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.482 - 0s 855us/step - loss: 0.4155 - val_loss: 1.0335\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.365 - 0s 826us/step - loss: 0.4088 - val_loss: 1.0086\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.422 - 0s 845us/step - loss: 0.4048 - val_loss: 1.1293\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.830 - ETA: 0s - loss: 0.252 - 0s 921us/step - loss: 0.4073 - val_loss: 1.0263\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.402 - 0s 855us/step - loss: 0.4055 - val_loss: 1.0424\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.366 - 0s 855us/step - loss: 0.4061 - val_loss: 0.9369\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.375 - 0s 836us/step - loss: 0.4094 - val_loss: 1.0378\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.386 - 0s 798us/step - loss: 0.4090 - val_loss: 0.9900\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.448 - 0s 855us/step - loss: 0.4074 - val_loss: 0.9676\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.354 - 0s 902us/step - loss: 0.3989 - val_loss: 0.9832\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.464 - ETA: 0s - loss: 0.392 - 0s 874us/step - loss: 0.4086 - val_loss: 0.9573\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.903 - ETA: 0s - loss: 0.393 - 0s 826us/step - loss: 0.4002 - val_loss: 0.9440\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.394 - 0s 883us/step - loss: 0.3917 - val_loss: 0.9617\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.410 - 0s 874us/step - loss: 0.4014 - val_loss: 0.9167\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 8\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 12.317 - ETA: 0s - loss: 1.1476  - 1s 9ms/step - loss: 0.9503 - val_loss: 0.0419\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.474 - 0s 798us/step - loss: 0.7136 - val_loss: 0.2080\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.533 - ETA: 0s - loss: 0.751 - 0s 855us/step - loss: 0.6075 - val_loss: 0.3716\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.616 - 0s 817us/step - loss: 0.5434 - val_loss: 0.4852\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.778 - ETA: 0s - loss: 0.619 - 0s 1ms/step - loss: 0.5041 - val_loss: 0.4913\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.564 - 0s 1ms/step - loss: 0.5053 - val_loss: 0.5156\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.512 - 0s 788us/step - loss: 0.4616 - val_loss: 0.5809\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.521 - 0s 846us/step - loss: 0.4731 - val_loss: 0.5224\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.513 - 0s 808us/step - loss: 0.4578 - val_loss: 0.5240\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.519 - ETA: 0s - loss: 0.469 - 0s 902us/step - loss: 0.4275 - val_loss: 0.4808\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.527 - 0s 864us/step - loss: 0.4630 - val_loss: 0.4532\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.486 - 0s 902us/step - loss: 0.4479 - val_loss: 0.4434\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.456 - 0s 912us/step - loss: 0.4236 - val_loss: 0.4332\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.410 - 0s 883us/step - loss: 0.3964 - val_loss: 0.4796\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.432 - 0s 997us/step - loss: 0.4199 - val_loss: 0.4837\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.504 - 0s 874us/step - loss: 0.4228 - val_loss: 0.4508\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.235 - 0s 893us/step - loss: 0.3973 - val_loss: 0.4430\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.232 - 0s 931us/step - loss: 0.4019 - val_loss: 0.4786\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.432 - 0s 874us/step - loss: 0.3932 - val_loss: 0.4889\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.415 - 0s 912us/step - loss: 0.3799 - val_loss: 0.4315\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.442 - 0s 940us/step - loss: 0.4161 - val_loss: 0.4318\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.503 - 0s 931us/step - loss: 0.3856 - val_loss: 0.4891\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.586 - ETA: 0s - loss: 0.279 - 0s 921us/step - loss: 0.3765 - val_loss: 0.4092\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.432 - 0s 874us/step - loss: 0.3755 - val_loss: 0.4715\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.423 - 0s 988us/step - loss: 0.3916 - val_loss: 0.4777\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.483 - 0s 940us/step - loss: 0.3830 - val_loss: 0.4332\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.475 - 0s 921us/step - loss: 0.3972 - val_loss: 0.4702\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.210 - 0s 865us/step - loss: 0.3961 - val_loss: 0.4319\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.428 - 0s 864us/step - loss: 0.3728 - val_loss: 0.4994\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.471 - ETA: 0s - loss: 0.239 - 0s 874us/step - loss: 0.3779 - val_loss: 0.4159\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.419 - ETA: 0s - loss: 0.431 - 0s 874us/step - loss: 0.3809 - val_loss: 0.5003\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 9\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 5.23 - ETA: 0s - loss: 0.9113 - 1s 9ms/step - loss: 0.8883 - val_loss: 0.0935\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.109 - ETA: 0s - loss: 0.823 - 0s 864us/step - loss: 0.7078 - val_loss: 0.3113\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.657 - 0s 826us/step - loss: 0.5733 - val_loss: 0.5508\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.496 - 0s 826us/step - loss: 0.4940 - val_loss: 0.7726\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.966 - ETA: 0s - loss: 0.540 - 0s 940us/step - loss: 0.4845 - val_loss: 0.8637\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.312 - 0s 826us/step - loss: 0.4300 - val_loss: 0.8980\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.415 - 0s 798us/step - loss: 0.4275 - val_loss: 0.9330\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.425 - 0s 817us/step - loss: 0.4037 - val_loss: 0.8759\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.354 - 0s 788us/step - loss: 0.3636 - val_loss: 0.7239\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.382 - 0s 864us/step - loss: 0.3443 - val_loss: 0.8092\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.353 - 0s 921us/step - loss: 0.3343 - val_loss: 0.8122\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.463 - ETA: 0s - loss: 0.328 - 0s 817us/step - loss: 0.3385 - val_loss: 0.6459\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.284 - 0s 893us/step - loss: 0.3229 - val_loss: 0.7231\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.312 - 0s 836us/step - loss: 0.3172 - val_loss: 0.5916\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.342 - 0s 912us/step - loss: 0.2850 - val_loss: 0.6470\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.278 - 0s 855us/step - loss: 0.2909 - val_loss: 0.6768\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.432 - ETA: 0s - loss: 0.303 - 0s 874us/step - loss: 0.3073 - val_loss: 0.5478\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.278 - 0s 779us/step - loss: 0.2881 - val_loss: 0.6317\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.303 - 0s 760us/step - loss: 0.2868 - val_loss: 0.6120\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.361 - 0s 893us/step - loss: 0.2784 - val_loss: 0.4727\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.268 - 0s 883us/step - loss: 0.2565 - val_loss: 0.4400\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.295 - 0s 883us/step - loss: 0.2769 - val_loss: 0.5800\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.246 - 0s 855us/step - loss: 0.2765 - val_loss: 0.3474\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.289 - 0s 874us/step - loss: 0.2780 - val_loss: 0.4426\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.273 - 0s 1ms/step - loss: 0.2687 - val_loss: 0.3719\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.294 - 0s 940us/step - loss: 0.2589 - val_loss: 0.3977\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.292 - 0s 855us/step - loss: 0.2507 - val_loss: 0.3622\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.233 - 0s 969us/step - loss: 0.2483 - val_loss: 0.3932\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.203 - 0s 893us/step - loss: 0.2581 - val_loss: 0.3956\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.294 - 0s 941us/step - loss: 0.2412 - val_loss: 0.3694\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.176 - 0s 912us/step - loss: 0.2496 - val_loss: 0.3092\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 3.50 - ETA: 0s - loss: 0.6266 - 1s 9ms/step - loss: 0.8913 - val_loss: 0.0413\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.968 - 0s 940us/step - loss: 0.7221 - val_loss: 0.1857\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.691 - 0s 855us/step - loss: 0.6173 - val_loss: 0.3460\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.334 - 0s 1ms/step - loss: 0.5158 - val_loss: 0.4896\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.588 - 0s 912us/step - loss: 0.4662 - val_loss: 0.6792\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.426 - 0s 865us/step - loss: 0.4483 - val_loss: 0.6739\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.203 - 0s 912us/step - loss: 0.4332 - val_loss: 0.7694\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.480 - 0s 978us/step - loss: 0.4023 - val_loss: 0.8023\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.335 - 0s 826us/step - loss: 0.3495 - val_loss: 0.6287\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.766 - ETA: 0s - loss: 0.484 - 0s 883us/step - loss: 0.3569 - val_loss: 0.6535\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.252 - 0s 817us/step - loss: 0.3400 - val_loss: 0.6914\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.380 - 0s 779us/step - loss: 0.3317 - val_loss: 0.5565\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.344 - 0s 807us/step - loss: 0.3179 - val_loss: 0.6316\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.320 - 0s 1ms/step - loss: 0.3325 - val_loss: 0.6330\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.326 - 0s 902us/step - loss: 0.2958 - val_loss: 0.6304\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.293 - 0s 817us/step - loss: 0.3028 - val_loss: 0.6852\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.809 - ETA: 0s - loss: 0.338 - 0s 845us/step - loss: 0.2890 - val_loss: 0.6897\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.531 - ETA: 0s - loss: 0.208 - 0s 903us/step - loss: 0.3235 - val_loss: 0.6005\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.349 - 0s 902us/step - loss: 0.3168 - val_loss: 0.8531\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.410 - 0s 883us/step - loss: 0.3210 - val_loss: 0.6384\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.695 - ETA: 0s - loss: 0.286 - 0s 855us/step - loss: 0.2899 - val_loss: 0.5579\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.476 - ETA: 0s - loss: 0.269 - 0s 826us/step - loss: 0.2740 - val_loss: 0.6202\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.919 - ETA: 0s - loss: 0.237 - 0s 874us/step - loss: 0.2574 - val_loss: 0.6376\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.519 - ETA: 0s - loss: 0.272 - 0s 864us/step - loss: 0.2813 - val_loss: 0.6643\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.787 - ETA: 0s - loss: 0.280 - 0s 864us/step - loss: 0.2610 - val_loss: 0.7667\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.900 - ETA: 0s - loss: 0.287 - 0s 883us/step - loss: 0.2816 - val_loss: 0.6927\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.161 - 0s 874us/step - loss: 0.2729 - val_loss: 0.7417\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.242 - 0s 912us/step - loss: 0.2406 - val_loss: 0.7647\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.276 - 0s 807us/step - loss: 0.2460 - val_loss: 0.6788\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.725 - ETA: 0s - loss: 0.288 - 0s 845us/step - loss: 0.2577 - val_loss: 0.6903\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.188 - 0s 874us/step - loss: 0.2271 - val_loss: 0.6981\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 11\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 2.19 - ETA: 0s - loss: 0.7607 - 1s 9ms/step - loss: 0.8857 - val_loss: 0.0984\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.999 - ETA: 0s - loss: 0.563 - 0s 931us/step - loss: 0.6981 - val_loss: 0.3228\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.003 - ETA: 0s - loss: 0.729 - 0s 855us/step - loss: 0.6110 - val_loss: 0.6238\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.711 - ETA: 0s - loss: 0.447 - 0s 931us/step - loss: 0.5513 - val_loss: 0.7424\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.707 - ETA: 0s - loss: 0.570 - 0s 883us/step - loss: 0.5344 - val_loss: 1.0182\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.574 - 0s 807us/step - loss: 0.5058 - val_loss: 1.2524\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.573 - 0s 931us/step - loss: 0.4919 - val_loss: 1.1305\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.535 - ETA: 0s - loss: 0.446 - 0s 874us/step - loss: 0.4532 - val_loss: 1.2913\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.398 - ETA: 0s - loss: 0.311 - 0s 864us/step - loss: 0.4496 - val_loss: 1.3362\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.428 - 0s 940us/step - loss: 0.4444 - val_loss: 1.3233\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.366 - 0s 864us/step - loss: 0.4387 - val_loss: 1.3318\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.289 - 0s 969us/step - loss: 0.4422 - val_loss: 1.2496\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.492 - 0s 912us/step - loss: 0.4214 - val_loss: 1.3423\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.317 - 0s 1ms/step - loss: 0.4214 - val_loss: 1.3240\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.370 - 0s 959us/step - loss: 0.4132 - val_loss: 1.5313\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.479 - ETA: 0s - loss: 0.251 - 0s 978us/step - loss: 0.4008 - val_loss: 1.5204\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.332 - ETA: 0s - loss: 0.436 - 0s 931us/step - loss: 0.4017 - val_loss: 1.5745\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.489 - ETA: 0s - loss: 0.627 - 0s 1ms/step - loss: 0.4076 - val_loss: 1.6037\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.264 - 0s 1ms/step - loss: 0.3998 - val_loss: 1.4820\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.358 - 0s 874us/step - loss: 0.4019 - val_loss: 1.6906\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.635 - ETA: 0s - loss: 0.433 - 0s 893us/step - loss: 0.4015 - val_loss: 1.5345\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.324 - 0s 912us/step - loss: 0.3923 - val_loss: 1.6475\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.517 - ETA: 0s - loss: 0.446 - 0s 950us/step - loss: 0.3817 - val_loss: 1.6268\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.311 - 0s 959us/step - loss: 0.3966 - val_loss: 1.4909\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.298 - 0s 922us/step - loss: 0.3882 - val_loss: 1.5803\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.762 - ETA: 0s - loss: 0.421 - 0s 864us/step - loss: 0.3779 - val_loss: 1.6523\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.385 - 0s 864us/step - loss: 0.3902 - val_loss: 1.7808\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.460 - ETA: 0s - loss: 0.407 - 0s 817us/step - loss: 0.3738 - val_loss: 1.6388\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.411 - 0s 807us/step - loss: 0.3847 - val_loss: 1.6797\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.479 - ETA: 0s - loss: 0.411 - 0s 845us/step - loss: 0.3682 - val_loss: 1.6754\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.382 - 0s 798us/step - loss: 0.3786 - val_loss: 1.6590\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 12\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.68 - ETA: 0s - loss: 0.7844 - 1s 9ms/step - loss: 0.7815 - val_loss: 0.3143\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.568 - ETA: 0s - loss: 0.505 - 0s 912us/step - loss: 0.5952 - val_loss: 0.6888\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.452 - 0s 960us/step - loss: 0.4879 - val_loss: 1.2606\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.223 - ETA: 0s - loss: 0.413 - 0s 855us/step - loss: 0.4329 - val_loss: 1.4758\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.388 - ETA: 0s - loss: 0.351 - 0s 874us/step - loss: 0.4011 - val_loss: 1.4775\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.360 - 0s 893us/step - loss: 0.3884 - val_loss: 1.7836\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.725 - ETA: 0s - loss: 0.369 - 0s 845us/step - loss: 0.3665 - val_loss: 1.6240\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.628 - ETA: 0s - loss: 0.386 - 0s 893us/step - loss: 0.3827 - val_loss: 1.6790\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.783 - ETA: 0s - loss: 0.399 - 0s 864us/step - loss: 0.3638 - val_loss: 1.5523\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.411 - ETA: 0s - loss: 0.325 - 0s 836us/step - loss: 0.3478 - val_loss: 1.5027\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.593 - ETA: 0s - loss: 0.397 - 0s 902us/step - loss: 0.3443 - val_loss: 1.5458\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.645 - ETA: 0s - loss: 0.376 - 0s 997us/step - loss: 0.3534 - val_loss: 1.5585\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.308 - 0s 902us/step - loss: 0.3402 - val_loss: 1.4583\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.365 - 0s 874us/step - loss: 0.3425 - val_loss: 1.5856\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.343 - 0s 826us/step - loss: 0.3397 - val_loss: 1.4772\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.075 - ETA: 0s - loss: 0.339 - 0s 883us/step - loss: 0.3399 - val_loss: 1.4327\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.647 - ETA: 0s - loss: 0.279 - 0s 845us/step - loss: 0.3376 - val_loss: 1.4625\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.319 - 0s 921us/step - loss: 0.3207 - val_loss: 1.4882\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.189 - ETA: 0s - loss: 0.327 - 0s 855us/step - loss: 0.3416 - val_loss: 1.3937\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.336 - 0s 864us/step - loss: 0.3216 - val_loss: 1.4755\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.338 - 0s 817us/step - loss: 0.3395 - val_loss: 1.3065\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.366 - 0s 1ms/step - loss: 0.3409 - val_loss: 1.3148\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.878 - ETA: 0s - loss: 0.364 - 0s 845us/step - loss: 0.3312 - val_loss: 1.3473\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.294 - 0s 893us/step - loss: 0.3226 - val_loss: 1.1622\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.547 - ETA: 0s - loss: 0.349 - 0s 845us/step - loss: 0.3284 - val_loss: 1.3417\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.696 - ETA: 0s - loss: 0.352 - 0s 883us/step - loss: 0.3196 - val_loss: 1.5138\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.333 - 0s 883us/step - loss: 0.3170 - val_loss: 1.2745\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.259 - 0s 921us/step - loss: 0.3091 - val_loss: 1.3665\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.358 - 0s 931us/step - loss: 0.3192 - val_loss: 1.5710\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.354 - 0s 845us/step - loss: 0.3183 - val_loss: 1.3008\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.291 - 0s 912us/step - loss: 0.3198 - val_loss: 1.3848\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 13\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 17s - loss: 0.48 - ETA: 0s - loss: 0.8816 - 1s 9ms/step - loss: 0.8932 - val_loss: 0.0538\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.664 - 0s 931us/step - loss: 0.7076 - val_loss: 0.2209\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.114 - ETA: 0s - loss: 0.530 - 0s 855us/step - loss: 0.6125 - val_loss: 0.4023\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.551 - 0s 864us/step - loss: 0.5726 - val_loss: 0.4665\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.177 - ETA: 0s - loss: 0.504 - ETA: 0s - loss: 0.518 - 0s 1ms/step - loss: 0.5315 - val_loss: 0.6268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.432 - ETA: 0s - loss: 0.523 - 0s 1ms/step - loss: 0.5189 - val_loss: 0.7922\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.509 - 0s 864us/step - loss: 0.4998 - val_loss: 0.7856\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.533 - 0s 874us/step - loss: 0.4879 - val_loss: 0.8753\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.755 - ETA: 0s - loss: 0.359 - 0s 1ms/step - loss: 0.4816 - val_loss: 0.8880\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.503 - 0s 826us/step - loss: 0.4759 - val_loss: 0.9297\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.673 - ETA: 0s - loss: 0.444 - 0s 912us/step - loss: 0.4750 - val_loss: 0.9233\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.377 - 0s 864us/step - loss: 0.4634 - val_loss: 0.9672\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.616 - ETA: 0s - loss: 0.474 - 0s 874us/step - loss: 0.4513 - val_loss: 1.0055\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.512 - 0s 826us/step - loss: 0.4473 - val_loss: 1.0922\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.775 - ETA: 0s - loss: 0.363 - 0s 864us/step - loss: 0.4657 - val_loss: 1.0657\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.465 - 0s 834us/step - loss: 0.4568 - val_loss: 1.2305\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.506 - 0s 865us/step - loss: 0.4364 - val_loss: 1.2496\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.009 - ETA: 0s - loss: 0.493 - 0s 864us/step - loss: 0.4370 - val_loss: 1.2222\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.370 - 0s 865us/step - loss: 0.4433 - val_loss: 1.3027\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.381 - ETA: 0s - loss: 0.471 - 0s 836us/step - loss: 0.4456 - val_loss: 1.2966\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.394 - 0s 1ms/step - loss: 0.4326 - val_loss: 1.3948\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.460 - 0s 855us/step - loss: 0.4369 - val_loss: 1.2706\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.550 - ETA: 0s - loss: 0.483 - 0s 855us/step - loss: 0.4334 - val_loss: 1.4143\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.545 - ETA: 0s - loss: 0.406 - 0s 845us/step - loss: 0.4263 - val_loss: 1.3156\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.429 - ETA: 0s - loss: 0.502 - 0s 912us/step - loss: 0.4250 - val_loss: 1.4916\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.461 - 0s 864us/step - loss: 0.4366 - val_loss: 1.4941\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.424 - 0s 912us/step - loss: 0.4389 - val_loss: 1.4972\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.588 - ETA: 0s - loss: 0.353 - 0s 893us/step - loss: 0.4397 - val_loss: 1.5741\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.327 - 0s 893us/step - loss: 0.4332 - val_loss: 1.3261\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.559 - ETA: 0s - loss: 0.429 - 0s 874us/step - loss: 0.4233 - val_loss: 1.6188\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.469 - 0s 958us/step - loss: 0.4129 - val_loss: 1.6397\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 14\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.64 - ETA: 0s - loss: 0.7699 - 1s 9ms/step - loss: 0.8791 - val_loss: 0.0826\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.009 - ETA: 0s - loss: 0.731 - 0s 940us/step - loss: 0.7368 - val_loss: 0.2656\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.720 - 0s 883us/step - loss: 0.6797 - val_loss: 0.4988\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.669 - 0s 864us/step - loss: 0.6287 - val_loss: 0.6599\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.417 - ETA: 0s - loss: 0.581 - 0s 874us/step - loss: 0.5852 - val_loss: 0.7595\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.522 - 0s 855us/step - loss: 0.5580 - val_loss: 0.9583\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.680 - ETA: 0s - loss: 0.523 - 0s 855us/step - loss: 0.5402 - val_loss: 1.0057\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.445 - 0s 883us/step - loss: 0.5240 - val_loss: 1.1027\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.379 - 0s 938us/step - loss: 0.5094 - val_loss: 1.1634\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.559 - 0s 912us/step - loss: 0.5031 - val_loss: 1.3442\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.542 - 0s 889us/step - loss: 0.4995 - val_loss: 1.4544\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.747 - ETA: 0s - loss: 0.478 - 0s 870us/step - loss: 0.4791 - val_loss: 1.4999\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.523 - 0s 937us/step - loss: 0.4944 - val_loss: 1.5552\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.453 - ETA: 0s - loss: 0.531 - 0s 897us/step - loss: 0.4832 - val_loss: 1.5663\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.396 - 0s 855us/step - loss: 0.4703 - val_loss: 1.6178\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.427 - 0s 883us/step - loss: 0.4717 - val_loss: 1.7525\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.494 - 0s 883us/step - loss: 0.4704 - val_loss: 1.7221\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.755 - ETA: 0s - loss: 0.467 - 0s 912us/step - loss: 0.4795 - val_loss: 1.6692\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.507 - 0s 902us/step - loss: 0.4601 - val_loss: 1.8236\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.418 - 0s 959us/step - loss: 0.4696 - val_loss: 1.5421\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.458 - 0s 969us/step - loss: 0.4554 - val_loss: 1.6493\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.475 - 0s 883us/step - loss: 0.4742 - val_loss: 1.7867\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.403 - 0s 931us/step - loss: 0.4715 - val_loss: 1.5064\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.535 - 0s 864us/step - loss: 0.4773 - val_loss: 1.6488\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.405 - 0s 1ms/step - loss: 0.4509 - val_loss: 1.7079\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.795 - ETA: 0s - loss: 0.497 - 0s 826us/step - loss: 0.4585 - val_loss: 1.6780\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 0.555 - 0s 881us/step - loss: 0.4631 - val_loss: 1.9019\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.799 - ETA: 0s - loss: 0.401 - 0s 863us/step - loss: 0.4500 - val_loss: 1.8060\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.613 - ETA: 0s - loss: 0.438 - 0s 1ms/step - loss: 0.4487 - val_loss: 1.9348\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.394 - 0s 855us/step - loss: 0.4473 - val_loss: 2.0997\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.436 - 0s 826us/step - loss: 0.4398 - val_loss: 2.0618\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 15\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.74 - ETA: 0s - loss: 0.9191 - 1s 9ms/step - loss: 0.8879 - val_loss: 0.0901\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.807 - 0s 940us/step - loss: 0.7504 - val_loss: 0.2480\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.582 - ETA: 0s - loss: 0.746 - 0s 812us/step - loss: 0.6748 - val_loss: 0.3566\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.908 - ETA: 0s - loss: 0.739 - 0s 803us/step - loss: 0.6521 - val_loss: 0.3770\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.480 - ETA: 0s - loss: 0.700 - 0s 817us/step - loss: 0.6278 - val_loss: 0.3862\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.321 - 0s 846us/step - loss: 0.6065 - val_loss: 0.3944\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.576 - 0s 845us/step - loss: 0.5958 - val_loss: 0.3863\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.645 - ETA: 0s - loss: 0.645 - 0s 836us/step - loss: 0.5927 - val_loss: 0.4298\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.976 - ETA: 0s - loss: 0.655 - 0s 912us/step - loss: 0.5749 - val_loss: 0.3654\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.361 - ETA: 0s - loss: 0.544 - 0s 874us/step - loss: 0.5690 - val_loss: 0.3595\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.516 - 0s 826us/step - loss: 0.5625 - val_loss: 0.3456\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.619 - 0s 845us/step - loss: 0.5341 - val_loss: 0.3198\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.689 - 0s 883us/step - loss: 0.5328 - val_loss: 0.3196\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.571 - 0s 874us/step - loss: 0.5315 - val_loss: 0.3135\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.834 - ETA: 0s - loss: 0.468 - 0s 845us/step - loss: 0.5133 - val_loss: 0.3263\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.598 - ETA: 0s - loss: 0.642 - 0s 921us/step - loss: 0.5251 - val_loss: 0.3236\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.584 - 0s 912us/step - loss: 0.5064 - val_loss: 0.2487\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.420 - 0s 912us/step - loss: 0.5159 - val_loss: 0.2703\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.535 - 0s 883us/step - loss: 0.5010 - val_loss: 0.2677\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.902 - ETA: 0s - loss: 0.509 - 0s 883us/step - loss: 0.5023 - val_loss: 0.2671\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.380 - 0s 893us/step - loss: 0.4979 - val_loss: 0.2775\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.518 - 0s 826us/step - loss: 0.4931 - val_loss: 0.2558\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.558 - 0s 845us/step - loss: 0.4949 - val_loss: 0.2566\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.388 - ETA: 0s - loss: 0.516 - 0s 931us/step - loss: 0.4996 - val_loss: 0.2786\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.341 - 0s 845us/step - loss: 0.4835 - val_loss: 0.2625\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.614 - ETA: 0s - loss: 0.534 - 0s 893us/step - loss: 0.4911 - val_loss: 0.2795\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.448 - 0s 855us/step - loss: 0.4749 - val_loss: 0.2726\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.390 - 0s 836us/step - loss: 0.4738 - val_loss: 0.2647\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.478 - 0s 874us/step - loss: 0.4820 - val_loss: 0.2433\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.524 - 0s 912us/step - loss: 0.4826 - val_loss: 0.2498\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.443 - 0s 988us/step - loss: 0.4854 - val_loss: 0.2476\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 16\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 1.06 - ETA: 0s - loss: 0.7980 - 1s 9ms/step - loss: 0.8685 - val_loss: 0.1546\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.632 - ETA: 0s - loss: 0.556 - 0s 931us/step - loss: 0.6925 - val_loss: 0.4168\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.581 - 0s 864us/step - loss: 0.5994 - val_loss: 0.7048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.646 - 0s 931us/step - loss: 0.5689 - val_loss: 0.8831\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.477 - 0s 826us/step - loss: 0.5632 - val_loss: 0.7973\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.765 - ETA: 0s - loss: 0.616 - 0s 817us/step - loss: 0.5511 - val_loss: 0.9042\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.871 - ETA: 0s - loss: 0.528 - 0s 912us/step - loss: 0.5417 - val_loss: 0.9179\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.451 - ETA: 0s - loss: 0.537 - 0s 845us/step - loss: 0.5434 - val_loss: 0.9489\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.922 - ETA: 0s - loss: 0.533 - 0s 874us/step - loss: 0.5121 - val_loss: 0.8894\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.727 - ETA: 0s - loss: 0.502 - 0s 845us/step - loss: 0.4998 - val_loss: 0.9948\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.547 - 0s 779us/step - loss: 0.5144 - val_loss: 0.9934\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.571 - ETA: 0s - loss: 0.450 - 0s 807us/step - loss: 0.5202 - val_loss: 0.8483\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.439 - 0s 855us/step - loss: 0.4959 - val_loss: 1.0434\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.443 - 0s 798us/step - loss: 0.4854 - val_loss: 1.1153\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.662 - ETA: 0s - loss: 0.437 - 0s 845us/step - loss: 0.4904 - val_loss: 1.1733\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.512 - 0s 836us/step - loss: 0.5006 - val_loss: 1.0929\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.536 - ETA: 0s - loss: 0.436 - 0s 855us/step - loss: 0.4774 - val_loss: 1.2040\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.376 - 0s 959us/step - loss: 0.4757 - val_loss: 1.2159\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.382 - 0s 855us/step - loss: 0.4617 - val_loss: 1.2380\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.507 - ETA: 0s - loss: 0.517 - 0s 836us/step - loss: 0.4651 - val_loss: 1.3737\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.293 - ETA: 0s - loss: 0.427 - 0s 836us/step - loss: 0.4663 - val_loss: 1.2537\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.512 - 0s 855us/step - loss: 0.4673 - val_loss: 1.4178\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.519 - 0s 893us/step - loss: 0.4685 - val_loss: 1.5546\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.672 - ETA: 0s - loss: 0.428 - 0s 826us/step - loss: 0.4571 - val_loss: 1.3501\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.815 - ETA: 0s - loss: 0.515 - 0s 836us/step - loss: 0.4520 - val_loss: 1.5616\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.383 - 0s 893us/step - loss: 0.4565 - val_loss: 1.5070\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.482 - 0s 874us/step - loss: 0.4402 - val_loss: 1.5733\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.472 - ETA: 0s - loss: 0.407 - 0s 902us/step - loss: 0.4672 - val_loss: 1.5600\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.458 - 0s 874us/step - loss: 0.4409 - val_loss: 1.5768\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.433 - 0s 940us/step - loss: 0.4367 - val_loss: 1.6595\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.492 - ETA: 0s - loss: 0.469 - 0s 1ms/step - loss: 0.4459 - val_loss: 1.6007\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 17\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.71 - ETA: 0s - loss: 0.9525 - 1s 9ms/step - loss: 0.9057 - val_loss: 0.0825\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.981 - ETA: 0s - loss: 0.710 - 0s 807us/step - loss: 0.7864 - val_loss: 0.2338\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.712 - ETA: 0s - loss: 0.891 - 0s 893us/step - loss: 0.7304 - val_loss: 0.3897\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.564 - 0s 978us/step - loss: 0.7075 - val_loss: 0.5141\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.411 - ETA: 0s - loss: 0.528 - 0s 931us/step - loss: 0.7044 - val_loss: 0.6056\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.795 - 0s 940us/step - loss: 0.6885 - val_loss: 0.6526\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.589 - ETA: 0s - loss: 0.803 - 0s 931us/step - loss: 0.6737 - val_loss: 0.6262\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.828 - ETA: 0s - loss: 0.641 - 0s 998us/step - loss: 0.6579 - val_loss: 0.7112\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.357 - ETA: 0s - loss: 0.509 - 0s 855us/step - loss: 0.6393 - val_loss: 0.5056\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.730 - 0s 921us/step - loss: 0.6502 - val_loss: 0.6226\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.767 - 0s 940us/step - loss: 0.6370 - val_loss: 0.6282\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.440 - ETA: 0s - loss: 0.683 - 0s 893us/step - loss: 0.6407 - val_loss: 0.8066\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.308 - ETA: 0s - loss: 0.777 - 0s 903us/step - loss: 0.6110 - val_loss: 0.7368\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.675 - 0s 893us/step - loss: 0.6288 - val_loss: 0.7440\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.381 - ETA: 0s - loss: 0.693 - 0s 940us/step - loss: 0.6309 - val_loss: 0.7769\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.574 - 0s 883us/step - loss: 0.6026 - val_loss: 0.7510\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.515 - 0s 921us/step - loss: 0.6129 - val_loss: 0.8340\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.795 - ETA: 0s - loss: 0.702 - 0s 931us/step - loss: 0.5991 - val_loss: 0.8497\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.445 - 0s 912us/step - loss: 0.5995 - val_loss: 0.8597\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.623 - 0s 883us/step - loss: 0.6131 - val_loss: 0.8265\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.661 - 0s 864us/step - loss: 0.5974 - val_loss: 0.9347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.540 - 0s 864us/step - loss: 0.5882 - val_loss: 0.9596\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.603 - 0s 836us/step - loss: 0.5859 - val_loss: 0.6835\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.496 - 0s 769us/step - loss: 0.5907 - val_loss: 0.8475\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.562 - 0s 864us/step - loss: 0.5844 - val_loss: 0.8613\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.619 - 0s 817us/step - loss: 0.5766 - val_loss: 0.9686\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.691 - 0s 912us/step - loss: 0.5703 - val_loss: 0.9734\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.648 - ETA: 0s - loss: 0.500 - 0s 921us/step - loss: 0.5888 - val_loss: 1.0397\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.420 - ETA: 0s - loss: 0.530 - 0s 846us/step - loss: 0.5764 - val_loss: 0.9993\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.560 - ETA: 0s - loss: 0.558 - 0s 788us/step - loss: 0.5761 - val_loss: 1.0537\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.801 - ETA: 0s - loss: 0.509 - 0s 807us/step - loss: 0.5803 - val_loss: 0.9907\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 18\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.59 - ETA: 0s - loss: 0.9427 - 1s 9ms/step - loss: 0.9020 - val_loss: 0.1211\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.866 - 0s 950us/step - loss: 0.7492 - val_loss: 0.3545\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.601 - 0s 855us/step - loss: 0.6568 - val_loss: 0.5656\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.837 - ETA: 0s - loss: 0.566 - 0s 874us/step - loss: 0.6064 - val_loss: 0.7212\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.580 - 0s 864us/step - loss: 0.5676 - val_loss: 0.9438\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.128 - ETA: 0s - loss: 0.631 - 0s 864us/step - loss: 0.5656 - val_loss: 1.0280\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.027 - ETA: 0s - loss: 0.533 - 0s 855us/step - loss: 0.5314 - val_loss: 1.0761\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.622 - 0s 864us/step - loss: 0.5281 - val_loss: 1.0937\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.415 - 0s 883us/step - loss: 0.5255 - val_loss: 1.0536\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.506 - 0s 893us/step - loss: 0.5256 - val_loss: 1.0141\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.335 - ETA: 0s - loss: 0.509 - 0s 883us/step - loss: 0.5348 - val_loss: 1.0875\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.132 - ETA: 0s - loss: 0.564 - 0s 874us/step - loss: 0.5342 - val_loss: 1.0722\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.531 - 0s 874us/step - loss: 0.5137 - val_loss: 0.9956\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.441 - 0s 893us/step - loss: 0.4921 - val_loss: 0.9003\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.419 - ETA: 0s - loss: 0.552 - 0s 836us/step - loss: 0.5112 - val_loss: 1.0494\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.727 - ETA: 0s - loss: 0.561 - 0s 874us/step - loss: 0.5053 - val_loss: 1.0038\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.595 - 0s 893us/step - loss: 0.4986 - val_loss: 1.0935\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.591 - 0s 845us/step - loss: 0.5140 - val_loss: 0.9799\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.610 - ETA: 0s - loss: 0.357 - 0s 883us/step - loss: 0.4850 - val_loss: 0.9342\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.437 - ETA: 0s - loss: 0.478 - 0s 883us/step - loss: 0.4912 - val_loss: 0.9538\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.161 - ETA: 0s - loss: 0.576 - 0s 855us/step - loss: 0.4918 - val_loss: 1.0345\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.398 - 0s 902us/step - loss: 0.4918 - val_loss: 0.8455\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.446 - 0s 807us/step - loss: 0.4951 - val_loss: 0.9261\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.470 - ETA: 0s - loss: 0.534 - 0s 874us/step - loss: 0.5051 - val_loss: 0.9535\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.466 - 0s 883us/step - loss: 0.4891 - val_loss: 0.8790\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.522 - ETA: 0s - loss: 0.535 - 0s 893us/step - loss: 0.4821 - val_loss: 0.8615\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.740 - ETA: 0s - loss: 0.491 - 0s 940us/step - loss: 0.4941 - val_loss: 0.8472\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.551 - 0s 893us/step - loss: 0.4973 - val_loss: 0.9745\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.523 - 0s 874us/step - loss: 0.4845 - val_loss: 0.9368\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.536 - 0s 921us/step - loss: 0.4880 - val_loss: 1.1014\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.571 - 0s 997us/step - loss: 0.4886 - val_loss: 0.9316\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 19\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.77 - ETA: 0s - loss: 1.1023 - 1s 9ms/step - loss: 0.9248 - val_loss: 0.0582\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.799 - ETA: 0s - loss: 0.708 - 0s 903us/step - loss: 0.7300 - val_loss: 0.2606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.395 - 0s 884us/step - loss: 0.6233 - val_loss: 0.5212\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.489 - ETA: 0s - loss: 0.610 - 0s 893us/step - loss: 0.5730 - val_loss: 0.8356\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.415 - 0s 883us/step - loss: 0.5374 - val_loss: 0.8644\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.939 - ETA: 0s - loss: 0.483 - 0s 864us/step - loss: 0.5438 - val_loss: 0.9516\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.525 - ETA: 0s - loss: 0.385 - 0s 959us/step - loss: 0.5088 - val_loss: 0.8399\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.588 - 0s 902us/step - loss: 0.5127 - val_loss: 0.9986\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.458 - 0s 836us/step - loss: 0.4923 - val_loss: 0.8580\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.597 - 0s 893us/step - loss: 0.5278 - val_loss: 0.8710\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.618 - ETA: 0s - loss: 0.430 - 0s 817us/step - loss: 0.4895 - val_loss: 0.9418\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.421 - 0s 902us/step - loss: 0.4945 - val_loss: 0.8495\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.508 - ETA: 0s - loss: 0.517 - 0s 845us/step - loss: 0.4887 - val_loss: 0.9979\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.792 - ETA: 0s - loss: 0.531 - 0s 893us/step - loss: 0.4751 - val_loss: 0.8873\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.746 - ETA: 0s - loss: 0.538 - 0s 836us/step - loss: 0.5092 - val_loss: 0.8048\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.803 - ETA: 0s - loss: 0.495 - 0s 883us/step - loss: 0.4745 - val_loss: 0.8933\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.801 - ETA: 0s - loss: 0.443 - 0s 940us/step - loss: 0.4732 - val_loss: 0.8764\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.665 - ETA: 0s - loss: 0.522 - 0s 912us/step - loss: 0.4863 - val_loss: 0.9806\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.522 - 0s 855us/step - loss: 0.4649 - val_loss: 0.8881\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.446 - 0s 893us/step - loss: 0.4661 - val_loss: 0.7757\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.977 - ETA: 0s - loss: 0.434 - 0s 864us/step - loss: 0.4846 - val_loss: 0.8401\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.503 - 0s 902us/step - loss: 0.4762 - val_loss: 0.8349\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.486 - ETA: 0s - loss: 0.454 - 0s 874us/step - loss: 0.4582 - val_loss: 0.7856\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.496 - 0s 921us/step - loss: 0.4562 - val_loss: 0.6903\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.574 - ETA: 0s - loss: 0.437 - 0s 912us/step - loss: 0.4630 - val_loss: 0.7123\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.547 - 0s 912us/step - loss: 0.4673 - val_loss: 0.6452\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.477 - 0s 950us/step - loss: 0.4679 - val_loss: 0.5266\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.475 - 0s 836us/step - loss: 0.4545 - val_loss: 0.6442\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.389 - 0s 855us/step - loss: 0.4558 - val_loss: 0.6976\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.949 - ETA: 0s - loss: 0.567 - 0s 902us/step - loss: 0.4569 - val_loss: 0.5426\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.457 - 0s 893us/step - loss: 0.4414 - val_loss: 0.6278\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 20\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.53 - ETA: 0s - loss: 0.9143 - 1s 9ms/step - loss: 0.9289 - val_loss: 0.1122\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.203 - ETA: 0s - loss: 0.878 - 0s 912us/step - loss: 0.8056 - val_loss: 0.3290\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.287 - ETA: 0s - loss: 0.889 - 0s 921us/step - loss: 0.7625 - val_loss: 0.4951\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.437 - ETA: 0s - loss: 0.772 - 0s 845us/step - loss: 0.7324 - val_loss: 0.5739\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.810 - 0s 855us/step - loss: 0.7177 - val_loss: 0.6841\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.707 - 0s 893us/step - loss: 0.7068 - val_loss: 0.7072\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.648 - 0s 855us/step - loss: 0.6945 - val_loss: 0.6781\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.340 - ETA: 0s - loss: 0.808 - 0s 836us/step - loss: 0.6850 - val_loss: 0.8061\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.630 - 0s 893us/step - loss: 0.6942 - val_loss: 0.7505\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.668 - ETA: 0s - loss: 0.709 - 0s 893us/step - loss: 0.6591 - val_loss: 0.9041\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.682 - ETA: 0s - loss: 0.467 - 0s 940us/step - loss: 0.6664 - val_loss: 0.9844\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.761 - ETA: 0s - loss: 0.738 - 0s 921us/step - loss: 0.6411 - val_loss: 0.8461\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.644 - 0s 902us/step - loss: 0.6446 - val_loss: 0.8766\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.614 - ETA: 0s - loss: 0.745 - 0s 883us/step - loss: 0.6638 - val_loss: 0.8569\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.821 - ETA: 0s - loss: 0.735 - 0s 893us/step - loss: 0.6450 - val_loss: 0.8424\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.716 - 0s 988us/step - loss: 0.6384 - val_loss: 1.0017\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.753 - 0s 931us/step - loss: 0.6536 - val_loss: 0.9606\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.272 - ETA: 0s - loss: 0.687 - 0s 883us/step - loss: 0.6413 - val_loss: 1.0288\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.102 - ETA: 0s - loss: 0.621 - 0s 921us/step - loss: 0.6344 - val_loss: 1.0115\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.820 - 0s 950us/step - loss: 0.6346 - val_loss: 1.0804\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.359 - ETA: 0s - loss: 0.652 - 0s 836us/step - loss: 0.6305 - val_loss: 1.0238\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.533 - ETA: 0s - loss: 0.674 - 0s 855us/step - loss: 0.6371 - val_loss: 1.0807\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.724 - 0s 836us/step - loss: 0.6171 - val_loss: 1.1545\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.751 - ETA: 0s - loss: 0.719 - 0s 817us/step - loss: 0.6363 - val_loss: 1.1808\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.902 - ETA: 0s - loss: 0.652 - 0s 855us/step - loss: 0.6209 - val_loss: 1.1629\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.536 - ETA: 0s - loss: 0.719 - 0s 902us/step - loss: 0.6398 - val_loss: 1.2017\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.605 - 0s 855us/step - loss: 0.6225 - val_loss: 1.1727\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.333 - ETA: 0s - loss: 0.613 - 0s 950us/step - loss: 0.6258 - val_loss: 1.2274\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.593 - 0s 903us/step - loss: 0.6096 - val_loss: 1.1716\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.606 - ETA: 0s - loss: 0.495 - 0s 874us/step - loss: 0.6217 - val_loss: 1.2605\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.394 - ETA: 0s - loss: 0.665 - 0s 826us/step - loss: 0.6226 - val_loss: 1.6997\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 21\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.71 - ETA: 0s - loss: 1.0460 - 1s 9ms/step - loss: 0.9327 - val_loss: 0.1349\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.506 - ETA: 0s - loss: 0.807 - 0s 836us/step - loss: 0.7512 - val_loss: 0.5009\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.753 - ETA: 0s - loss: 0.708 - 0s 836us/step - loss: 0.6679 - val_loss: 0.9184\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.629 - 0s 836us/step - loss: 0.6396 - val_loss: 1.1606\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.494 - 0s 893us/step - loss: 0.6249 - val_loss: 1.0764\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.653 - ETA: 0s - loss: 0.653 - 0s 1ms/step - loss: 0.6065 - val_loss: 1.1429\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.365 - ETA: 0s - loss: 0.702 - 0s 874us/step - loss: 0.6109 - val_loss: 1.2447\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.538 - ETA: 0s - loss: 0.634 - 0s 855us/step - loss: 0.5991 - val_loss: 1.2830\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.565 - 0s 836us/step - loss: 0.5959 - val_loss: 1.1441\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.473 - ETA: 0s - loss: 0.631 - 0s 845us/step - loss: 0.6144 - val_loss: 1.0991\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.561 - 0s 874us/step - loss: 0.6010 - val_loss: 1.1458\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.543 - 0s 883us/step - loss: 0.6018 - val_loss: 1.1513\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.524 - 0s 893us/step - loss: 0.5958 - val_loss: 1.1468\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.663 - 0s 874us/step - loss: 0.5850 - val_loss: 0.9416\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.773 - ETA: 0s - loss: 0.558 - 0s 912us/step - loss: 0.5856 - val_loss: 1.0280\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.018 - ETA: 0s - loss: 0.672 - 0s 997us/step - loss: 0.5757 - val_loss: 1.1798\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.876 - ETA: 0s - loss: 0.552 - 0s 883us/step - loss: 0.5830 - val_loss: 0.8890\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.563 - ETA: 0s - loss: 0.618 - 0s 950us/step - loss: 0.5938 - val_loss: 1.0063\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.490 - 0s 864us/step - loss: 0.5812 - val_loss: 0.9505\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.540 - ETA: 0s - loss: 0.475 - 0s 874us/step - loss: 0.5737 - val_loss: 0.9701\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.653 - 0s 883us/step - loss: 0.5861 - val_loss: 0.9283\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.784 - ETA: 0s - loss: 0.664 - 0s 883us/step - loss: 0.5582 - val_loss: 1.0408\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.487 - 0s 874us/step - loss: 0.5834 - val_loss: 0.9896\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.480 - 0s 921us/step - loss: 0.5667 - val_loss: 0.9352\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.599 - 0s 931us/step - loss: 0.5790 - val_loss: 0.9215\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.861 - ETA: 0s - loss: 0.557 - 0s 883us/step - loss: 0.5679 - val_loss: 1.0297\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.615 - ETA: 0s - loss: 0.600 - 0s 959us/step - loss: 0.5654 - val_loss: 1.0273\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.569 - ETA: 0s - loss: 0.539 - 0s 912us/step - loss: 0.5788 - val_loss: 0.9540\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.298 - ETA: 0s - loss: 0.580 - 0s 883us/step - loss: 0.5622 - val_loss: 1.0408\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.526 - 0s 874us/step - loss: 0.5756 - val_loss: 0.7837\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.689 - 0s 912us/step - loss: 0.5638 - val_loss: 0.8663\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 22\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.30 - ETA: 0s - loss: 1.0166 - 1s 9ms/step - loss: 0.8588 - val_loss: 0.1427\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.830 - 0s 912us/step - loss: 0.6944 - val_loss: 0.4346\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.178 - 0s 874us/step - loss: 0.6671 - val_loss: 0.6093\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.808 - 0s 893us/step - loss: 0.6453 - val_loss: 0.8337\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.732 - 0s 874us/step - loss: 0.6177 - val_loss: 0.8809\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.703 - 0s 874us/step - loss: 0.5863 - val_loss: 0.9038\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.610 - 0s 874us/step - loss: 0.5829 - val_loss: 0.8632\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.668 - 0s 827us/step - loss: 0.6108 - val_loss: 0.8669\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.663 - 0s 855us/step - loss: 0.5844 - val_loss: 0.9650\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.352 - 0s 855us/step - loss: 0.5710 - val_loss: 0.8387\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.208 - 0s 903us/step - loss: 0.5705 - val_loss: 1.3395\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.602 - ETA: 0s - loss: 0.426 - 0s 902us/step - loss: 0.5910 - val_loss: 1.0650\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.691 - 0s 893us/step - loss: 0.5431 - val_loss: 1.1228\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.347 - ETA: 0s - loss: 0.595 - 0s 950us/step - loss: 0.5353 - val_loss: 1.0476\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.706 - 0s 865us/step - loss: 0.5421 - val_loss: 1.0389\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.655 - 0s 836us/step - loss: 0.5272 - val_loss: 1.2063\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.875 - ETA: 0s - loss: 0.633 - 0s 855us/step - loss: 0.5336 - val_loss: 1.0989\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.338 - 0s 893us/step - loss: 0.5504 - val_loss: 0.9289\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.178 - ETA: 0s - loss: 0.585 - 0s 846us/step - loss: 0.5482 - val_loss: 1.1313\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.675 - 0s 846us/step - loss: 0.5264 - val_loss: 1.1058\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.363 - 0s 855us/step - loss: 0.5374 - val_loss: 0.9519\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.696 - 0s 902us/step - loss: 0.5275 - val_loss: 1.0955\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.312 - 0s 921us/step - loss: 0.5396 - val_loss: 1.0133\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.311 - 0s 969us/step - loss: 0.5040 - val_loss: 1.0460\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.643 - 0s 826us/step - loss: 0.5421 - val_loss: 1.1327\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.623 - ETA: 0s - loss: 0.594 - 0s 817us/step - loss: 0.5198 - val_loss: 1.2806\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.589 - 0s 902us/step - loss: 0.5083 - val_loss: 1.2747\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.163 - ETA: 0s - loss: 0.641 - 0s 902us/step - loss: 0.5197 - val_loss: 1.0045\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.277 - 0s 1ms/step - loss: 0.5102 - val_loss: 1.0134\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.637 - 0s 807us/step - loss: 0.5331 - val_loss: 1.2205\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.588 - 0s 826us/step - loss: 0.5204 - val_loss: 1.1004\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 23\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.44 - ETA: 0s - loss: 0.9713 - 1s 9ms/step - loss: 0.8733 - val_loss: 0.1156\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.835 - 0s 912us/step - loss: 0.6547 - val_loss: 0.4430\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.348 - 0s 845us/step - loss: 0.5243 - val_loss: 0.7545\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.516 - 0s 874us/step - loss: 0.4409 - val_loss: 1.3496\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.759 - ETA: 0s - loss: 0.495 - 0s 893us/step - loss: 0.4083 - val_loss: 1.5688\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.353 - 0s 902us/step - loss: 0.3681 - val_loss: 1.7043\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.289 - 0s 864us/step - loss: 0.3601 - val_loss: 1.8582\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.370 - 0s 864us/step - loss: 0.3571 - val_loss: 1.9679\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.978 - ETA: 0s - loss: 0.333 - 0s 893us/step - loss: 0.3554 - val_loss: 2.0327\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.351 - 0s 874us/step - loss: 0.3342 - val_loss: 2.1209\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.357 - 0s 893us/step - loss: 0.3479 - val_loss: 2.2964\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.387 - 0s 864us/step - loss: 0.3361 - val_loss: 2.0840\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.211 - 0s 931us/step - loss: 0.3292 - val_loss: 2.1234\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.364 - 0s 1ms/step - loss: 0.3490 - val_loss: 2.2912\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.307 - 0s 912us/step - loss: 0.3513 - val_loss: 1.9715\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.373 - 0s 874us/step - loss: 0.3568 - val_loss: 2.6660\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.325 - 0s 969us/step - loss: 0.3507 - val_loss: 2.4017\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.360 - 0s 902us/step - loss: 0.3241 - val_loss: 2.1282\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.358 - 0s 912us/step - loss: 0.3259 - val_loss: 2.2001\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.357 - 0s 931us/step - loss: 0.3155 - val_loss: 1.9405\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.335 - 0s 902us/step - loss: 0.3259 - val_loss: 2.0012\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.397 - 0s 845us/step - loss: 0.3356 - val_loss: 2.0625\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.361 - 0s 1ms/step - loss: 0.3264 - val_loss: 2.0542\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.911 - ETA: 0s - loss: 0.282 - 0s 855us/step - loss: 0.3361 - val_loss: 1.9150\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.334 - 0s 883us/step - loss: 0.3411 - val_loss: 2.1210\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.215 - 0s 874us/step - loss: 0.3349 - val_loss: 2.0416\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.354 - 0s 855us/step - loss: 0.3309 - val_loss: 2.0325\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.378 - 0s 827us/step - loss: 0.3274 - val_loss: 2.0606\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.703 - ETA: 0s - loss: 0.321 - 0s 826us/step - loss: 0.3240 - val_loss: 1.9703\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.333 - 0s 836us/step - loss: 0.3298 - val_loss: 2.0152\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.548 - ETA: 0s - loss: 0.291 - 0s 855us/step - loss: 0.3141 - val_loss: 1.8552\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 24\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.13 - ETA: 0s - loss: 0.8669 - 1s 9ms/step - loss: 0.9150 - val_loss: 0.1436\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.896 - 0s 855us/step - loss: 0.6973 - val_loss: 0.5033\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.546 - 0s 807us/step - loss: 0.5856 - val_loss: 0.8616\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.483 - ETA: 0s - loss: 0.501 - 0s 893us/step - loss: 0.5586 - val_loss: 1.2546\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.650 - 0s 846us/step - loss: 0.5479 - val_loss: 1.4758\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.498 - 0s 836us/step - loss: 0.4824 - val_loss: 1.7442\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.602 - ETA: 0s - loss: 0.318 - 0s 855us/step - loss: 0.5002 - val_loss: 1.5965\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.491 - 0s 903us/step - loss: 0.4840 - val_loss: 1.7949\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.462 - 0s 836us/step - loss: 0.4644 - val_loss: 2.1757\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.641 - ETA: 0s - loss: 0.407 - 0s 826us/step - loss: 0.4499 - val_loss: 1.9034\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.522 - 0s 826us/step - loss: 0.4475 - val_loss: 2.2509\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.786 - ETA: 0s - loss: 0.613 - 0s 931us/step - loss: 0.4755 - val_loss: 2.3673\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.401 - 0s 817us/step - loss: 0.4445 - val_loss: 2.1376\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.336 - 0s 1ms/step - loss: 0.4344 - val_loss: 2.7964\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.381 - 0s 940us/step - loss: 0.4334 - val_loss: 2.5319\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 0.410 - 0s 988us/step - loss: 0.3985 - val_loss: 2.6536\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.462 - 0s 912us/step - loss: 0.4315 - val_loss: 2.8860\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.407 - ETA: 0s - loss: 0.413 - 0s 883us/step - loss: 0.4133 - val_loss: 2.4312\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.476 - 0s 940us/step - loss: 0.4472 - val_loss: 2.8871\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.314 - 0s 883us/step - loss: 0.4088 - val_loss: 2.4177\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.460 - 0s 912us/step - loss: 0.3984 - val_loss: 2.8548\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.288 - 0s 931us/step - loss: 0.4129 - val_loss: 2.7320\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.355 - 0s 959us/step - loss: 0.3794 - val_loss: 2.9830\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.481 - 0s 941us/step - loss: 0.4226 - val_loss: 2.8813\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.426 - ETA: 0s - loss: 0.457 - 0s 902us/step - loss: 0.4066 - val_loss: 3.0445\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.397 - 0s 921us/step - loss: 0.3850 - val_loss: 2.4409\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.283 - 0s 864us/step - loss: 0.3738 - val_loss: 3.0787\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.447 - 0s 912us/step - loss: 0.3894 - val_loss: 3.1568\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.283 - 0s 912us/step - loss: 0.3723 - val_loss: 2.8252\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.412 - 0s 940us/step - loss: 0.3674 - val_loss: 3.2347\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.416 - 0s 1ms/step - loss: 0.3641 - val_loss: 3.4229\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 25\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.13 - ETA: 0s - loss: 0.4828 - 1s 9ms/step - loss: 0.8837 - val_loss: 0.0824\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.750 - ETA: 0s - loss: 0.957 - 0s 836us/step - loss: 0.7571 - val_loss: 0.3054\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.843 - 0s 864us/step - loss: 0.6834 - val_loss: 0.4699\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.809 - 0s 855us/step - loss: 0.6377 - val_loss: 0.5927\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 8.6004e-0 - ETA: 0s - loss: 0.7285    - 0s 855us/step - loss: 0.5952 - val_loss: 0.6824\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.351 - ETA: 0s - loss: 0.572 - 0s 846us/step - loss: 0.5683 - val_loss: 0.8043\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.580 - 0s 845us/step - loss: 0.5186 - val_loss: 0.8944\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.411 - 0s 864us/step - loss: 0.4839 - val_loss: 1.3398\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.573 - 0s 855us/step - loss: 0.4933 - val_loss: 1.3413\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.679 - ETA: 0s - loss: 0.554 - 0s 864us/step - loss: 0.4740 - val_loss: 1.2747\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.235 - 0s 988us/step - loss: 0.4558 - val_loss: 1.2424\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.540 - 0s 874us/step - loss: 0.4759 - val_loss: 1.2012\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.495 - 0s 921us/step - loss: 0.4493 - val_loss: 1.3106\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.397 - 0s 855us/step - loss: 0.4622 - val_loss: 1.2245\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.253 - 0s 864us/step - loss: 0.4494 - val_loss: 1.4957\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.781 - ETA: 0s - loss: 0.537 - 0s 864us/step - loss: 0.4482 - val_loss: 1.7347\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.407 - 0s 855us/step - loss: 0.4468 - val_loss: 1.5118\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.168 - 0s 874us/step - loss: 0.4228 - val_loss: 1.6055\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.354 - 0s 921us/step - loss: 0.4239 - val_loss: 1.6960\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.440 - 0s 845us/step - loss: 0.4174 - val_loss: 1.5077\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.470 - ETA: 0s - loss: 0.381 - 0s 902us/step - loss: 0.4311 - val_loss: 1.3202\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.587 - 0s 1ms/step - loss: 0.4295 - val_loss: 1.4429\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.574 - 0s 912us/step - loss: 0.4375 - val_loss: 1.4032\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.313 - 0s 883us/step - loss: 0.4079 - val_loss: 1.4121\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.554 - 0s 864us/step - loss: 0.4151 - val_loss: 1.4096\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.499 - ETA: 0s - loss: 0.218 - 0s 883us/step - loss: 0.4065 - val_loss: 1.3383\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.395 - 0s 855us/step - loss: 0.4278 - val_loss: 1.6102\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.376 - 0s 864us/step - loss: 0.4031 - val_loss: 1.5507\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.358 - 0s 931us/step - loss: 0.4012 - val_loss: 1.3875\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.343 - 0s 912us/step - loss: 0.3998 - val_loss: 1.4709\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.465 - 0s 940us/step - loss: 0.4017 - val_loss: 1.4525\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 26\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.39 - ETA: 0s - loss: 0.7353 - 1s 9ms/step - loss: 0.8508 - val_loss: 0.1836\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.434 - ETA: 0s - loss: 0.705 - 0s 959us/step - loss: 0.6380 - val_loss: 0.5546\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.645 - ETA: 0s - loss: 0.509 - 0s 883us/step - loss: 0.5268 - val_loss: 0.8973\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.866 - ETA: 0s - loss: 0.462 - 0s 845us/step - loss: 0.4943 - val_loss: 1.3826\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.482 - 0s 902us/step - loss: 0.4626 - val_loss: 1.5493\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.830 - ETA: 0s - loss: 0.552 - 0s 960us/step - loss: 0.4578 - val_loss: 1.5127\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.423 - 0s 902us/step - loss: 0.4526 - val_loss: 1.6235\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.343 - 0s 912us/step - loss: 0.4413 - val_loss: 1.6541\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.041 - ETA: 0s - loss: 0.343 - 0s 969us/step - loss: 0.4478 - val_loss: 1.6724\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.442 - 0s 893us/step - loss: 0.4562 - val_loss: 1.7096\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.734 - ETA: 0s - loss: 0.372 - 0s 874us/step - loss: 0.4410 - val_loss: 1.8210\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.442 - 0s 874us/step - loss: 0.4458 - val_loss: 1.7793\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.425 - 0s 2ms/step - loss: 0.4511 - val_loss: 1.8928\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.593 - ETA: 0s - loss: 0.411 - 0s 846us/step - loss: 0.4241 - val_loss: 1.7857\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.341 - 0s 912us/step - loss: 0.4206 - val_loss: 1.8389\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.452 - 0s 931us/step - loss: 0.4233 - val_loss: 1.8738\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.492 - 0s 893us/step - loss: 0.4384 - val_loss: 1.9128\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.415 - 0s 969us/step - loss: 0.4291 - val_loss: 1.8416\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.447 - 0s 855us/step - loss: 0.4325 - val_loss: 2.0427\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.489 - 0s 884us/step - loss: 0.4302 - val_loss: 1.9081\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.481 - 0s 912us/step - loss: 0.4271 - val_loss: 1.9636\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.378 - 0s 931us/step - loss: 0.4276 - val_loss: 2.1007\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.447 - ETA: 0s - loss: 0.497 - 0s 893us/step - loss: 0.4367 - val_loss: 1.8113\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.664 - ETA: 0s - loss: 0.382 - 0s 1ms/step - loss: 0.4150 - val_loss: 2.2200\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.410 - 0s 1ms/step - loss: 0.4303 - val_loss: 2.1216\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.305 - 0s 1ms/step - loss: 0.4231 - val_loss: 2.1856\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.383 - 0s 1ms/step - loss: 0.4146 - val_loss: 1.9377\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.499 - ETA: 0s - loss: 0.428 - 0s 1ms/step - loss: 0.4251 - val_loss: 2.1542\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.553 - ETA: 0s - loss: 0.464 - 0s 950us/step - loss: 0.4246 - val_loss: 2.1326\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.427 - 0s 978us/step - loss: 0.4101 - val_loss: 2.3593\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.383 - 0s 1ms/step - loss: 0.4077 - val_loss: 2.3597\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 27\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 21s - loss: 0.34 - ETA: 3s - loss: 1.4467 - ETA: 0s - loss: 1.001 - 1s 11ms/step - loss: 0.9474 - val_loss: 0.0501\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.546 - ETA: 0s - loss: 0.671 - 0s 1ms/step - loss: 0.8179 - val_loss: 0.1550\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.606 - ETA: 0s - loss: 0.942 - ETA: 0s - loss: 0.782 - 0s 2ms/step - loss: 0.7622 - val_loss: 0.3092\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.722 - 0s 722us/step - loss: 0.7159 - val_loss: 0.3673\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.844 - 0s 874us/step - loss: 0.7047 - val_loss: 0.3694\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.747 - 0s 864us/step - loss: 0.6980 - val_loss: 0.3769\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.666 - 0s 883us/step - loss: 0.6963 - val_loss: 0.3527\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.471 - ETA: 0s - loss: 0.699 - ETA: 0s - loss: 0.614 - 0s 1ms/step - loss: 0.6778 - val_loss: 0.3781\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.801 - 0s 893us/step - loss: 0.6744 - val_loss: 0.4397\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.826 - ETA: 0s - loss: 0.736 - 0s 883us/step - loss: 0.6673 - val_loss: 0.3551\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.621 - 0s 950us/step - loss: 0.6465 - val_loss: 0.3852\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.049 - ETA: 0s - loss: 0.490 - 0s 883us/step - loss: 0.6540 - val_loss: 0.3875\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.584 - 0s 1ms/step - loss: 0.6436 - val_loss: 0.3390\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.445 - ETA: 0s - loss: 0.850 - 0s 1ms/step - loss: 0.6497 - val_loss: 0.3279\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.812 - ETA: 0s - loss: 0.822 - 0s 969us/step - loss: 0.6431 - val_loss: 0.2973\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.676 - 0s 959us/step - loss: 0.6391 - val_loss: 0.3203\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 0.536 - 0s 865us/step - loss: 0.6352 - val_loss: 0.2927\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.506 - 0s 864us/step - loss: 0.6213 - val_loss: 0.3490\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.550 - ETA: 0s - loss: 0.666 - 0s 912us/step - loss: 0.6239 - val_loss: 0.3270\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.538 - 0s 940us/step - loss: 0.6467 - val_loss: 0.3339\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.930 - ETA: 0s - loss: 0.650 - 0s 1ms/step - loss: 0.6393 - val_loss: 0.3399\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.698 - ETA: 0s - loss: 0.640 - 0s 1ms/step - loss: 0.6137 - val_loss: 0.3246\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.680 - 0s 912us/step - loss: 0.6255 - val_loss: 0.3201\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.592 - 0s 846us/step - loss: 0.6265 - val_loss: 0.3014\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.462 - 0s 912us/step - loss: 0.6208 - val_loss: 0.3038\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.643 - ETA: 0s - loss: 0.688 - 0s 940us/step - loss: 0.6282 - val_loss: 0.3465\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.461 - ETA: 0s - loss: 0.731 - 0s 959us/step - loss: 0.6179 - val_loss: 0.3085\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.431 - ETA: 0s - loss: 0.351 - 0s 1ms/step - loss: 0.6176 - val_loss: 0.2938\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.606 - 0s 1ms/step - loss: 0.6223 - val_loss: 0.3089\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.721 - 0s 912us/step - loss: 0.6254 - val_loss: 0.3111\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.510 - 0s 1ms/step - loss: 0.6035 - val_loss: 0.3196\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 28\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.36 - ETA: 0s - loss: 1.1533 - 1s 9ms/step - loss: 0.8941 - val_loss: 0.1294\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.591 - 0s 950us/step - loss: 0.6860 - val_loss: 0.4363\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.519 - 0s 931us/step - loss: 0.5607 - val_loss: 0.9150\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.121 - ETA: 0s - loss: 0.523 - 0s 798us/step - loss: 0.5007 - val_loss: 1.5380\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.776 - ETA: 0s - loss: 0.438 - 0s 912us/step - loss: 0.4659 - val_loss: 1.7903\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.427 - ETA: 0s - loss: 0.431 - 0s 902us/step - loss: 0.4379 - val_loss: 2.2859\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.810 - ETA: 0s - loss: 0.447 - 0s 902us/step - loss: 0.4262 - val_loss: 2.6052\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.467 - 0s 988us/step - loss: 0.4027 - val_loss: 2.7829\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.381 - 0s 950us/step - loss: 0.4051 - val_loss: 2.8893\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.404 - 0s 959us/step - loss: 0.3996 - val_loss: 3.3645\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.343 - 0s 912us/step - loss: 0.3987 - val_loss: 3.0531\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.349 - ETA: 0s - loss: 0.404 - 0s 855us/step - loss: 0.3933 - val_loss: 2.9797\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.433 - ETA: 0s - loss: 0.443 - 0s 902us/step - loss: 0.3718 - val_loss: 3.0449\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.364 - 0s 912us/step - loss: 0.3656 - val_loss: 3.2628\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.386 - 0s 874us/step - loss: 0.3779 - val_loss: 3.0871\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.439 - 0s 921us/step - loss: 0.3788 - val_loss: 3.4885\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.456 - ETA: 0s - loss: 0.372 - 0s 817us/step - loss: 0.3899 - val_loss: 3.3227\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.502 - ETA: 0s - loss: 0.396 - 0s 1ms/step - loss: 0.3889 - val_loss: 3.7058\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.778 - ETA: 0s - loss: 0.369 - 0s 874us/step - loss: 0.3681 - val_loss: 3.2750\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.363 - 0s 1ms/step - loss: 0.3620 - val_loss: 3.4633\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.435 - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.373 - 0s 2ms/step - loss: 0.3704 - val_loss: 3.4306\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.449 - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.357 - 0s 1ms/step - loss: 0.3641 - val_loss: 3.4334\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.477 - ETA: 0s - loss: 0.317 - 0s 1ms/step - loss: 0.3704 - val_loss: 3.7459\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.332 - 0s 950us/step - loss: 0.3653 - val_loss: 3.5315\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.569 - ETA: 0s - loss: 0.354 - 0s 931us/step - loss: 0.3658 - val_loss: 3.4063\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.799 - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.319 - 0s 2ms/step - loss: 0.3737 - val_loss: 3.6442\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.367 - 0s 808us/step - loss: 0.3551 - val_loss: 3.8181\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.362 - 0s 883us/step - loss: 0.3556 - val_loss: 3.7957\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.254 - 0s 912us/step - loss: 0.3535 - val_loss: 3.2487\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.393 - 0s 893us/step - loss: 0.3485 - val_loss: 3.8984\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.439 - ETA: 0s - loss: 0.341 - 0s 921us/step - loss: 0.3522 - val_loss: 4.2099\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 29\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.27 - ETA: 0s - loss: 0.8540 - 1s 9ms/step - loss: 0.9338 - val_loss: 0.0298\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.735 - ETA: 0s - loss: 0.818 - 0s 969us/step - loss: 0.7397 - val_loss: 0.2581\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.365 - ETA: 0s - loss: 0.576 - 0s 959us/step - loss: 0.5928 - val_loss: 0.5239\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.083 - ETA: 0s - loss: 0.672 - 0s 997us/step - loss: 0.5226 - val_loss: 0.8631\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.486 - 0s 978us/step - loss: 0.4967 - val_loss: 1.0666\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.461 - ETA: 0s - loss: 0.450 - 0s 921us/step - loss: 0.4604 - val_loss: 1.2000\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.599 - ETA: 0s - loss: 0.440 - 0s 855us/step - loss: 0.4593 - val_loss: 1.3173\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.729 - ETA: 0s - loss: 0.510 - 0s 883us/step - loss: 0.4677 - val_loss: 1.2792\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.635 - ETA: 0s - loss: 0.450 - 0s 1ms/step - loss: 0.4409 - val_loss: 1.2575\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.706 - ETA: 0s - loss: 0.459 - 0s 978us/step - loss: 0.4293 - val_loss: 1.3962\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.418 - 0s 978us/step - loss: 0.4348 - val_loss: 1.4839\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.451 - 0s 912us/step - loss: 0.4322 - val_loss: 1.2829\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.524 - ETA: 0s - loss: 0.476 - 0s 950us/step - loss: 0.4294 - val_loss: 1.3844\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.417 - 0s 845us/step - loss: 0.4430 - val_loss: 1.3043\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.525 - ETA: 0s - loss: 0.370 - 0s 883us/step - loss: 0.4361 - val_loss: 1.4956\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.434 - 0s 940us/step - loss: 0.4111 - val_loss: 1.4401\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.045 - ETA: 0s - loss: 0.365 - 0s 836us/step - loss: 0.4366 - val_loss: 1.3792\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.470 - ETA: 0s - loss: 0.306 - 0s 826us/step - loss: 0.4133 - val_loss: 1.4249\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.372 - 0s 845us/step - loss: 0.4283 - val_loss: 1.4889\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.544 - 0s 950us/step - loss: 0.4181 - val_loss: 1.5821\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.353 - 0s 1ms/step - loss: 0.4186 - val_loss: 1.4534\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.657 - ETA: 0s - loss: 0.493 - 0s 1ms/step - loss: 0.4127 - val_loss: 1.5543\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.432 - ETA: 0s - loss: 0.498 - 0s 1ms/step - loss: 0.4235 - val_loss: 1.4127\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.420 - 0s 959us/step - loss: 0.4092 - val_loss: 1.4381\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.704 - ETA: 0s - loss: 0.420 - 0s 864us/step - loss: 0.4106 - val_loss: 1.4336\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.492 - 0s 864us/step - loss: 0.4059 - val_loss: 1.4120\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.405 - 0s 940us/step - loss: 0.3953 - val_loss: 1.4628\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.369 - 0s 978us/step - loss: 0.4175 - val_loss: 1.3348\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.394 - 0s 912us/step - loss: 0.3951 - val_loss: 1.4168\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.435 - 0s 950us/step - loss: 0.4084 - val_loss: 1.4812\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.480 - 0s 884us/step - loss: 0.4113 - val_loss: 1.5502\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 30\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.63 - ETA: 0s - loss: 0.6972 - 1s 10ms/step - loss: 0.8702 - val_loss: 0.0798\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.011 - ETA: 0s - loss: 0.588 - 0s 874us/step - loss: 0.6927 - val_loss: 0.3042\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.643 - 0s 902us/step - loss: 0.6032 - val_loss: 0.5446\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.602 - 0s 874us/step - loss: 0.5695 - val_loss: 0.5906\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.471 - 0s 836us/step - loss: 0.5398 - val_loss: 0.6084\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.507 - 0s 940us/step - loss: 0.5253 - val_loss: 0.5830\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.405 - 0s 969us/step - loss: 0.5109 - val_loss: 0.5615\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.447 - 0s 940us/step - loss: 0.5159 - val_loss: 0.5839\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.636 - 0s 969us/step - loss: 0.5186 - val_loss: 0.5150\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.594 - ETA: 0s - loss: 0.532 - 0s 921us/step - loss: 0.5098 - val_loss: 0.5579\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.848 - ETA: 0s - loss: 0.557 - ETA: 0s - loss: 0.507 - 0s 1ms/step - loss: 0.4961 - val_loss: 0.4537\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.419 - ETA: 0s - loss: 0.521 - 0s 1ms/step - loss: 0.4945 - val_loss: 0.5044\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.564 - 0s 1ms/step - loss: 0.5044 - val_loss: 0.5001\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.583 - 0s 959us/step - loss: 0.5084 - val_loss: 0.4832\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.465 - 0s 1ms/step - loss: 0.5026 - val_loss: 0.4082\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.466 - 0s 1ms/step - loss: 0.4940 - val_loss: 0.4351\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.516 - 0s 950us/step - loss: 0.4882 - val_loss: 0.4713\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.383 - ETA: 0s - loss: 0.498 - 0s 978us/step - loss: 0.4858 - val_loss: 0.5021\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.378 - ETA: 0s - loss: 0.494 - 0s 969us/step - loss: 0.4737 - val_loss: 0.4770\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.100 - ETA: 0s - loss: 0.498 - 0s 921us/step - loss: 0.4764 - val_loss: 0.4505\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.967 - ETA: 0s - loss: 0.488 - 0s 912us/step - loss: 0.4761 - val_loss: 0.4293\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.499 - 0s 883us/step - loss: 0.4743 - val_loss: 0.4206\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.645 - 0s 1ms/step - loss: 0.4705 - val_loss: 0.4562\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.213 - ETA: 0s - loss: 0.493 - 0s 1ms/step - loss: 0.4817 - val_loss: 0.4285\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.512 - 0s 959us/step - loss: 0.4840 - val_loss: 0.4338\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.407 - 0s 855us/step - loss: 0.4667 - val_loss: 0.4356\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.451 - ETA: 0s - loss: 0.494 - 0s 978us/step - loss: 0.4719 - val_loss: 0.4432\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.445 - 0s 1ms/step - loss: 0.4630 - val_loss: 0.4396\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.777 - ETA: 0s - loss: 0.579 - 0s 1ms/step - loss: 0.4821 - val_loss: 0.4427\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.897 - ETA: 0s - loss: 0.502 - 0s 883us/step - loss: 0.4721 - val_loss: 0.4479\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.424 - 0s 798us/step - loss: 0.4733 - val_loss: 0.4082\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 31\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 23s - loss: 3.22 - ETA: 3s - loss: 1.9860 - ETA: 2s - loss: 1.598 - ETA: 1s - loss: 1.293 - ETA: 1s - loss: 1.304 - ETA: 0s - loss: 1.150 - ETA: 0s - loss: 1.106 - ETA: 0s - loss: 1.047 - ETA: 0s - loss: 0.922 - ETA: 0s - loss: 0.871 - 2s 16ms/step - loss: 0.8647 - val_loss: 0.1161\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.700 - ETA: 0s - loss: 0.903 - ETA: 0s - loss: 0.731 - ETA: 0s - loss: 1.088 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 0.954 - ETA: 0s - loss: 0.817 - ETA: 0s - loss: 0.745 - ETA: 0s - loss: 0.713 - ETA: 0s - loss: 0.706 - ETA: 0s - loss: 0.687 - 1s 7ms/step - loss: 0.6606 - val_loss: 0.3389\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 1s - loss: 0.114 - ETA: 0s - loss: 1.184 - ETA: 0s - loss: 0.769 - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.781 - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.603 - ETA: 0s - loss: 0.582 - ETA: 0s - loss: 0.557 - ETA: 0s - loss: 0.542 - 1s 7ms/step - loss: 0.5197 - val_loss: 0.5973\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 1.032 - ETA: 0s - loss: 0.793 - ETA: 0s - loss: 0.580 - 0s 2ms/step - loss: 0.4782 - val_loss: 0.7733\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.511 - 0s 807us/step - loss: 0.4558 - val_loss: 0.8121\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 0.443 - 0s 826us/step - loss: 0.4359 - val_loss: 0.7510\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.441 - 0s 883us/step - loss: 0.4295 - val_loss: 0.7361\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.517 - 0s 978us/step - loss: 0.4179 - val_loss: 0.7557\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.450 - 0s 931us/step - loss: 0.4046 - val_loss: 0.6215\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.453 - 0s 912us/step - loss: 0.4274 - val_loss: 0.7388\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.291 - 0s 864us/step - loss: 0.3850 - val_loss: 0.6970\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.329 - 0s 988us/step - loss: 0.4151 - val_loss: 0.5272\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.521 - ETA: 0s - loss: 0.531 - ETA: 0s - loss: 0.425 - 0s 1ms/step - loss: 0.3922 - val_loss: 0.5284\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.240 - 0s 1ms/step - loss: 0.3958 - val_loss: 0.5593\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.390 - 0s 950us/step - loss: 0.3707 - val_loss: 0.4888\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.810 - ETA: 0s - loss: 0.361 - 0s 902us/step - loss: 0.3866 - val_loss: 0.4966\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.415 - 0s 941us/step - loss: 0.4084 - val_loss: 0.5245\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.397 - 0s 959us/step - loss: 0.3837 - val_loss: 0.4095\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.048 - ETA: 0s - loss: 0.509 - 0s 893us/step - loss: 0.3977 - val_loss: 0.4472\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.499 - ETA: 0s - loss: 0.350 - 0s 960us/step - loss: 0.4003 - val_loss: 0.4522\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.398 - 0s 902us/step - loss: 0.3931 - val_loss: 0.4770\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.382 - 0s 950us/step - loss: 0.3853 - val_loss: 0.4659\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.348 - 0s 874us/step - loss: 0.3940 - val_loss: 0.4184\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.459 - 0s 959us/step - loss: 0.3905 - val_loss: 0.5195\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.347 - 0s 988us/step - loss: 0.3964 - val_loss: 0.4448\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.440 - 0s 912us/step - loss: 0.3821 - val_loss: 0.4452\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.411 - 0s 874us/step - loss: 0.3679 - val_loss: 0.4869\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.349 - ETA: 0s - loss: 0.465 - 0s 903us/step - loss: 0.3872 - val_loss: 0.4363\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.075 - ETA: 0s - loss: 0.398 - 0s 1ms/step - loss: 0.3932 - val_loss: 0.4489\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.448 - ETA: 0s - loss: 0.347 - 0s 959us/step - loss: 0.3726 - val_loss: 0.4143\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.410 - 0s 931us/step - loss: 0.3804 - val_loss: 0.4253\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 32\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 7.30 - ETA: 0s - loss: 1.1322 - 1s 9ms/step - loss: 0.9024 - val_loss: 0.0568\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.529 - 0s 893us/step - loss: 0.6907 - val_loss: 0.2173\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.427 - ETA: 0s - loss: 0.705 - 0s 893us/step - loss: 0.6101 - val_loss: 0.5144\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.118 - ETA: 0s - loss: 0.695 - 0s 978us/step - loss: 0.5539 - val_loss: 0.6509\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.072 - ETA: 0s - loss: 0.380 - 0s 978us/step - loss: 0.5292 - val_loss: 0.5802\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.539 - 0s 902us/step - loss: 0.5110 - val_loss: 0.6326\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.486 - 0s 855us/step - loss: 0.4811 - val_loss: 0.6530\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.552 - 0s 921us/step - loss: 0.4782 - val_loss: 0.7054\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.310 - 0s 997us/step - loss: 0.4756 - val_loss: 0.5652\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.356 - 0s 902us/step - loss: 0.4768 - val_loss: 0.6789\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.373 - 0s 940us/step - loss: 0.4864 - val_loss: 0.8648\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.484 - 0s 865us/step - loss: 0.4816 - val_loss: 0.7627\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.335 - 0s 893us/step - loss: 0.4664 - val_loss: 0.6116\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.468 - 0s 874us/step - loss: 0.4494 - val_loss: 0.7388\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.505 - 0s 1ms/step - loss: 0.4635 - val_loss: 0.6952\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.346 - 0s 826us/step - loss: 0.4580 - val_loss: 0.6334\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.461 - 0s 798us/step - loss: 0.4496 - val_loss: 0.7625\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.519 - ETA: 0s - loss: 0.502 - 0s 874us/step - loss: 0.4651 - val_loss: 0.6176\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.514 - 0s 845us/step - loss: 0.4551 - val_loss: 0.6765\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.517 - 0s 817us/step - loss: 0.4567 - val_loss: 0.6629\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.918 - ETA: 0s - loss: 0.519 - 0s 1ms/step - loss: 0.4580 - val_loss: 0.6110\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.179 - 0s 931us/step - loss: 0.4478 - val_loss: 0.6835\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.484 - 0s 874us/step - loss: 0.4369 - val_loss: 0.6527\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.974 - ETA: 0s - loss: 0.509 - 0s 855us/step - loss: 0.4492 - val_loss: 0.7231\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.532 - 0s 940us/step - loss: 0.4490 - val_loss: 0.6976\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.494 - 0s 912us/step - loss: 0.4631 - val_loss: 0.7012\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.462 - 0s 931us/step - loss: 0.4269 - val_loss: 0.7548\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.575 - 0s 940us/step - loss: 0.4403 - val_loss: 0.7222\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.506 - 0s 969us/step - loss: 0.4553 - val_loss: 0.7501\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.893 - ETA: 0s - loss: 0.451 - 0s 940us/step - loss: 0.4250 - val_loss: 0.7019\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.482 - 0s 941us/step - loss: 0.4346 - val_loss: 0.7044\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 33\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 20s - loss: 0.80 - ETA: 0s - loss: 0.1979 - ETA: 0s - loss: 1.016 - 1s 10ms/step - loss: 0.9371 - val_loss: 0.0800\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.464 - 0s 940us/step - loss: 0.8657 - val_loss: 0.1696\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.075 - ETA: 0s - loss: 1.375 - 0s 988us/step - loss: 0.8312 - val_loss: 0.2633\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.952 - 0s 978us/step - loss: 0.7977 - val_loss: 0.3287\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 12.61 - ETA: 0s - loss: 1.1395 - 0s 950us/step - loss: 0.7801 - val_loss: 0.4939\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 1.394 - ETA: 0s - loss: 0.769 - 0s 1ms/step - loss: 0.7558 - val_loss: 0.6039\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.878 - ETA: 0s - loss: 0.335 - 0s 1ms/step - loss: 0.7280 - val_loss: 0.6023\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.814 - 0s 988us/step - loss: 0.7152 - val_loss: 0.8132\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 1.073 - 0s 1ms/step - loss: 0.6728 - val_loss: 1.0283\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.648 - 0s 940us/step - loss: 0.6590 - val_loss: 1.1822\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.378 - 0s 931us/step - loss: 0.6416 - val_loss: 1.2169\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.968 - 0s 912us/step - loss: 0.6610 - val_loss: 1.6395\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.155 - 0s 969us/step - loss: 0.6326 - val_loss: 1.6202\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.618 - 0s 969us/step - loss: 0.5993 - val_loss: 2.0163\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.331 - 0s 1ms/step - loss: 0.6024 - val_loss: 2.2159\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.338 - 0s 969us/step - loss: 0.5788 - val_loss: 2.3022\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.972 - 0s 969us/step - loss: 0.5947 - val_loss: 2.6433\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.774 - 0s 997us/step - loss: 0.5609 - val_loss: 2.7696\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.322 - 0s 950us/step - loss: 0.5602 - val_loss: 2.8409\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.519 - 0s 988us/step - loss: 0.5136 - val_loss: 3.0965\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.726 - 0s 959us/step - loss: 0.5606 - val_loss: 3.3599\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.647 - 0s 940us/step - loss: 0.5394 - val_loss: 3.3904\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.700 - 0s 874us/step - loss: 0.5380 - val_loss: 3.7344\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.162 - 0s 874us/step - loss: 0.5049 - val_loss: 3.5226\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.505 - 0s 902us/step - loss: 0.4980 - val_loss: 4.9812\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.490 - 0s 921us/step - loss: 0.4818 - val_loss: 4.9631\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.739 - ETA: 0s - loss: 0.653 - 0s 959us/step - loss: 0.4770 - val_loss: 5.0098\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.539 - 0s 1ms/step - loss: 0.4918 - val_loss: 5.1010\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.721 - ETA: 0s - loss: 0.373 - 0s 931us/step - loss: 0.4968 - val_loss: 4.9581\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.454 - ETA: 0s - loss: 0.556 - 0s 836us/step - loss: 0.4572 - val_loss: 5.0796\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.328 - ETA: 0s - loss: 0.650 - 0s 845us/step - loss: 0.5161 - val_loss: 5.0815\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 34\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 4.00 - ETA: 0s - loss: 0.9473 - 1s 9ms/step - loss: 0.9362 - val_loss: 0.0673\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.466 - ETA: 0s - loss: 0.656 - 0s 941us/step - loss: 0.7835 - val_loss: 0.2836\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.327 - ETA: 0s - loss: 0.588 - 0s 893us/step - loss: 0.6694 - val_loss: 0.5162\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.590 - ETA: 0s - loss: 0.673 - 0s 941us/step - loss: 0.6231 - val_loss: 0.8429\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.520 - ETA: 0s - loss: 0.673 - 0s 902us/step - loss: 0.5771 - val_loss: 1.1581\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.618 - ETA: 0s - loss: 0.524 - 0s 921us/step - loss: 0.5611 - val_loss: 1.2530\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.496 - ETA: 0s - loss: 0.564 - 0s 893us/step - loss: 0.5305 - val_loss: 1.3127\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.497 - 0s 902us/step - loss: 0.5376 - val_loss: 1.3576\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.559 - 0s 883us/step - loss: 0.5070 - val_loss: 1.5551\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.543 - 0s 959us/step - loss: 0.5178 - val_loss: 1.5824\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.486 - 0s 931us/step - loss: 0.4967 - val_loss: 1.6138\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.446 - 0s 1ms/step - loss: 0.5033 - val_loss: 1.5769\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.091 - ETA: 0s - loss: 0.499 - 0s 921us/step - loss: 0.4715 - val_loss: 1.7086\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.242 - ETA: 0s - loss: 0.488 - 0s 921us/step - loss: 0.4984 - val_loss: 1.6948\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.489 - 0s 912us/step - loss: 0.4811 - val_loss: 1.8479\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.795 - ETA: 0s - loss: 0.477 - 0s 912us/step - loss: 0.4782 - val_loss: 1.7723\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.011 - ETA: 0s - loss: 0.484 - 0s 931us/step - loss: 0.4665 - val_loss: 1.8450\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.432 - 0s 988us/step - loss: 0.4730 - val_loss: 1.8215\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.935 - ETA: 0s - loss: 0.528 - 0s 969us/step - loss: 0.4683 - val_loss: 1.8895\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.821 - ETA: 0s - loss: 0.459 - 0s 902us/step - loss: 0.4611 - val_loss: 1.7946\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.490 - 0s 950us/step - loss: 0.4424 - val_loss: 1.7722\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.188 - ETA: 0s - loss: 0.432 - 0s 997us/step - loss: 0.4439 - val_loss: 1.9406\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.509 - ETA: 0s - loss: 0.503 - 0s 950us/step - loss: 0.4464 - val_loss: 1.9978\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.458 - 0s 988us/step - loss: 0.4459 - val_loss: 1.9077\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.440 - 0s 893us/step - loss: 0.4424 - val_loss: 2.0318\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.588 - ETA: 0s - loss: 0.429 - 0s 940us/step - loss: 0.4302 - val_loss: 1.9877\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.839 - ETA: 0s - loss: 0.380 - 0s 836us/step - loss: 0.4245 - val_loss: 1.9919\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.469 - ETA: 0s - loss: 0.480 - 0s 892us/step - loss: 0.4274 - val_loss: 2.1794\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.463 - 0s 940us/step - loss: 0.4418 - val_loss: 2.1692\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.465 - 0s 912us/step - loss: 0.4378 - val_loss: 1.8303\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.365 - 0s 959us/step - loss: 0.4140 - val_loss: 1.9303\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 35\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.25 - ETA: 0s - loss: 0.8660 - 1s 9ms/step - loss: 0.9639 - val_loss: 0.0795\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.374 - 0s 1ms/step - loss: 0.9179 - val_loss: 0.1326\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.355 - 0s 922us/step - loss: 0.9000 - val_loss: 0.1475\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.833 - 0s 883us/step - loss: 0.8958 - val_loss: 0.1367\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 1.131 - 0s 864us/step - loss: 0.8889 - val_loss: 0.1689\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 1.167 - 0s 865us/step - loss: 0.8865 - val_loss: 0.1298\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 1.081 - 0s 817us/step - loss: 0.8797 - val_loss: 0.0950\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.314 - 0s 903us/step - loss: 0.8742 - val_loss: 0.0901\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.567 - 0s 883us/step - loss: 0.8661 - val_loss: 0.1214\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.829 - ETA: 0s - loss: 0.505 - 0s 874us/step - loss: 0.8743 - val_loss: 0.1015\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 1.466 - ETA: 0s - loss: 0.926 - 0s 1ms/step - loss: 0.8688 - val_loss: 0.0804\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.706 - ETA: 0s - loss: 0.600 - 0s 931us/step - loss: 0.8653 - val_loss: 0.0619\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 1.000 - 0s 808us/step - loss: 0.8588 - val_loss: 0.0711\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 1.093 - 0s 864us/step - loss: 0.8641 - val_loss: 0.0499\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.339 - ETA: 0s - loss: 0.572 - 0s 864us/step - loss: 0.8683 - val_loss: 0.0513\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.735 - 0s 864us/step - loss: 0.8441 - val_loss: 0.0546\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.227 - 0s 921us/step - loss: 0.8617 - val_loss: 0.0580\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 1.153 - 0s 903us/step - loss: 0.8606 - val_loss: 0.0548\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.482 - ETA: 0s - loss: 0.543 - 0s 902us/step - loss: 0.8520 - val_loss: 0.0542\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 1.023 - 0s 912us/step - loss: 0.8545 - val_loss: 0.0603\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 1.051 - 0s 893us/step - loss: 0.8409 - val_loss: 0.0602\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 1.068 - 0s 902us/step - loss: 0.8484 - val_loss: 0.0621\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 1.083 - 0s 883us/step - loss: 0.8552 - val_loss: 0.0581\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.474 - ETA: 0s - loss: 0.322 - 0s 931us/step - loss: 0.8593 - val_loss: 0.0642\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.465 - ETA: 0s - loss: 0.585 - 0s 883us/step - loss: 0.8464 - val_loss: 0.0624\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.901 - 0s 978us/step - loss: 0.8487 - val_loss: 0.0674\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.517 - 0s 940us/step - loss: 0.8476 - val_loss: 0.0714\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.094 - ETA: 0s - loss: 1.119 - 0s 931us/step - loss: 0.8589 - val_loss: 0.0725\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.566 - 0s 950us/step - loss: 0.8462 - val_loss: 0.0738\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.127 - ETA: 0s - loss: 0.236 - 0s 1ms/step - loss: 0.8461 - val_loss: 0.0697\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.351 - ETA: 0s - loss: 1.224 - 0s 960us/step - loss: 0.8449 - val_loss: 0.0730\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 1.139 - 0s 912us/step - loss: 0.8450 - val_loss: 0.0778\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 1.049 - 0s 902us/step - loss: 0.8491 - val_loss: 0.0773\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.260 - 0s 883us/step - loss: 0.8500 - val_loss: 0.0836\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 15.22 - ETA: 0s - loss: 1.1988 - 0s 1ms/step - loss: 0.8536 - val_loss: 0.0842\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.848 - 0s 883us/step - loss: 0.8538 - val_loss: 0.0880\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.254 - 0s 883us/step - loss: 0.8384 - val_loss: 0.0908\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.531 - ETA: 0s - loss: 0.986 - 0s 817us/step - loss: 0.8319 - val_loss: 0.1000\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.277 - 0s 959us/step - loss: 0.8471 - val_loss: 0.0996\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.605 - 0s 959us/step - loss: 0.8443 - val_loss: 0.1046\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 1.029 - 0s 845us/step - loss: 0.8399 - val_loss: 0.1118\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.568 - 0s 874us/step - loss: 0.8407 - val_loss: 0.1233\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.778 - 0s 836us/step - loss: 0.8508 - val_loss: 0.1263\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.842 - ETA: 0s - loss: 0.754 - 0s 893us/step - loss: 0.8451 - val_loss: 0.1319\n",
      "Epoch 00044: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 36\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.41 - ETA: 0s - loss: 1.4409 - 1s 9ms/step - loss: 0.9957 - val_loss: 0.0309\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.212 - 0s 912us/step - loss: 0.8766 - val_loss: 0.1305\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.175 - 0s 902us/step - loss: 0.8219 - val_loss: 0.2729\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.180 - 0s 912us/step - loss: 0.7555 - val_loss: 0.4461\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.193 - 0s 1ms/step - loss: 0.7583 - val_loss: 0.5436\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 1.025 - 0s 959us/step - loss: 0.7262 - val_loss: 0.6990\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.941 - 0s 912us/step - loss: 0.7023 - val_loss: 0.7553\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.975 - 0s 912us/step - loss: 0.6800 - val_loss: 0.7681\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.882 - 0s 883us/step - loss: 0.6971 - val_loss: 0.8308\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.794 - 0s 817us/step - loss: 0.6404 - val_loss: 0.8766\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.610 - ETA: 0s - loss: 0.819 - 0s 874us/step - loss: 0.6238 - val_loss: 0.9330\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.765 - 0s 826us/step - loss: 0.6148 - val_loss: 1.0285\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.758 - 0s 883us/step - loss: 0.6046 - val_loss: 0.9970\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 0.226 - 0s 903us/step - loss: 0.6360 - val_loss: 0.9353\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.765 - 0s 912us/step - loss: 0.6212 - val_loss: 1.0157\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.686 - 0s 855us/step - loss: 0.5749 - val_loss: 1.0826\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.539 - ETA: 0s - loss: 0.718 - 0s 855us/step - loss: 0.5862 - val_loss: 1.1256\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.226 - 0s 855us/step - loss: 0.5580 - val_loss: 1.1424\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.209 - 0s 864us/step - loss: 0.5455 - val_loss: 1.0876\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.462 - ETA: 0s - loss: 0.628 - 0s 807us/step - loss: 0.5445 - val_loss: 1.4483\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.727 - 0s 845us/step - loss: 0.5821 - val_loss: 1.3772\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.779 - ETA: 0s - loss: 0.225 - 0s 883us/step - loss: 0.5224 - val_loss: 1.3744\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.683 - 0s 940us/step - loss: 0.5229 - val_loss: 1.5388\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.585 - 0s 902us/step - loss: 0.4973 - val_loss: 1.5231\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.193 - 0s 1ms/step - loss: 0.5118 - val_loss: 1.5488\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.196 - 0s 836us/step - loss: 0.4945 - val_loss: 1.3018\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.197 - 0s 836us/step - loss: 0.4910 - val_loss: 1.3691\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.638 - 0s 883us/step - loss: 0.5667 - val_loss: 1.7549\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.818 - ETA: 0s - loss: 0.503 - 0s 845us/step - loss: 0.4315 - val_loss: 1.7054\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.601 - 0s 912us/step - loss: 0.4992 - val_loss: 1.6730\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.202 - 0s 855us/step - loss: 0.4785 - val_loss: 1.4664\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 37\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,501\n",
      "Trainable params: 42,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.87 - ETA: 0s - loss: 0.7357 - 1s 9ms/step - loss: 0.9216 - val_loss: 0.0933\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.539 - ETA: 0s - loss: 0.622 - 0s 912us/step - loss: 0.7330 - val_loss: 0.3332\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.447 - ETA: 0s - loss: 0.595 - 0s 874us/step - loss: 0.6313 - val_loss: 0.6012\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.659 - 0s 1ms/step - loss: 0.5753 - val_loss: 0.9869\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.526 - 0s 826us/step - loss: 0.5493 - val_loss: 1.1883\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.567 - ETA: 0s - loss: 0.511 - 0s 826us/step - loss: 0.5184 - val_loss: 1.2234\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.968 - ETA: 0s - loss: 0.519 - 0s 864us/step - loss: 0.5184 - val_loss: 1.2700\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.787 - ETA: 0s - loss: 0.553 - 0s 864us/step - loss: 0.5100 - val_loss: 1.2774\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.372 - 0s 912us/step - loss: 0.4895 - val_loss: 1.3206\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.514 - 0s 969us/step - loss: 0.4734 - val_loss: 1.8501\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.786 - ETA: 0s - loss: 0.475 - 0s 836us/step - loss: 0.4699 - val_loss: 1.7890\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.448 - 0s 921us/step - loss: 0.4585 - val_loss: 1.7106\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.451 - 0s 884us/step - loss: 0.4739 - val_loss: 1.6714\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.509 - ETA: 0s - loss: 0.443 - 0s 912us/step - loss: 0.4623 - val_loss: 1.6936\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.901 - ETA: 0s - loss: 0.536 - 0s 912us/step - loss: 0.4738 - val_loss: 1.7687\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.491 - 0s 931us/step - loss: 0.4723 - val_loss: 1.7278\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.489 - 0s 959us/step - loss: 0.4701 - val_loss: 1.5973\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.448 - ETA: 0s - loss: 0.499 - 0s 893us/step - loss: 0.4638 - val_loss: 1.7521\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.523 - ETA: 0s - loss: 0.441 - 0s 864us/step - loss: 0.4730 - val_loss: 1.7989\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.458 - 0s 921us/step - loss: 0.4450 - val_loss: 1.6507\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.466 - 0s 893us/step - loss: 0.4696 - val_loss: 1.5168\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.447 - ETA: 0s - loss: 0.456 - 0s 855us/step - loss: 0.4566 - val_loss: 1.7180\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 0.424 - 0s 1ms/step - loss: 0.4573 - val_loss: 1.6404\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.759 - ETA: 0s - loss: 0.477 - 0s 1ms/step - loss: 0.4423 - val_loss: 1.7302\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.971 - ETA: 0s - loss: 0.492 - 0s 1ms/step - loss: 0.4465 - val_loss: 1.6481\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.814 - ETA: 0s - loss: 0.473 - 0s 883us/step - loss: 0.4413 - val_loss: 1.6665\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.459 - ETA: 0s - loss: 0.432 - 0s 845us/step - loss: 0.4567 - val_loss: 1.6245\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.350 - 0s 808us/step - loss: 0.4369 - val_loss: 1.4203\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.941 - ETA: 0s - loss: 0.460 - 0s 855us/step - loss: 0.4219 - val_loss: 1.5685\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.327 - ETA: 0s - loss: 0.477 - 0s 884us/step - loss: 0.4659 - val_loss: 1.6923\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.219 - ETA: 0s - loss: 0.447 - 0s 855us/step - loss: 0.4235 - val_loss: 1.4439\n",
      "Epoch 00031: early stopping\n",
      "MSE: 640.868972\n",
      "RMSE: 25.315390\n",
      "MAE: 19.827986\n",
      "MAPE: 24.468805\n",
      "\n",
      "Quantile 1, between 39.99999999999999 and 77.5\n",
      "MSE: 641.092795\n",
      "RMSE: 25.319810\n",
      "MAE: 19.829724\n",
      "MAPE: 35.107843\n",
      "\n",
      "Quantile 2, between 77.5 and 87.5\n",
      "MSE: 579.468146\n",
      "RMSE: 24.072145\n",
      "MAE: 17.887131\n",
      "MAPE: 21.757714\n",
      "\n",
      "Quantile 3, between 87.5 and 98.00000000000001\n",
      "MSE: 597.516819\n",
      "RMSE: 24.444157\n",
      "MAE: 19.257627\n",
      "MAPE: 20.611646\n",
      "\n",
      "Quantile 4, between 98.00000000000001 and 130.00000000000003\n",
      "MSE: 734.922832\n",
      "RMSE: 27.109460\n",
      "MAE: 22.086340\n",
      "MAPE: 19.741193\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_test = 15\n",
    "n_hexa = 5\n",
    "name_error = 'LSTM predictions - 15 Days - 5 hex -4q'\n",
    "path_error = \"imgs/ips_predictions/prediction_lstm_15_days_5hex_4q.png\"\n",
    "name_map = '15 days - 5 hex - 4q'\n",
    "path_map = \"imgs/ips_predictions/maps_LSTM_prediction_15_total_data_5_hex_4q.png\"\n",
    "\n",
    "dict_pred,pred_lstm,pred_lstm_cases = lstm_hex_selected(data_days, dict_pred_sel_hexa,n_test,n_hexa,name_error,path_error,name_map,path_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,) (15,)\n",
      "model compiled 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.27 - ETA: 0s - loss: 1.0985 - 1s 9ms/step - loss: 0.9266 - val_loss: 0.0801\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.807 - ETA: 0s - loss: 0.784 - 0s 912us/step - loss: 0.7456 - val_loss: 0.3340\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.421 - ETA: 0s - loss: 0.635 - 0s 978us/step - loss: 0.6170 - val_loss: 0.6445\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.453 - 0s 940us/step - loss: 0.5527 - val_loss: 0.9823\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.433 - ETA: 0s - loss: 0.646 - 0s 1ms/step - loss: 0.5318 - val_loss: 1.2881\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.445 - ETA: 0s - loss: 0.549 - 0s 922us/step - loss: 0.5325 - val_loss: 1.4327\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.464 - ETA: 0s - loss: 0.544 - 0s 969us/step - loss: 0.5163 - val_loss: 1.5151\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.592 - 0s 960us/step - loss: 0.5281 - val_loss: 1.6051\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.510 - 0s 950us/step - loss: 0.5197 - val_loss: 1.6107\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.418 - 0s 931us/step - loss: 0.5134 - val_loss: 1.6700\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.111 - ETA: 0s - loss: 0.464 - 0s 1ms/step - loss: 0.5115 - val_loss: 1.6420\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.019 - ETA: 0s - loss: 0.597 - 0s 979us/step - loss: 0.5308 - val_loss: 1.8348\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.004 - ETA: 0s - loss: 0.538 - 0s 1ms/step - loss: 0.5097 - val_loss: 1.6842\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.450 - 0s 931us/step - loss: 0.5121 - val_loss: 1.8313\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.335 - ETA: 0s - loss: 0.423 - 0s 1ms/step - loss: 0.5046 - val_loss: 1.9374\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.680 - ETA: 0s - loss: 0.548 - 0s 959us/step - loss: 0.5167 - val_loss: 1.6952\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.501 - 0s 1ms/step - loss: 0.5011 - val_loss: 1.6658\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.502 - 0s 1ms/step - loss: 0.4999 - val_loss: 1.8691\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.268 - ETA: 0s - loss: 0.419 - 0s 902us/step - loss: 0.5203 - val_loss: 1.9715\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.042 - ETA: 0s - loss: 0.533 - 0s 902us/step - loss: 0.5179 - val_loss: 1.5404\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.784 - ETA: 0s - loss: 0.433 - 0s 978us/step - loss: 0.4997 - val_loss: 1.7603\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.511 - 0s 903us/step - loss: 0.5025 - val_loss: 1.8475\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.366 - ETA: 0s - loss: 0.503 - 0s 969us/step - loss: 0.4961 - val_loss: 1.8826\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.124 - ETA: 0s - loss: 0.427 - 0s 1ms/step - loss: 0.4994 - val_loss: 1.7576\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.497 - ETA: 0s - loss: 0.556 - 0s 1ms/step - loss: 0.5095 - val_loss: 1.8128\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.888 - ETA: 0s - loss: 0.534 - 0s 950us/step - loss: 0.5109 - val_loss: 1.9833\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.640 - 0s 1ms/step - loss: 0.5050 - val_loss: 1.8967\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.474 - 0s 1ms/step - loss: 0.5008 - val_loss: 1.9035\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.418 - 0s 1ms/step - loss: 0.4960 - val_loss: 2.0232\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.966 - ETA: 0s - loss: 0.565 - 0s 978us/step - loss: 0.5019 - val_loss: 1.9377\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.549 - 0s 921us/step - loss: 0.4966 - val_loss: 2.0592\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 1.35 - ETA: 0s - loss: 1.0638 - 1s 9ms/step - loss: 0.8759 - val_loss: 0.0506\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.875 - ETA: 0s - loss: 0.875 - 0s 959us/step - loss: 0.7375 - val_loss: 0.1367\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.690 - 0s 921us/step - loss: 0.6556 - val_loss: 0.2391\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.687 - 0s 1ms/step - loss: 0.6014 - val_loss: 0.3533\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.519 - 0s 969us/step - loss: 0.5623 - val_loss: 0.4424\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.386 - ETA: 0s - loss: 0.633 - 0s 864us/step - loss: 0.5541 - val_loss: 0.4974\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.548 - 0s 865us/step - loss: 0.5431 - val_loss: 0.5258\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.400 - 0s 931us/step - loss: 0.5355 - val_loss: 0.4772\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.497 - ETA: 0s - loss: 0.646 - 0s 883us/step - loss: 0.5252 - val_loss: 0.4376\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.872 - ETA: 0s - loss: 0.561 - 0s 931us/step - loss: 0.5231 - val_loss: 0.4699\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.540 - 0s 950us/step - loss: 0.5157 - val_loss: 0.4528\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.322 - ETA: 0s - loss: 0.398 - 0s 874us/step - loss: 0.5078 - val_loss: 0.4404\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.614 - ETA: 0s - loss: 0.639 - 0s 874us/step - loss: 0.4999 - val_loss: 0.4635\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.572 - 0s 940us/step - loss: 0.5076 - val_loss: 0.4295\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.294 - ETA: 0s - loss: 0.471 - 0s 788us/step - loss: 0.5096 - val_loss: 0.4260\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.581 - 0s 826us/step - loss: 0.4963 - val_loss: 0.4503\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.511 - 0s 807us/step - loss: 0.5100 - val_loss: 0.4722\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.454 - 0s 845us/step - loss: 0.5014 - val_loss: 0.4652\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.386 - 0s 836us/step - loss: 0.4872 - val_loss: 0.4184\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.909 - ETA: 0s - loss: 0.435 - 0s 836us/step - loss: 0.4920 - val_loss: 0.3556\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.484 - 0s 902us/step - loss: 0.4937 - val_loss: 0.3968\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.414 - 0s 836us/step - loss: 0.4887 - val_loss: 0.3989\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.531 - 0s 836us/step - loss: 0.4855 - val_loss: 0.4143\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.355 - 0s 864us/step - loss: 0.4873 - val_loss: 0.3523\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.548 - ETA: 0s - loss: 0.463 - 0s 893us/step - loss: 0.4776 - val_loss: 0.3673\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.567 - 0s 855us/step - loss: 0.4806 - val_loss: 0.3313\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.511 - 0s 836us/step - loss: 0.4877 - val_loss: 0.3678\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.821 - ETA: 0s - loss: 0.402 - 0s 902us/step - loss: 0.4801 - val_loss: 0.3719\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.740 - ETA: 0s - loss: 0.431 - 0s 902us/step - loss: 0.4887 - val_loss: 0.3840\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.611 - ETA: 0s - loss: 0.439 - 0s 997us/step - loss: 0.4703 - val_loss: 0.3853\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.882 - ETA: 0s - loss: 0.429 - 0s 921us/step - loss: 0.4751 - val_loss: 0.3512\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 1.11 - ETA: 0s - loss: 1.0082 - 1s 9ms/step - loss: 0.8789 - val_loss: 0.1691\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.386 - ETA: 0s - loss: 0.676 - 0s 950us/step - loss: 0.7824 - val_loss: 0.3603\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.776 - ETA: 0s - loss: 0.659 - 0s 922us/step - loss: 0.7205 - val_loss: 0.6124\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.783 - 0s 912us/step - loss: 0.7150 - val_loss: 0.9525\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.359 - ETA: 0s - loss: 0.737 - 0s 893us/step - loss: 0.6649 - val_loss: 1.0305\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.645 - 0s 883us/step - loss: 0.6775 - val_loss: 1.0421\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.904 - ETA: 0s - loss: 0.854 - 0s 921us/step - loss: 0.6730 - val_loss: 1.2346\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.596 - 0s 931us/step - loss: 0.6662 - val_loss: 1.1863\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.089 - ETA: 0s - loss: 0.534 - 0s 883us/step - loss: 0.6604 - val_loss: 1.1140\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.790 - 0s 874us/step - loss: 0.6513 - val_loss: 1.1238\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.591 - 0s 874us/step - loss: 0.6619 - val_loss: 1.1523\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.655 - 0s 874us/step - loss: 0.6572 - val_loss: 1.0688\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.681 - 0s 979us/step - loss: 0.6531 - val_loss: 1.1152\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.562 - 0s 950us/step - loss: 0.6536 - val_loss: 1.1246\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.877 - ETA: 0s - loss: 0.794 - 0s 931us/step - loss: 0.6614 - val_loss: 1.0535\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.498 - ETA: 0s - loss: 0.603 - 0s 845us/step - loss: 0.6544 - val_loss: 1.0775\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.724 - 0s 864us/step - loss: 0.6473 - val_loss: 1.1089\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.302 - ETA: 0s - loss: 0.651 - 0s 874us/step - loss: 0.6583 - val_loss: 1.1116\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.597 - 0s 884us/step - loss: 0.6542 - val_loss: 1.0498\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.643 - 0s 902us/step - loss: 0.6553 - val_loss: 1.1058\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.700 - 0s 959us/step - loss: 0.6360 - val_loss: 1.0783\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.643 - 0s 922us/step - loss: 0.6339 - val_loss: 1.0260\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.332 - ETA: 0s - loss: 0.608 - 0s 1ms/step - loss: 0.6498 - val_loss: 1.0224\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.622 - 0s 884us/step - loss: 0.6386 - val_loss: 1.1112\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.554 - ETA: 0s - loss: 0.648 - 0s 931us/step - loss: 0.6371 - val_loss: 1.0617\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.576 - 0s 865us/step - loss: 0.6269 - val_loss: 1.0704\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.878 - ETA: 0s - loss: 0.672 - 0s 855us/step - loss: 0.6365 - val_loss: 1.0399\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.585 - 0s 883us/step - loss: 0.6272 - val_loss: 1.0431\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.696 - 0s 893us/step - loss: 0.6437 - val_loss: 1.0641\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.779 - ETA: 0s - loss: 0.466 - 0s 836us/step - loss: 0.6475 - val_loss: 1.0648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.186 - ETA: 0s - loss: 0.548 - 0s 874us/step - loss: 0.6365 - val_loss: 1.0679\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 20s - loss: 1.73 - ETA: 0s - loss: 0.9446 - 1s 10ms/step - loss: 0.8555 - val_loss: 0.1008\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.209 - ETA: 0s - loss: 0.669 - 0s 940us/step - loss: 0.6613 - val_loss: 0.4010\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.600 - 0s 826us/step - loss: 0.5146 - val_loss: 0.9590\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.455 - 0s 855us/step - loss: 0.4096 - val_loss: 1.5061\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.422 - 0s 978us/step - loss: 0.3881 - val_loss: 2.0031\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.538 - ETA: 0s - loss: 0.405 - 0s 845us/step - loss: 0.3652 - val_loss: 2.3122\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.529 - ETA: 0s - loss: 0.374 - 0s 893us/step - loss: 0.3532 - val_loss: 2.4586\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.375 - 0s 912us/step - loss: 0.3665 - val_loss: 2.4699\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.402 - 0s 883us/step - loss: 0.3693 - val_loss: 2.3234\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.519 - ETA: 0s - loss: 0.374 - 0s 893us/step - loss: 0.3514 - val_loss: 2.4379\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.867 - ETA: 0s - loss: 0.316 - 0s 940us/step - loss: 0.3626 - val_loss: 2.3536\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.374 - 0s 1ms/step - loss: 0.3519 - val_loss: 2.5546\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.278 - ETA: 0s - loss: 0.368 - 0s 892us/step - loss: 0.3599 - val_loss: 2.6111\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.311 - 0s 845us/step - loss: 0.3589 - val_loss: 2.2288\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.353 - 0s 893us/step - loss: 0.3577 - val_loss: 2.4249\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 0.371 - 0s 826us/step - loss: 0.3599 - val_loss: 2.4139\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.326 - 0s 893us/step - loss: 0.3534 - val_loss: 2.4122\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.005 - ETA: 0s - loss: 0.385 - 0s 855us/step - loss: 0.3668 - val_loss: 2.4809\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.282 - 0s 865us/step - loss: 0.3463 - val_loss: 2.4536\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.350 - 0s 864us/step - loss: 0.3474 - val_loss: 2.6612\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.364 - 0s 883us/step - loss: 0.3463 - val_loss: 2.6033\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.366 - 0s 921us/step - loss: 0.3426 - val_loss: 2.4532\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.332 - 0s 922us/step - loss: 0.3492 - val_loss: 2.2198\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.560 - ETA: 0s - loss: 0.392 - 0s 855us/step - loss: 0.3579 - val_loss: 2.4690\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.331 - 0s 874us/step - loss: 0.3593 - val_loss: 2.3504\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.362 - 0s 1ms/step - loss: 0.3350 - val_loss: 2.4259\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.336 - 0s 836us/step - loss: 0.3390 - val_loss: 2.5541\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.272 - 0s 855us/step - loss: 0.3373 - val_loss: 2.3778\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.352 - 0s 836us/step - loss: 0.3431 - val_loss: 2.5146\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.361 - 0s 883us/step - loss: 0.3526 - val_loss: 2.1855\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.470 - ETA: 0s - loss: 0.275 - 0s 884us/step - loss: 0.3347 - val_loss: 2.4652\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 3.53 - ETA: 0s - loss: 0.9371 - 1s 9ms/step - loss: 0.9415 - val_loss: 0.0438\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.205 - ETA: 0s - loss: 0.691 - 0s 950us/step - loss: 0.8815 - val_loss: 0.1403\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.706 - 0s 912us/step - loss: 0.8405 - val_loss: 0.2628\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.596 - ETA: 0s - loss: 0.814 - 0s 874us/step - loss: 0.8237 - val_loss: 0.3212\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.810 - 0s 874us/step - loss: 0.8201 - val_loss: 0.3488\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.626 - ETA: 0s - loss: 0.761 - 0s 874us/step - loss: 0.8224 - val_loss: 0.3456\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.733 - 0s 893us/step - loss: 0.8188 - val_loss: 0.3387\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.919 - ETA: 0s - loss: 0.764 - 0s 912us/step - loss: 0.8118 - val_loss: 0.3355\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.501 - ETA: 0s - loss: 0.927 - 0s 941us/step - loss: 0.8062 - val_loss: 0.3286\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.887 - 0s 779us/step - loss: 0.7930 - val_loss: 0.3409\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.816 - 0s 902us/step - loss: 0.8032 - val_loss: 0.2920\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.767 - 0s 883us/step - loss: 0.7957 - val_loss: 0.2980\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.144 - ETA: 0s - loss: 1.165 - 0s 979us/step - loss: 0.7897 - val_loss: 0.2845\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.563 - ETA: 0s - loss: 0.861 - 0s 751us/step - loss: 0.7937 - val_loss: 0.2806\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.863 - ETA: 0s - loss: 0.925 - 0s 770us/step - loss: 0.7912 - val_loss: 0.2796\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.868 - 0s 845us/step - loss: 0.7854 - val_loss: 0.2882\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.844 - 0s 798us/step - loss: 0.7978 - val_loss: 0.2634\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.711 - 0s 779us/step - loss: 0.7879 - val_loss: 0.2610\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.791 - ETA: 0s - loss: 0.792 - 0s 865us/step - loss: 0.7874 - val_loss: 0.2685\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.932 - ETA: 0s - loss: 0.767 - 0s 855us/step - loss: 0.7872 - val_loss: 0.2471\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.627 - ETA: 0s - loss: 0.643 - 0s 997us/step - loss: 0.7802 - val_loss: 0.2490\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.158 - ETA: 0s - loss: 0.718 - 0s 817us/step - loss: 0.7784 - val_loss: 0.2466\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.773 - 0s 836us/step - loss: 0.7788 - val_loss: 0.2291\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.880 - 0s 874us/step - loss: 0.7866 - val_loss: 0.2116\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.508 - ETA: 0s - loss: 0.708 - 0s 874us/step - loss: 0.7835 - val_loss: 0.2459\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.701 - ETA: 0s - loss: 0.656 - 0s 855us/step - loss: 0.7773 - val_loss: 0.2651\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.746 - 0s 807us/step - loss: 0.7823 - val_loss: 0.2713\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.735 - 0s 864us/step - loss: 0.7686 - val_loss: 0.2815\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.986 - ETA: 0s - loss: 0.901 - 0s 807us/step - loss: 0.7784 - val_loss: 0.2335\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.595 - ETA: 0s - loss: 0.749 - 0s 836us/step - loss: 0.7691 - val_loss: 0.1988\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.619 - 0s 969us/step - loss: 0.7708 - val_loss: 0.1869\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.47 - ETA: 0s - loss: 0.8598 - 1s 9ms/step - loss: 0.8874 - val_loss: 0.0501\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.836 - 0s 883us/step - loss: 0.7961 - val_loss: 0.1387\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.551 - 0s 864us/step - loss: 0.7166 - val_loss: 0.2344\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.753 - 0s 884us/step - loss: 0.6681 - val_loss: 0.4157\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.417 - ETA: 0s - loss: 0.748 - 0s 931us/step - loss: 0.6121 - val_loss: 0.5707\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.563 - 0s 884us/step - loss: 0.5922 - val_loss: 0.6051\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.519 - 0s 884us/step - loss: 0.5848 - val_loss: 0.6426\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.600 - ETA: 0s - loss: 0.641 - 0s 874us/step - loss: 0.5522 - val_loss: 0.7355\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.669 - 0s 864us/step - loss: 0.5552 - val_loss: 0.7667\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.552 - 0s 883us/step - loss: 0.5412 - val_loss: 0.7706\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.593 - 0s 979us/step - loss: 0.5260 - val_loss: 0.8278\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.009 - ETA: 0s - loss: 0.442 - 0s 1ms/step - loss: 0.5227 - val_loss: 0.7717\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.505 - ETA: 0s - loss: 0.518 - 0s 836us/step - loss: 0.5280 - val_loss: 0.7760\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.553 - 0s 883us/step - loss: 0.5165 - val_loss: 0.8931\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.692 - ETA: 0s - loss: 0.598 - 0s 922us/step - loss: 0.5270 - val_loss: 0.8691\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.534 - 0s 931us/step - loss: 0.5100 - val_loss: 0.9152\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.467 - 0s 969us/step - loss: 0.5096 - val_loss: 0.8773\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.506 - 0s 893us/step - loss: 0.5186 - val_loss: 0.8730\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.487 - 0s 950us/step - loss: 0.5100 - val_loss: 0.8735\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.869 - ETA: 0s - loss: 0.449 - 0s 931us/step - loss: 0.5187 - val_loss: 0.8296\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.488 - 0s 912us/step - loss: 0.5145 - val_loss: 0.9043\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.639 - 0s 931us/step - loss: 0.5275 - val_loss: 0.9089\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.426 - ETA: 0s - loss: 0.549 - 0s 912us/step - loss: 0.5133 - val_loss: 0.8739\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.340 - ETA: 0s - loss: 0.527 - 0s 940us/step - loss: 0.5235 - val_loss: 0.9085\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.581 - 0s 864us/step - loss: 0.5215 - val_loss: 0.8234\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.325 - 0s 912us/step - loss: 0.5038 - val_loss: 0.8269\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.616 - 0s 903us/step - loss: 0.5112 - val_loss: 0.9178\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.545 - 0s 940us/step - loss: 0.5214 - val_loss: 0.9470\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.451 - ETA: 0s - loss: 0.554 - 0s 922us/step - loss: 0.5122 - val_loss: 0.9237\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.536 - 0s 912us/step - loss: 0.5251 - val_loss: 0.8728\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.453 - 0s 836us/step - loss: 0.5319 - val_loss: 0.7285\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.65 - ETA: 0s - loss: 0.9762 - 1s 9ms/step - loss: 0.9973 - val_loss: 0.0075\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.853 - 0s 855us/step - loss: 0.9118 - val_loss: 0.0892\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.775 - 0s 826us/step - loss: 0.8517 - val_loss: 0.2247\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.867 - ETA: 0s - loss: 0.904 - 0s 836us/step - loss: 0.8050 - val_loss: 0.3398\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.303 - ETA: 0s - loss: 0.818 - 0s 836us/step - loss: 0.7978 - val_loss: 0.4469\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.715 - ETA: 0s - loss: 0.694 - 0s 931us/step - loss: 0.7662 - val_loss: 0.5733\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.474 - 0s 940us/step - loss: 0.7742 - val_loss: 0.6262\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.913 - ETA: 0s - loss: 0.899 - 0s 931us/step - loss: 0.7666 - val_loss: 0.7219\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.452 - 0s 874us/step - loss: 0.7651 - val_loss: 0.8234\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.772 - ETA: 0s - loss: 0.635 - 0s 874us/step - loss: 0.7664 - val_loss: 0.7402\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.877 - 0s 893us/step - loss: 0.7804 - val_loss: 0.8942\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.814 - 0s 864us/step - loss: 0.7634 - val_loss: 0.8168\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.821 - 0s 836us/step - loss: 0.7525 - val_loss: 0.8735\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.903 - 0s 874us/step - loss: 0.7657 - val_loss: 0.9758\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.535 - ETA: 0s - loss: 0.575 - 0s 845us/step - loss: 0.7653 - val_loss: 0.9725\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.792 - 0s 826us/step - loss: 0.7689 - val_loss: 0.9757\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.793 - 0s 902us/step - loss: 0.7592 - val_loss: 0.9250\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.659 - 0s 969us/step - loss: 0.7643 - val_loss: 1.0192\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.914 - 0s 940us/step - loss: 0.7692 - val_loss: 1.0634\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.327 - ETA: 0s - loss: 0.683 - 0s 902us/step - loss: 0.7636 - val_loss: 0.9733\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.776 - ETA: 0s - loss: 0.660 - 0s 912us/step - loss: 0.7604 - val_loss: 1.0220\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.464 - ETA: 0s - loss: 0.607 - 0s 902us/step - loss: 0.7580 - val_loss: 1.0196\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.518 - ETA: 0s - loss: 0.884 - 0s 855us/step - loss: 0.7540 - val_loss: 1.0928\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.492 - ETA: 0s - loss: 0.436 - 0s 921us/step - loss: 0.7676 - val_loss: 1.1572\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.944 - 0s 912us/step - loss: 0.7566 - val_loss: 1.0010\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.744 - 0s 931us/step - loss: 0.7673 - val_loss: 1.0363\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.580 - 0s 902us/step - loss: 0.7575 - val_loss: 1.0747\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.639 - ETA: 0s - loss: 1.026 - 0s 969us/step - loss: 0.7661 - val_loss: 1.3043\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.377 - ETA: 0s - loss: 0.862 - 0s 845us/step - loss: 0.7536 - val_loss: 1.2724\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.623 - ETA: 0s - loss: 0.872 - 0s 855us/step - loss: 0.7611 - val_loss: 1.0417\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.549 - ETA: 0s - loss: 0.720 - 0s 1ms/step - loss: 0.7606 - val_loss: 1.2217\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 7\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 1.08 - ETA: 0s - loss: 1.0629 - 1s 9ms/step - loss: 0.8754 - val_loss: 0.0903\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.800 - 0s 883us/step - loss: 0.7772 - val_loss: 0.2744\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.461 - ETA: 0s - loss: 0.739 - 0s 931us/step - loss: 0.7352 - val_loss: 0.4786\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.562 - 0s 893us/step - loss: 0.6836 - val_loss: 0.6279\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.531 - ETA: 0s - loss: 0.712 - 0s 941us/step - loss: 0.6685 - val_loss: 0.9035\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.707 - 0s 941us/step - loss: 0.6586 - val_loss: 0.8634\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.951 - ETA: 0s - loss: 0.655 - 0s 940us/step - loss: 0.6675 - val_loss: 0.9499\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.494 - ETA: 0s - loss: 0.738 - 0s 874us/step - loss: 0.6566 - val_loss: 0.8904\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.125 - ETA: 0s - loss: 0.734 - 0s 921us/step - loss: 0.6545 - val_loss: 0.8742\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.439 - ETA: 0s - loss: 0.622 - 0s 855us/step - loss: 0.6461 - val_loss: 0.9738\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.491 - 0s 855us/step - loss: 0.6334 - val_loss: 1.0064\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.493 - ETA: 0s - loss: 0.582 - 0s 855us/step - loss: 0.6195 - val_loss: 1.0339\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.663 - 0s 836us/step - loss: 0.6237 - val_loss: 1.0897\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.643 - 0s 874us/step - loss: 0.6210 - val_loss: 1.0824\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.992 - ETA: 0s - loss: 0.614 - 0s 845us/step - loss: 0.6084 - val_loss: 1.0383\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.606 - 0s 817us/step - loss: 0.6021 - val_loss: 1.0273\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.577 - 0s 959us/step - loss: 0.6230 - val_loss: 1.1413\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.700 - 0s 874us/step - loss: 0.6151 - val_loss: 1.3801\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.484 - 0s 836us/step - loss: 0.5990 - val_loss: 1.2009\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.129 - ETA: 0s - loss: 0.580 - 0s 855us/step - loss: 0.6006 - val_loss: 1.2481\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.958 - ETA: 0s - loss: 0.641 - 0s 855us/step - loss: 0.5965 - val_loss: 1.3635\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.656 - 0s 874us/step - loss: 0.5945 - val_loss: 1.2026\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.622 - ETA: 0s - loss: 0.519 - 0s 940us/step - loss: 0.6047 - val_loss: 1.2097\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.621 - ETA: 0s - loss: 0.555 - 0s 864us/step - loss: 0.5777 - val_loss: 1.5893\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.042 - ETA: 0s - loss: 0.653 - 0s 855us/step - loss: 0.5879 - val_loss: 1.4407\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.591 - 0s 931us/step - loss: 0.5917 - val_loss: 1.4453\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.565 - 0s 931us/step - loss: 0.5909 - val_loss: 1.3812\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.504 - 0s 893us/step - loss: 0.5991 - val_loss: 1.2916\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.553 - 0s 883us/step - loss: 0.5727 - val_loss: 1.3074\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.561 - 0s 883us/step - loss: 0.5904 - val_loss: 1.3852\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.545 - 0s 864us/step - loss: 0.5725 - val_loss: 1.4503\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 8\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.30 - ETA: 0s - loss: 1.1640 - 1s 9ms/step - loss: 0.9125 - val_loss: 0.0924\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.563 - 0s 874us/step - loss: 0.7487 - val_loss: 0.2833\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.838 - 0s 864us/step - loss: 0.6852 - val_loss: 0.3927\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.447 - ETA: 0s - loss: 0.665 - 0s 902us/step - loss: 0.6270 - val_loss: 0.5305\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.747 - 0s 893us/step - loss: 0.6012 - val_loss: 0.6309\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.719 - 0s 883us/step - loss: 0.5767 - val_loss: 0.6059\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.706 - 0s 883us/step - loss: 0.5698 - val_loss: 0.6000\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.323 - 0s 921us/step - loss: 0.5385 - val_loss: 0.6685\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.569 - 0s 883us/step - loss: 0.5375 - val_loss: 0.7931\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.693 - 0s 912us/step - loss: 0.5593 - val_loss: 0.8045\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.554 - ETA: 0s - loss: 0.647 - 0s 921us/step - loss: 0.5277 - val_loss: 0.7419\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.435 - ETA: 0s - loss: 0.403 - 0s 921us/step - loss: 0.5216 - val_loss: 0.7476\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.541 - ETA: 0s - loss: 0.613 - 0s 903us/step - loss: 0.5178 - val_loss: 0.7747\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.659 - 0s 902us/step - loss: 0.5311 - val_loss: 0.8454\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.254 - 0s 988us/step - loss: 0.5340 - val_loss: 0.6643\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.367 - 0s 950us/step - loss: 0.5183 - val_loss: 0.9372\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.332 - 0s 921us/step - loss: 0.5260 - val_loss: 0.8060\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.700 - ETA: 0s - loss: 0.640 - 0s 931us/step - loss: 0.5015 - val_loss: 0.8544\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.611 - 0s 921us/step - loss: 0.5191 - val_loss: 0.8445\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.556 - 0s 893us/step - loss: 0.5056 - val_loss: 0.7837\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.521 - 0s 865us/step - loss: 0.5118 - val_loss: 0.7750\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.356 - ETA: 0s - loss: 0.377 - 0s 883us/step - loss: 0.5148 - val_loss: 0.7715\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.340 - ETA: 0s - loss: 0.595 - 0s 912us/step - loss: 0.5162 - val_loss: 0.7783\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.271 - 0s 893us/step - loss: 0.5156 - val_loss: 0.7554\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.760 - ETA: 0s - loss: 0.698 - 0s 997us/step - loss: 0.5112 - val_loss: 0.8257\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.588 - ETA: 0s - loss: 0.524 - 0s 798us/step - loss: 0.4986 - val_loss: 0.7813\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 5.417 - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.523 - 0s 1ms/step - loss: 0.5190 - val_loss: 0.7890\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.548 - 0s 874us/step - loss: 0.4946 - val_loss: 0.7904\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.335 - 0s 931us/step - loss: 0.5232 - val_loss: 0.7987\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.262 - 0s 855us/step - loss: 0.5049 - val_loss: 0.8184\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.312 - 0s 893us/step - loss: 0.5152 - val_loss: 0.7832\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 9\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 2.44 - ETA: 0s - loss: 0.9359 - 1s 9ms/step - loss: 0.9274 - val_loss: 0.0341\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.768 - ETA: 0s - loss: 0.879 - 0s 836us/step - loss: 0.7853 - val_loss: 0.1218\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.653 - 0s 826us/step - loss: 0.6779 - val_loss: 0.2711\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.498 - ETA: 0s - loss: 0.869 - ETA: 0s - loss: 0.646 - 0s 1ms/step - loss: 0.5959 - val_loss: 0.4616\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.564 - 0s 921us/step - loss: 0.5296 - val_loss: 0.5400\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.430 - 0s 865us/step - loss: 0.5160 - val_loss: 0.7056\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.050 - ETA: 0s - loss: 0.529 - 0s 893us/step - loss: 0.4907 - val_loss: 0.7810\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.027 - ETA: 0s - loss: 0.554 - 0s 940us/step - loss: 0.4391 - val_loss: 0.7633\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.462 - 0s 874us/step - loss: 0.4367 - val_loss: 0.7829\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.510 - 0s 931us/step - loss: 0.4053 - val_loss: 0.8606\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.283 - 0s 979us/step - loss: 0.3907 - val_loss: 0.7814\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.332 - 0s 969us/step - loss: 0.3858 - val_loss: 0.8011\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.434 - ETA: 0s - loss: 0.318 - 0s 940us/step - loss: 0.3690 - val_loss: 0.7326\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.266 - 0s 912us/step - loss: 0.3917 - val_loss: 0.7328\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.046 - ETA: 0s - loss: 0.320 - 0s 931us/step - loss: 0.3686 - val_loss: 0.7830\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.388 - 0s 931us/step - loss: 0.3781 - val_loss: 0.7129\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.342 - 0s 912us/step - loss: 0.3821 - val_loss: 0.7297\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.321 - 0s 931us/step - loss: 0.3780 - val_loss: 0.6938\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.438 - ETA: 0s - loss: 0.318 - 0s 855us/step - loss: 0.3765 - val_loss: 0.6878\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.653 - ETA: 0s - loss: 0.482 - 0s 940us/step - loss: 0.3788 - val_loss: 0.7319\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.746 - ETA: 0s - loss: 0.415 - 0s 883us/step - loss: 0.3595 - val_loss: 0.7413\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.526 - ETA: 0s - loss: 0.403 - 0s 864us/step - loss: 0.3744 - val_loss: 0.6059\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.396 - 0s 950us/step - loss: 0.3620 - val_loss: 0.6208\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.497 - ETA: 0s - loss: 0.390 - 0s 902us/step - loss: 0.3853 - val_loss: 0.6058\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.198 - ETA: 0s - loss: 0.445 - 0s 855us/step - loss: 0.3803 - val_loss: 0.6684\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.513 - ETA: 0s - loss: 0.442 - 0s 836us/step - loss: 0.3670 - val_loss: 0.6142\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.251 - 0s 864us/step - loss: 0.3637 - val_loss: 0.6114\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.268 - 0s 864us/step - loss: 0.3589 - val_loss: 0.6168\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.412 - ETA: 0s - loss: 0.401 - 0s 893us/step - loss: 0.3688 - val_loss: 0.7263\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.369 - 0s 931us/step - loss: 0.3613 - val_loss: 0.6238\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 0.221 - 0s 1ms/step - loss: 0.3638 - val_loss: 0.6355\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.13 - ETA: 0s - loss: 0.4119 - 1s 9ms/step - loss: 0.8105 - val_loss: 0.0774\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.859 - 0s 845us/step - loss: 0.6899 - val_loss: 0.2011\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.618 - 0s 874us/step - loss: 0.5611 - val_loss: 0.3841\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.943 - ETA: 0s - loss: 0.553 - 0s 874us/step - loss: 0.5314 - val_loss: 0.6942\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.420 - ETA: 0s - loss: 0.558 - 0s 969us/step - loss: 0.4633 - val_loss: 0.8085\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.492 - 0s 912us/step - loss: 0.4211 - val_loss: 0.8560\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.388 - 0s 902us/step - loss: 0.4387 - val_loss: 0.8674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.527 - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.359 - 0s 1ms/step - loss: 0.3776 - val_loss: 0.9111\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.087 - ETA: 0s - loss: 0.462 - 0s 893us/step - loss: 0.3856 - val_loss: 0.9718\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.303 - 0s 959us/step - loss: 0.3741 - val_loss: 0.9400\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.286 - 0s 884us/step - loss: 0.3699 - val_loss: 1.0234\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.394 - 0s 874us/step - loss: 0.3389 - val_loss: 1.2888\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.255 - 0s 922us/step - loss: 0.3499 - val_loss: 1.0979\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.320 - 0s 883us/step - loss: 0.3436 - val_loss: 1.0871\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.767 - ETA: 0s - loss: 0.322 - 0s 874us/step - loss: 0.3470 - val_loss: 1.0873\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.721 - ETA: 0s - loss: 0.262 - 0s 893us/step - loss: 0.3499 - val_loss: 1.0187\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.279 - 0s 883us/step - loss: 0.3290 - val_loss: 1.0959\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.354 - 0s 1ms/step - loss: 0.3469 - val_loss: 1.0794\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.417 - 0s 817us/step - loss: 0.3475 - val_loss: 1.0901\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.226 - 0s 979us/step - loss: 0.3219 - val_loss: 1.0152\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.289 - 0s 902us/step - loss: 0.3117 - val_loss: 1.1550\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.308 - 0s 912us/step - loss: 0.3353 - val_loss: 1.0017\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.001 - ETA: 0s - loss: 0.267 - 0s 883us/step - loss: 0.3140 - val_loss: 1.0678\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.306 - 0s 1ms/step - loss: 0.3383 - val_loss: 1.1191\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.209 - 0s 921us/step - loss: 0.3052 - val_loss: 1.1494\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.250 - 0s 893us/step - loss: 0.3094 - val_loss: 1.1165\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.428 - 0s 931us/step - loss: 0.3210 - val_loss: 1.0767\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.290 - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.334 - 0s 2ms/step - loss: 0.3141 - val_loss: 1.1435\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.458 - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.239 - 0s 2ms/step - loss: 0.3031 - val_loss: 1.0156\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.890 - ETA: 0s - loss: 0.307 - 0s 741us/step - loss: 0.3141 - val_loss: 1.2084\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.320 - 0s 741us/step - loss: 0.3246 - val_loss: 1.0066\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 11\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.10 - ETA: 0s - loss: 1.1492 - 1s 9ms/step - loss: 0.9367 - val_loss: 0.0442\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.401 - ETA: 0s - loss: 0.952 - 0s 950us/step - loss: 0.7755 - val_loss: 0.1967\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.064 - ETA: 0s - loss: 0.699 - 0s 921us/step - loss: 0.6857 - val_loss: 0.3804\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.665 - 0s 931us/step - loss: 0.6135 - val_loss: 0.6053\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.443 - ETA: 0s - loss: 0.622 - 0s 902us/step - loss: 0.5691 - val_loss: 0.7943\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.563 - 0s 988us/step - loss: 0.5264 - val_loss: 0.9651\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.577 - 0s 902us/step - loss: 0.5230 - val_loss: 1.0524\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.421 - ETA: 0s - loss: 0.550 - 0s 902us/step - loss: 0.5120 - val_loss: 1.1225\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.532 - 0s 893us/step - loss: 0.5019 - val_loss: 1.1598\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.731 - ETA: 0s - loss: 0.542 - 0s 959us/step - loss: 0.4764 - val_loss: 1.2635\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.280 - 0s 921us/step - loss: 0.4941 - val_loss: 1.2584\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.550 - 0s 940us/step - loss: 0.4799 - val_loss: 1.2627\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.512 - 0s 902us/step - loss: 0.4485 - val_loss: 1.3337\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.432 - ETA: 0s - loss: 0.574 - 0s 921us/step - loss: 0.4810 - val_loss: 1.3345\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.501 - 0s 912us/step - loss: 0.4469 - val_loss: 1.4552\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.437 - 0s 969us/step - loss: 0.4510 - val_loss: 1.3770\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.478 - 0s 931us/step - loss: 0.4614 - val_loss: 1.3719\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.012 - ETA: 0s - loss: 0.523 - 0s 893us/step - loss: 0.4695 - val_loss: 1.4386\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.100 - ETA: 0s - loss: 0.486 - 0s 902us/step - loss: 0.4563 - val_loss: 1.3890\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.630 - ETA: 0s - loss: 0.500 - 0s 931us/step - loss: 0.4621 - val_loss: 1.4964\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.379 - 0s 950us/step - loss: 0.4708 - val_loss: 1.4863\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.534 - 0s 902us/step - loss: 0.4524 - val_loss: 1.2061\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.552 - ETA: 0s - loss: 0.446 - 0s 950us/step - loss: 0.4600 - val_loss: 1.2436\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.501 - ETA: 0s - loss: 0.483 - 0s 893us/step - loss: 0.4485 - val_loss: 1.3673\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.661 - ETA: 0s - loss: 0.461 - 0s 902us/step - loss: 0.4574 - val_loss: 1.4456\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.484 - 0s 884us/step - loss: 0.4561 - val_loss: 1.3677\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.456 - ETA: 0s - loss: 0.468 - 0s 855us/step - loss: 0.4551 - val_loss: 1.5074\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.296 - 0s 817us/step - loss: 0.4453 - val_loss: 1.3931\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.446 - 0s 836us/step - loss: 0.4358 - val_loss: 1.4956\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.371 - 0s 902us/step - loss: 0.4367 - val_loss: 1.5409\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.398 - ETA: 0s - loss: 0.449 - 0s 865us/step - loss: 0.4372 - val_loss: 1.5759\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 12\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.66 - ETA: 0s - loss: 0.8648 - 1s 10ms/step - loss: 0.9398 - val_loss: 0.0552\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.332 - ETA: 0s - loss: 0.818 - 0s 883us/step - loss: 0.7909 - val_loss: 0.2459\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.666 - ETA: 0s - loss: 0.680 - 0s 1ms/step - loss: 0.6650 - val_loss: 0.4832\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.742 - 0s 969us/step - loss: 0.5764 - val_loss: 0.8859\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.542 - 0s 826us/step - loss: 0.5199 - val_loss: 1.0583\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - ETA: 0s - loss: 0.466 - 0s 769us/step - loss: 0.4964 - val_loss: 1.3219\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.450 - 0s 845us/step - loss: 0.4765 - val_loss: 1.3020\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.392 - 0s 808us/step - loss: 0.4664 - val_loss: 1.2980\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.425 - 0s 874us/step - loss: 0.4568 - val_loss: 1.3419\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.727 - ETA: 0s - loss: 0.527 - ETA: 0s - loss: 0.457 - 0s 1ms/step - loss: 0.4484 - val_loss: 1.2976\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.086 - ETA: 0s - loss: 0.491 - 0s 922us/step - loss: 0.4528 - val_loss: 1.2993\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.356 - 0s 902us/step - loss: 0.4442 - val_loss: 1.2188\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.596 - ETA: 0s - loss: 0.500 - 0s 893us/step - loss: 0.4391 - val_loss: 1.3274\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.043 - ETA: 0s - loss: 0.444 - 0s 807us/step - loss: 0.4287 - val_loss: 1.2850\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.115 - ETA: 0s - loss: 0.454 - 0s 855us/step - loss: 0.4374 - val_loss: 1.1373\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.495 - 0s 912us/step - loss: 0.4246 - val_loss: 1.1770\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.722 - ETA: 0s - loss: 0.454 - 0s 1ms/step - loss: 0.4291 - val_loss: 1.2120\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.437 - 0s 883us/step - loss: 0.4445 - val_loss: 1.1860\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.277 - 0s 864us/step - loss: 0.4283 - val_loss: 1.2517\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.357 - 0s 864us/step - loss: 0.4145 - val_loss: 1.3956\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.758 - ETA: 0s - loss: 0.464 - 0s 874us/step - loss: 0.4123 - val_loss: 1.3035\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.455 - 0s 921us/step - loss: 0.4137 - val_loss: 1.1617\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.421 - 0s 893us/step - loss: 0.4255 - val_loss: 1.0989\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.392 - 0s 902us/step - loss: 0.4147 - val_loss: 1.0715\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.327 - ETA: 0s - loss: 0.478 - 0s 912us/step - loss: 0.4146 - val_loss: 1.2033\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.608 - ETA: 0s - loss: 0.439 - 0s 883us/step - loss: 0.4259 - val_loss: 1.2165\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.627 - ETA: 0s - loss: 0.368 - 0s 883us/step - loss: 0.4074 - val_loss: 1.2714\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.508 - 0s 902us/step - loss: 0.4062 - val_loss: 1.2502\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.408 - 0s 912us/step - loss: 0.3946 - val_loss: 1.1689\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.454 - 0s 988us/step - loss: 0.4004 - val_loss: 1.0699\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.378 - 0s 922us/step - loss: 0.4175 - val_loss: 1.1701\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 13\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 18s - loss: 0.72 - ETA: 0s - loss: 0.9174 - 1s 9ms/step - loss: 0.9216 - val_loss: 0.0264\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.880 - 0s 950us/step - loss: 0.7736 - val_loss: 0.1099\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.453 - ETA: 0s - loss: 0.617 - 0s 817us/step - loss: 0.7033 - val_loss: 0.2587\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.554 - 0s 902us/step - loss: 0.6494 - val_loss: 0.3707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.668 - 0s 826us/step - loss: 0.6360 - val_loss: 0.5135\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.007 - ETA: 0s - loss: 0.641 - 0s 855us/step - loss: 0.6293 - val_loss: 0.5085\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.006 - ETA: 0s - loss: 0.468 - 0s 845us/step - loss: 0.6159 - val_loss: 0.5648\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.721 - 0s 836us/step - loss: 0.6235 - val_loss: 0.7064\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.516 - ETA: 0s - loss: 0.709 - 0s 845us/step - loss: 0.6245 - val_loss: 0.6768\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.079 - ETA: 0s - loss: 0.706 - 0s 845us/step - loss: 0.6192 - val_loss: 0.7471\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.469 - ETA: 0s - loss: 0.601 - 0s 969us/step - loss: 0.6095 - val_loss: 0.7854\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.608 - 0s 959us/step - loss: 0.6151 - val_loss: 0.8073\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.486 - 0s 874us/step - loss: 0.6020 - val_loss: 0.8932\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.615 - 0s 826us/step - loss: 0.6099 - val_loss: 0.9194\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.311 - ETA: 0s - loss: 0.587 - 0s 941us/step - loss: 0.6141 - val_loss: 1.0253\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.688 - 0s 855us/step - loss: 0.6161 - val_loss: 1.1265\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.857 - ETA: 0s - loss: 0.623 - 0s 817us/step - loss: 0.5992 - val_loss: 1.0746\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.583 - 0s 960us/step - loss: 0.6030 - val_loss: 1.0671\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.462 - ETA: 0s - loss: 0.682 - 0s 798us/step - loss: 0.6050 - val_loss: 1.0933\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.662 - 0s 864us/step - loss: 0.6059 - val_loss: 1.1453\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.120 - ETA: 0s - loss: 0.668 - 0s 893us/step - loss: 0.6000 - val_loss: 1.1226\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.666 - ETA: 0s - loss: 0.599 - 0s 864us/step - loss: 0.6113 - val_loss: 1.0735\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.630 - 0s 836us/step - loss: 0.5911 - val_loss: 1.1244\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.377 - ETA: 0s - loss: 0.591 - 0s 845us/step - loss: 0.5955 - val_loss: 1.1773\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.975 - ETA: 0s - loss: 0.640 - 0s 864us/step - loss: 0.5999 - val_loss: 1.2118\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.981 - ETA: 0s - loss: 0.449 - 0s 845us/step - loss: 0.5841 - val_loss: 1.4243\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.502 - 0s 855us/step - loss: 0.5921 - val_loss: 1.4430\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.745 - 0s 912us/step - loss: 0.5907 - val_loss: 1.3649\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.170 - ETA: 0s - loss: 0.605 - 0s 826us/step - loss: 0.5980 - val_loss: 1.3368\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.667 - 0s 903us/step - loss: 0.6055 - val_loss: 1.3494\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.529 - 0s 912us/step - loss: 0.5912 - val_loss: 1.3425\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 14\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.49 - ETA: 0s - loss: 0.8535 - 1s 9ms/step - loss: 0.9041 - val_loss: 0.0667\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.730 - 0s 874us/step - loss: 0.7788 - val_loss: 0.1675\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.028 - ETA: 0s - loss: 0.611 - 0s 846us/step - loss: 0.7067 - val_loss: 0.3623\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.802 - ETA: 0s - loss: 0.803 - 0s 921us/step - loss: 0.6618 - val_loss: 0.5310\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.704 - ETA: 0s - loss: 0.641 - 0s 912us/step - loss: 0.6332 - val_loss: 0.6657\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.550 - 0s 902us/step - loss: 0.5978 - val_loss: 0.7215\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.426 - 0s 902us/step - loss: 0.5836 - val_loss: 0.7962\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.539 - 0s 969us/step - loss: 0.5850 - val_loss: 0.9772\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.620 - 0s 940us/step - loss: 0.5650 - val_loss: 1.1029\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.959 - ETA: 0s - loss: 0.613 - 0s 921us/step - loss: 0.5591 - val_loss: 1.2228\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.507 - 0s 874us/step - loss: 0.5636 - val_loss: 1.2007\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.536 - 0s 959us/step - loss: 0.5637 - val_loss: 1.2032\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.200 - ETA: 0s - loss: 0.510 - 0s 978us/step - loss: 0.5503 - val_loss: 1.3127\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.003 - ETA: 0s - loss: 0.602 - 0s 931us/step - loss: 0.5356 - val_loss: 1.1892\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.577 - ETA: 0s - loss: 0.485 - 0s 893us/step - loss: 0.5427 - val_loss: 1.2994\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.672 - ETA: 0s - loss: 0.484 - 0s 950us/step - loss: 0.5678 - val_loss: 1.2865\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.461 - 0s 902us/step - loss: 0.5433 - val_loss: 1.3861\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.961 - ETA: 0s - loss: 0.577 - 0s 931us/step - loss: 0.5419 - val_loss: 1.4211\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.559 - 0s 874us/step - loss: 0.5366 - val_loss: 1.5059\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.431 - 0s 931us/step - loss: 0.5429 - val_loss: 1.4831\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.108 - ETA: 0s - loss: 0.577 - 0s 912us/step - loss: 0.5561 - val_loss: 1.4107\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.024 - ETA: 0s - loss: 0.602 - 0s 921us/step - loss: 0.5476 - val_loss: 1.4036\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.426 - 0s 855us/step - loss: 0.5466 - val_loss: 1.4238\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.558 - ETA: 0s - loss: 0.530 - 0s 855us/step - loss: 0.5344 - val_loss: 1.4907\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.593 - 0s 884us/step - loss: 0.5232 - val_loss: 1.4485\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.204 - ETA: 0s - loss: 0.623 - 0s 874us/step - loss: 0.5490 - val_loss: 1.4811\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.993 - ETA: 0s - loss: 0.584 - 0s 855us/step - loss: 0.5353 - val_loss: 1.4138\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.530 - 0s 912us/step - loss: 0.5240 - val_loss: 1.4577\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.543 - 0s 864us/step - loss: 0.5560 - val_loss: 1.4630\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.571 - 0s 883us/step - loss: 0.5278 - val_loss: 1.5412\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.886 - ETA: 0s - loss: 0.506 - 0s 855us/step - loss: 0.5314 - val_loss: 1.5394\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 15\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 20s - loss: 0.32 - ETA: 1s - loss: 0.9068 - 1s 10ms/step - loss: 0.9648 - val_loss: 0.0254\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.878 - 0s 826us/step - loss: 0.9083 - val_loss: 0.1135\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.507 - ETA: 0s - loss: 0.498 - 0s 855us/step - loss: 0.8628 - val_loss: 0.1667\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 1.000 - 0s 836us/step - loss: 0.8420 - val_loss: 0.2183\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.664 - ETA: 0s - loss: 1.033 - 0s 874us/step - loss: 0.8401 - val_loss: 0.2701\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.435 - ETA: 0s - loss: 0.806 - 0s 855us/step - loss: 0.8282 - val_loss: 0.2870\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.884 - ETA: 0s - loss: 0.821 - 0s 1ms/step - loss: 0.8221 - val_loss: 0.2982\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.778 - ETA: 0s - loss: 0.930 - 0s 959us/step - loss: 0.8102 - val_loss: 0.3243\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.756 - ETA: 0s - loss: 0.738 - 0s 855us/step - loss: 0.8073 - val_loss: 0.3009\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.832 - 0s 864us/step - loss: 0.8034 - val_loss: 0.2687\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.625 - ETA: 0s - loss: 0.934 - 0s 836us/step - loss: 0.8085 - val_loss: 0.2605\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.412 - ETA: 0s - loss: 0.759 - 0s 826us/step - loss: 0.7982 - val_loss: 0.2654\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.989 - ETA: 0s - loss: 0.800 - 0s 893us/step - loss: 0.8088 - val_loss: 0.2691\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.826 - ETA: 0s - loss: 0.994 - 0s 855us/step - loss: 0.8057 - val_loss: 0.2780\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.062 - ETA: 0s - loss: 0.964 - 0s 921us/step - loss: 0.7911 - val_loss: 0.2548\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.732 - 0s 893us/step - loss: 0.7816 - val_loss: 0.2562\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.590 - ETA: 0s - loss: 0.813 - 0s 950us/step - loss: 0.7873 - val_loss: 0.2696\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.719 - 0s 789us/step - loss: 0.7755 - val_loss: 0.2811\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.941 - 0s 836us/step - loss: 0.7820 - val_loss: 0.2625\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.716 - 0s 893us/step - loss: 0.7820 - val_loss: 0.2621\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.555 - ETA: 0s - loss: 0.786 - 0s 902us/step - loss: 0.7833 - val_loss: 0.2471\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.432 - ETA: 0s - loss: 0.928 - 0s 902us/step - loss: 0.7703 - val_loss: 0.2398\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.875 - ETA: 0s - loss: 0.655 - 0s 893us/step - loss: 0.7763 - val_loss: 0.2720\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.738 - 0s 883us/step - loss: 0.7807 - val_loss: 0.2446\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.444 - 0s 912us/step - loss: 0.7738 - val_loss: 0.2414\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.760 - 0s 883us/step - loss: 0.7702 - val_loss: 0.2254\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.764 - 0s 902us/step - loss: 0.7628 - val_loss: 0.2242\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.630 - ETA: 0s - loss: 0.867 - 0s 893us/step - loss: 0.7720 - val_loss: 0.2324\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.725 - 0s 874us/step - loss: 0.7737 - val_loss: 0.2190\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.696 - 0s 836us/step - loss: 0.7647 - val_loss: 0.2396\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.720 - 0s 874us/step - loss: 0.7631 - val_loss: 0.2182\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 16\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.70 - ETA: 0s - loss: 1.0598 - 1s 9ms/step - loss: 0.9096 - val_loss: 0.0378\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.371 - ETA: 0s - loss: 0.897 - 0s 959us/step - loss: 0.7781 - val_loss: 0.1091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.797 - 0s 903us/step - loss: 0.6976 - val_loss: 0.2243\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.598 - 0s 779us/step - loss: 0.6427 - val_loss: 0.2815\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.978 - ETA: 0s - loss: 0.651 - 0s 921us/step - loss: 0.6152 - val_loss: 0.4334\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.688 - 0s 874us/step - loss: 0.6113 - val_loss: 0.4124\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.533 - 0s 855us/step - loss: 0.5825 - val_loss: 0.4484\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.641 - 0s 893us/step - loss: 0.5790 - val_loss: 0.5180\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.487 - 0s 922us/step - loss: 0.5693 - val_loss: 0.4714\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.688 - 0s 883us/step - loss: 0.5820 - val_loss: 0.5049\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.620 - 0s 855us/step - loss: 0.5701 - val_loss: 0.5408\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.513 - 0s 874us/step - loss: 0.5756 - val_loss: 0.4921\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.564 - 0s 883us/step - loss: 0.5669 - val_loss: 0.5042\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.220 - ETA: 0s - loss: 0.562 - 0s 912us/step - loss: 0.5611 - val_loss: 0.5105\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.130 - ETA: 0s - loss: 0.551 - 0s 931us/step - loss: 0.5662 - val_loss: 0.5101\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.940 - ETA: 0s - loss: 0.609 - 0s 883us/step - loss: 0.5696 - val_loss: 0.5408\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.622 - ETA: 0s - loss: 0.531 - 0s 817us/step - loss: 0.5652 - val_loss: 0.5093\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.650 - 0s 903us/step - loss: 0.5545 - val_loss: 0.5164\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.481 - ETA: 0s - loss: 0.622 - 0s 855us/step - loss: 0.5608 - val_loss: 0.5590\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.507 - 0s 864us/step - loss: 0.5620 - val_loss: 0.5832\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.505 - ETA: 0s - loss: 0.636 - 0s 931us/step - loss: 0.5606 - val_loss: 0.5106\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.647 - 0s 921us/step - loss: 0.5538 - val_loss: 0.4983\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.106 - ETA: 0s - loss: 0.663 - 0s 893us/step - loss: 0.5685 - val_loss: 0.5495\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.723 - ETA: 0s - loss: 0.514 - 0s 883us/step - loss: 0.5568 - val_loss: 0.5813\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.617 - ETA: 0s - loss: 0.595 - 0s 997us/step - loss: 0.5648 - val_loss: 0.5526\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.459 - ETA: 0s - loss: 0.556 - 0s 893us/step - loss: 0.5555 - val_loss: 0.5648\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.215 - ETA: 0s - loss: 0.567 - 0s 902us/step - loss: 0.5704 - val_loss: 0.5434\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.483 - ETA: 0s - loss: 0.553 - 0s 912us/step - loss: 0.5473 - val_loss: 0.6021\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.867 - ETA: 0s - loss: 0.601 - 0s 874us/step - loss: 0.5493 - val_loss: 0.5804\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.551 - 0s 902us/step - loss: 0.5532 - val_loss: 0.5399\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.448 - 0s 959us/step - loss: 0.5558 - val_loss: 0.5281\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 17\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 1.54 - ETA: 0s - loss: 0.7800 - 1s 9ms/step - loss: 0.9466 - val_loss: 0.0686\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.340 - ETA: 0s - loss: 1.111 - 0s 883us/step - loss: 0.8566 - val_loss: 0.2122\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.754 - 0s 893us/step - loss: 0.8004 - val_loss: 0.3977\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.875 - ETA: 0s - loss: 0.554 - 0s 940us/step - loss: 0.7723 - val_loss: 0.5451\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.710 - ETA: 0s - loss: 0.895 - 0s 855us/step - loss: 0.7658 - val_loss: 0.6198\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.604 - ETA: 0s - loss: 0.796 - 0s 931us/step - loss: 0.7470 - val_loss: 0.7373\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.625 - 0s 940us/step - loss: 0.7289 - val_loss: 0.7981\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.343 - ETA: 0s - loss: 0.598 - 0s 921us/step - loss: 0.7341 - val_loss: 0.8148\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.851 - 0s 940us/step - loss: 0.7312 - val_loss: 0.7129\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.645 - 0s 874us/step - loss: 0.7314 - val_loss: 0.6889\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.590 - 0s 940us/step - loss: 0.7152 - val_loss: 0.6487\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.989 - ETA: 0s - loss: 0.791 - 0s 969us/step - loss: 0.7253 - val_loss: 0.7281\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.771 - 0s 864us/step - loss: 0.7273 - val_loss: 0.7147\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.811 - 0s 1ms/step - loss: 0.7251 - val_loss: 0.6314\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.919 - 0s 988us/step - loss: 0.7260 - val_loss: 0.7094\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.740 - 0s 864us/step - loss: 0.7339 - val_loss: 0.5949\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.567 - 0s 1ms/step - loss: 0.7242 - val_loss: 0.5438\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.191 - ETA: 0s - loss: 0.557 - 0s 883us/step - loss: 0.7319 - val_loss: 0.6833\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.509 - ETA: 0s - loss: 0.449 - 0s 902us/step - loss: 0.7312 - val_loss: 0.6656\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.789 - 0s 921us/step - loss: 0.7357 - val_loss: 0.6597\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.901 - 0s 836us/step - loss: 0.7204 - val_loss: 0.5687\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.754 - 0s 845us/step - loss: 0.7203 - val_loss: 0.6173\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.634 - 0s 912us/step - loss: 0.7339 - val_loss: 0.5535\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.654 - 0s 883us/step - loss: 0.7227 - val_loss: 0.5285\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.826 - 0s 893us/step - loss: 0.7171 - val_loss: 0.6089\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.732 - 0s 855us/step - loss: 0.7175 - val_loss: 0.6316\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.698 - 0s 836us/step - loss: 0.7121 - val_loss: 0.6555\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.636 - 0s 865us/step - loss: 0.7219 - val_loss: 0.6735\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.738 - 0s 883us/step - loss: 0.7066 - val_loss: 0.6255\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.598 - ETA: 0s - loss: 0.790 - 0s 864us/step - loss: 0.7140 - val_loss: 0.6461\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.690 - 0s 836us/step - loss: 0.7000 - val_loss: 0.6888\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 18\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.06 - ETA: 0s - loss: 0.7008 - 1s 10ms/step - loss: 0.8570 - val_loss: 0.0618\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.716 - 0s 950us/step - loss: 0.7136 - val_loss: 0.2102\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.811 - ETA: 0s - loss: 0.637 - 0s 903us/step - loss: 0.6510 - val_loss: 0.4287\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.601 - 0s 950us/step - loss: 0.5868 - val_loss: 0.5859\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.415 - 0s 855us/step - loss: 0.5691 - val_loss: 0.7025\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.569 - 0s 874us/step - loss: 0.5579 - val_loss: 0.8008\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.493 - ETA: 0s - loss: 0.635 - 0s 864us/step - loss: 0.5543 - val_loss: 0.7962\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.516 - 0s 874us/step - loss: 0.5246 - val_loss: 0.8517\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.532 - 0s 893us/step - loss: 0.5281 - val_loss: 0.8336\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.535 - 0s 959us/step - loss: 0.5410 - val_loss: 0.8301\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.446 - 0s 959us/step - loss: 0.5162 - val_loss: 0.7736\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.490 - 0s 912us/step - loss: 0.5295 - val_loss: 0.8293\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.813 - ETA: 0s - loss: 0.580 - 0s 903us/step - loss: 0.5226 - val_loss: 0.8803\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.496 - 0s 931us/step - loss: 0.5288 - val_loss: 0.8342\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.877 - ETA: 0s - loss: 0.555 - ETA: 0s - loss: 0.542 - 0s 1ms/step - loss: 0.5352 - val_loss: 0.7634\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.427 - ETA: 0s - loss: 0.541 - 0s 997us/step - loss: 0.5238 - val_loss: 0.7925\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.555 - ETA: 0s - loss: 0.508 - 0s 874us/step - loss: 0.5153 - val_loss: 0.8130\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.518 - 0s 902us/step - loss: 0.5137 - val_loss: 0.7829\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.308 - ETA: 0s - loss: 0.496 - 0s 1ms/step - loss: 0.5157 - val_loss: 0.8011\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.519 - ETA: 0s - loss: 0.575 - 0s 1ms/step - loss: 0.5143 - val_loss: 0.7789\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.505 - ETA: 0s - loss: 0.547 - 0s 902us/step - loss: 0.5006 - val_loss: 0.7231\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.627 - ETA: 0s - loss: 0.543 - 0s 921us/step - loss: 0.5373 - val_loss: 0.8829\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.467 - 0s 864us/step - loss: 0.5146 - val_loss: 0.7631\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.596 - 0s 912us/step - loss: 0.5115 - val_loss: 0.7816\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.491 - 0s 874us/step - loss: 0.5145 - val_loss: 0.8852\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.611 - 0s 950us/step - loss: 0.5137 - val_loss: 0.8357\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.447 - 0s 912us/step - loss: 0.4969 - val_loss: 0.8650\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.707 - ETA: 0s - loss: 0.397 - 0s 874us/step - loss: 0.5105 - val_loss: 0.8177\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.195 - ETA: 0s - loss: 0.563 - 0s 997us/step - loss: 0.4988 - val_loss: 0.8246\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.539 - 0s 940us/step - loss: 0.5141 - val_loss: 0.7870\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.308 - ETA: 0s - loss: 0.479 - 0s 864us/step - loss: 0.4950 - val_loss: 0.8009\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 19\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.27 - ETA: 0s - loss: 0.6696 - 1s 9ms/step - loss: 0.8879 - val_loss: 0.0446\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.994 - ETA: 0s - loss: 0.869 - 0s 836us/step - loss: 0.7409 - val_loss: 0.2086\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.710 - 0s 855us/step - loss: 0.6320 - val_loss: 0.3244\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.646 - ETA: 0s - loss: 0.682 - 0s 855us/step - loss: 0.5937 - val_loss: 0.5252\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.461 - 0s 893us/step - loss: 0.5363 - val_loss: 0.6674\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.429 - 0s 921us/step - loss: 0.5603 - val_loss: 0.7171\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.605 - 0s 884us/step - loss: 0.5232 - val_loss: 1.0294\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.572 - ETA: 0s - loss: 0.504 - 0s 1ms/step - loss: 0.5009 - val_loss: 1.0771\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.930 - ETA: 0s - loss: 0.574 - 0s 912us/step - loss: 0.5149 - val_loss: 1.0085\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.581 - ETA: 0s - loss: 0.576 - 0s 846us/step - loss: 0.5318 - val_loss: 0.9710\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.562 - ETA: 0s - loss: 0.526 - 0s 893us/step - loss: 0.5169 - val_loss: 0.9963\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.480 - ETA: 0s - loss: 0.451 - 0s 789us/step - loss: 0.5086 - val_loss: 0.8871\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.503 - 0s 874us/step - loss: 0.5107 - val_loss: 0.9873\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.488 - 0s 883us/step - loss: 0.5130 - val_loss: 0.9954\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.541 - 0s 874us/step - loss: 0.4955 - val_loss: 0.9578\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.501 - 0s 855us/step - loss: 0.5206 - val_loss: 0.9203\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.506 - 0s 864us/step - loss: 0.5061 - val_loss: 0.8768\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.718 - ETA: 0s - loss: 0.459 - 0s 921us/step - loss: 0.4862 - val_loss: 0.9169\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.403 - ETA: 0s - loss: 0.515 - 0s 893us/step - loss: 0.4997 - val_loss: 1.0930\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.187 - ETA: 0s - loss: 0.439 - 0s 845us/step - loss: 0.5018 - val_loss: 0.9813\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.489 - 0s 883us/step - loss: 0.4818 - val_loss: 1.1270\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.496 - 0s 874us/step - loss: 0.5066 - val_loss: 1.1334\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.528 - 0s 845us/step - loss: 0.5038 - val_loss: 1.0282\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.753 - ETA: 0s - loss: 0.552 - 0s 855us/step - loss: 0.5112 - val_loss: 0.9414\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.071 - ETA: 0s - loss: 0.530 - 0s 883us/step - loss: 0.4857 - val_loss: 0.9641\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.095 - ETA: 0s - loss: 0.474 - 0s 845us/step - loss: 0.4894 - val_loss: 0.8675\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.432 - 0s 912us/step - loss: 0.4968 - val_loss: 0.9060\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.521 - 0s 921us/step - loss: 0.5006 - val_loss: 0.9316\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.551 - ETA: 0s - loss: 0.536 - 0s 912us/step - loss: 0.4980 - val_loss: 0.8111\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.540 - 0s 950us/step - loss: 0.4972 - val_loss: 0.8990\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.719 - ETA: 0s - loss: 0.479 - 0s 912us/step - loss: 0.4954 - val_loss: 0.9464\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 20\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.13 - ETA: 0s - loss: 1.1855 - 1s 9ms/step - loss: 0.9723 - val_loss: 0.0450\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.827 - 0s 864us/step - loss: 0.8784 - val_loss: 0.1878\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.506 - ETA: 0s - loss: 0.935 - 0s 865us/step - loss: 0.8176 - val_loss: 0.3596\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.657 - 0s 855us/step - loss: 0.7791 - val_loss: 0.4775\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.620 - 0s 893us/step - loss: 0.7638 - val_loss: 0.5739\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.574 - ETA: 0s - loss: 0.606 - 0s 988us/step - loss: 0.7393 - val_loss: 0.6990\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.290 - ETA: 0s - loss: 0.772 - 0s 969us/step - loss: 0.7317 - val_loss: 0.6969\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.699 - ETA: 0s - loss: 0.795 - 0s 902us/step - loss: 0.7059 - val_loss: 0.7319\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.625 - 0s 941us/step - loss: 0.7056 - val_loss: 0.7934\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.684 - ETA: 0s - loss: 0.732 - 0s 893us/step - loss: 0.7052 - val_loss: 0.7797\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.703 - ETA: 0s - loss: 0.646 - 0s 912us/step - loss: 0.7010 - val_loss: 0.8880\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.444 - ETA: 0s - loss: 0.467 - 0s 931us/step - loss: 0.6774 - val_loss: 0.8257\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.568 - 0s 950us/step - loss: 0.6929 - val_loss: 0.8826\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.339 - ETA: 0s - loss: 0.695 - 0s 912us/step - loss: 0.6744 - val_loss: 0.9584\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.827 - 0s 931us/step - loss: 0.6737 - val_loss: 0.8779\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.774 - 0s 1ms/step - loss: 0.6815 - val_loss: 0.9441\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.868 - 0s 902us/step - loss: 0.6648 - val_loss: 0.9879\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.953 - ETA: 0s - loss: 0.897 - 0s 950us/step - loss: 0.6608 - val_loss: 1.0618\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.678 - 0s 855us/step - loss: 0.6695 - val_loss: 1.0842\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.464 - ETA: 0s - loss: 0.798 - 0s 845us/step - loss: 0.6765 - val_loss: 1.0631\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.785 - 0s 921us/step - loss: 0.6602 - val_loss: 1.1301\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.327 - ETA: 0s - loss: 0.698 - 0s 922us/step - loss: 0.6528 - val_loss: 1.1321\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.551 - 0s 893us/step - loss: 0.6450 - val_loss: 1.1478\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.618 - 0s 902us/step - loss: 0.6613 - val_loss: 1.2184\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.505 - ETA: 0s - loss: 0.527 - 0s 940us/step - loss: 0.6653 - val_loss: 1.1009\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.732 - 0s 902us/step - loss: 0.6535 - val_loss: 1.2027\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.657 - ETA: 0s - loss: 0.753 - 0s 940us/step - loss: 0.6641 - val_loss: 1.2821\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.453 - ETA: 0s - loss: 0.729 - 0s 921us/step - loss: 0.6565 - val_loss: 1.2707\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.741 - 0s 883us/step - loss: 0.6572 - val_loss: 1.2574\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.635 - ETA: 0s - loss: 0.416 - 0s 883us/step - loss: 0.6554 - val_loss: 1.3652\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.632 - 0s 864us/step - loss: 0.6422 - val_loss: 1.3640\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 21\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.74 - ETA: 0s - loss: 1.0167 - 1s 9ms/step - loss: 0.9404 - val_loss: 0.0714\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.951 - ETA: 0s - loss: 0.878 - 0s 836us/step - loss: 0.8036 - val_loss: 0.2927\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 0.640 - 0s 893us/step - loss: 0.7088 - val_loss: 0.5706\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.642 - ETA: 0s - loss: 0.625 - 0s 988us/step - loss: 0.6556 - val_loss: 0.8519\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.466 - ETA: 0s - loss: 0.687 - 0s 874us/step - loss: 0.6414 - val_loss: 1.0115\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.555 - ETA: 0s - loss: 0.559 - 0s 902us/step - loss: 0.6285 - val_loss: 1.0929\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.548 - 0s 912us/step - loss: 0.6281 - val_loss: 1.2575\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.025 - ETA: 0s - loss: 0.641 - 0s 884us/step - loss: 0.6226 - val_loss: 1.1221\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.994 - ETA: 0s - loss: 0.671 - 0s 902us/step - loss: 0.6256 - val_loss: 1.1701\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.662 - ETA: 0s - loss: 0.633 - 0s 893us/step - loss: 0.6229 - val_loss: 1.1152\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.541 - 0s 893us/step - loss: 0.6111 - val_loss: 1.0886\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.527 - ETA: 0s - loss: 0.620 - 0s 893us/step - loss: 0.6293 - val_loss: 1.0615\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.783 - ETA: 0s - loss: 0.557 - 0s 884us/step - loss: 0.6203 - val_loss: 1.0166\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.404 - ETA: 0s - loss: 0.882 - ETA: 0s - loss: 0.714 - ETA: 0s - loss: 0.586 - 0s 2ms/step - loss: 0.6132 - val_loss: 1.0526\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.075 - ETA: 0s - loss: 0.518 - 0s 959us/step - loss: 0.6029 - val_loss: 1.0596\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.759 - ETA: 0s - loss: 0.540 - 0s 931us/step - loss: 0.6046 - val_loss: 1.0775\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.709 - 0s 921us/step - loss: 0.6080 - val_loss: 0.9588\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.453 - ETA: 0s - loss: 0.684 - 0s 921us/step - loss: 0.6083 - val_loss: 1.1556\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.516 - 0s 940us/step - loss: 0.6021 - val_loss: 1.0919\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.596 - ETA: 0s - loss: 0.558 - 0s 940us/step - loss: 0.5870 - val_loss: 1.0213\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.570 - 0s 883us/step - loss: 0.5957 - val_loss: 1.0521\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.587 - 0s 902us/step - loss: 0.5903 - val_loss: 1.0438\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.601 - 0s 988us/step - loss: 0.5979 - val_loss: 1.1963\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.310 - ETA: 0s - loss: 0.659 - 0s 941us/step - loss: 0.6057 - val_loss: 1.0068\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.434 - ETA: 0s - loss: 0.484 - 0s 960us/step - loss: 0.6004 - val_loss: 0.9973\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.504 - ETA: 0s - loss: 0.659 - 0s 931us/step - loss: 0.6061 - val_loss: 0.8479\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.006 - ETA: 0s - loss: 0.484 - 0s 893us/step - loss: 0.5973 - val_loss: 0.9493\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.708 - ETA: 0s - loss: 0.500 - 0s 931us/step - loss: 0.6041 - val_loss: 0.8984\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.536 - 0s 950us/step - loss: 0.5930 - val_loss: 0.9175\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.021 - ETA: 0s - loss: 0.648 - 0s 836us/step - loss: 0.6070 - val_loss: 1.0471\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.801 - ETA: 0s - loss: 0.588 - 0s 931us/step - loss: 0.5880 - val_loss: 1.0061\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 22\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 1.32 - ETA: 0s - loss: 0.4458 - 1s 9ms/step - loss: 0.8588 - val_loss: 0.0525\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.821 - ETA: 0s - loss: 0.743 - 0s 845us/step - loss: 0.7521 - val_loss: 0.1721\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.464 - 0s 883us/step - loss: 0.7028 - val_loss: 0.2834\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.771 - 0s 826us/step - loss: 0.6714 - val_loss: 0.4410\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.419 - ETA: 0s - loss: 0.747 - 0s 893us/step - loss: 0.6484 - val_loss: 0.4851\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.743 - 0s 855us/step - loss: 0.6505 - val_loss: 0.5919\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.607 - ETA: 0s - loss: 0.659 - 0s 855us/step - loss: 0.6591 - val_loss: 0.6304\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 0.695 - 0s 921us/step - loss: 0.6055 - val_loss: 0.6865\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.707 - 0s 836us/step - loss: 0.6295 - val_loss: 0.5759\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.720 - 0s 997us/step - loss: 0.6146 - val_loss: 0.6357\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.326 - 0s 874us/step - loss: 0.6429 - val_loss: 0.5963\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.736 - 0s 959us/step - loss: 0.6270 - val_loss: 0.6958\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.433 - ETA: 0s - loss: 0.715 - 0s 893us/step - loss: 0.6109 - val_loss: 0.6824\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.431 - 0s 950us/step - loss: 0.6185 - val_loss: 0.6385\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.367 - ETA: 0s - loss: 0.318 - 0s 902us/step - loss: 0.6022 - val_loss: 0.7072\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.795 - 0s 931us/step - loss: 0.6181 - val_loss: 0.7476\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.384 - 0s 845us/step - loss: 0.6254 - val_loss: 0.6347\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.626 - 0s 902us/step - loss: 0.6033 - val_loss: 0.6990\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.308 - ETA: 0s - loss: 0.401 - 0s 940us/step - loss: 0.5996 - val_loss: 0.5915\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.331 - 0s 1ms/step - loss: 0.5941 - val_loss: 0.6753\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.269 - 0s 1ms/step - loss: 0.5987 - val_loss: 0.7381\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.244 - ETA: 0s - loss: 0.475 - 0s 902us/step - loss: 0.5926 - val_loss: 0.7388\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.370 - 0s 912us/step - loss: 0.6199 - val_loss: 0.6475\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.739 - 0s 845us/step - loss: 0.5725 - val_loss: 0.6865\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.617 - 0s 883us/step - loss: 0.6107 - val_loss: 0.6763\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.302 - 0s 893us/step - loss: 0.5899 - val_loss: 0.5913\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.689 - 0s 902us/step - loss: 0.6036 - val_loss: 0.6248\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.752 - ETA: 0s - loss: 0.326 - 0s 883us/step - loss: 0.5839 - val_loss: 0.6030\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.343 - 0s 864us/step - loss: 0.6094 - val_loss: 0.6527\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.743 - ETA: 0s - loss: 0.263 - 0s 940us/step - loss: 0.5694 - val_loss: 0.6822\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.667 - 0s 978us/step - loss: 0.5906 - val_loss: 0.6053\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 23\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.29 - ETA: 0s - loss: 0.8291 - 1s 9ms/step - loss: 0.9100 - val_loss: 0.0350\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.713 - 0s 997us/step - loss: 0.7761 - val_loss: 0.1351\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.716 - 0s 883us/step - loss: 0.6614 - val_loss: 0.3016\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.143 - ETA: 0s - loss: 0.542 - 0s 921us/step - loss: 0.5790 - val_loss: 0.4876\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.487 - 0s 940us/step - loss: 0.5212 - val_loss: 0.6701\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.427 - ETA: 0s - loss: 0.578 - 0s 940us/step - loss: 0.4927 - val_loss: 0.8282\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.388 - 0s 941us/step - loss: 0.4915 - val_loss: 0.9836\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.469 - 0s 978us/step - loss: 0.4536 - val_loss: 1.1047\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.504 - ETA: 0s - loss: 0.508 - 0s 931us/step - loss: 0.4524 - val_loss: 1.1057\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.473 - 0s 921us/step - loss: 0.4497 - val_loss: 1.0908\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.293 - 0s 940us/step - loss: 0.4520 - val_loss: 1.1390\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.425 - 0s 912us/step - loss: 0.4522 - val_loss: 1.0557\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.497 - 0s 893us/step - loss: 0.4596 - val_loss: 1.1896\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.484 - 0s 931us/step - loss: 0.4548 - val_loss: 1.1730\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.830 - ETA: 0s - loss: 0.372 - 0s 884us/step - loss: 0.4414 - val_loss: 1.1538\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.343 - 0s 864us/step - loss: 0.4446 - val_loss: 1.1761\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.442 - 0s 874us/step - loss: 0.4398 - val_loss: 1.2162\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.425 - 0s 1ms/step - loss: 0.4303 - val_loss: 1.1394\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.301 - 0s 931us/step - loss: 0.4453 - val_loss: 1.0595\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.229 - 0s 902us/step - loss: 0.4418 - val_loss: 1.0654\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.618 - ETA: 0s - loss: 0.339 - 0s 893us/step - loss: 0.4492 - val_loss: 1.0893\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.937 - ETA: 0s - loss: 0.512 - 0s 921us/step - loss: 0.4432 - val_loss: 1.1012\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.655 - ETA: 0s - loss: 0.472 - 0s 845us/step - loss: 0.4374 - val_loss: 1.1319\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.444 - 0s 855us/step - loss: 0.4313 - val_loss: 1.1764\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.417 - 0s 893us/step - loss: 0.4377 - val_loss: 1.0875\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.558 - ETA: 0s - loss: 0.301 - 0s 884us/step - loss: 0.4521 - val_loss: 1.0143\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.665 - 0s 1ms/step - loss: 0.4360 - val_loss: 1.0529\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.447 - 0s 826us/step - loss: 0.4354 - val_loss: 1.0720\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.354 - ETA: 0s - loss: 0.458 - 0s 874us/step - loss: 0.4299 - val_loss: 1.1854\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.503 - 0s 798us/step - loss: 0.4415 - val_loss: 1.0796\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.429 - 0s 845us/step - loss: 0.4390 - val_loss: 1.1111\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 24\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 1.47 - ETA: 0s - loss: 1.0909 - 1s 10ms/step - loss: 0.9176 - val_loss: 0.0900\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 1.081 - 0s 940us/step - loss: 0.8111 - val_loss: 0.2674\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.645 - 0s 836us/step - loss: 0.7381 - val_loss: 0.4500\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.607 - 0s 893us/step - loss: 0.6750 - val_loss: 0.7059\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.464 - ETA: 0s - loss: 0.550 - 0s 931us/step - loss: 0.6452 - val_loss: 1.0028\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.730 - 0s 950us/step - loss: 0.6306 - val_loss: 1.2835\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.602 - 0s 902us/step - loss: 0.6288 - val_loss: 1.3843\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.342 - ETA: 0s - loss: 0.571 - 0s 940us/step - loss: 0.6131 - val_loss: 1.4158\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.805 - ETA: 0s - loss: 0.567 - 0s 969us/step - loss: 0.6276 - val_loss: 1.5239\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.368 - 0s 921us/step - loss: 0.5936 - val_loss: 1.6494\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.387 - ETA: 0s - loss: 0.719 - 0s 893us/step - loss: 0.5954 - val_loss: 1.8502\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.615 - ETA: 0s - loss: 0.550 - 0s 902us/step - loss: 0.6097 - val_loss: 1.7918\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.506 - 0s 921us/step - loss: 0.6062 - val_loss: 1.7491\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.447 - ETA: 0s - loss: 0.350 - 0s 1ms/step - loss: 0.5910 - val_loss: 1.9625\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.572 - 0s 940us/step - loss: 0.5794 - val_loss: 1.8848\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.548 - 0s 912us/step - loss: 0.5912 - val_loss: 1.9933\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.427 - ETA: 0s - loss: 0.629 - 0s 959us/step - loss: 0.5730 - val_loss: 2.0136\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.726 - ETA: 0s - loss: 0.685 - 0s 931us/step - loss: 0.5723 - val_loss: 2.1761\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.575 - ETA: 0s - loss: 0.750 - 0s 950us/step - loss: 0.5897 - val_loss: 2.0391\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.691 - 0s 883us/step - loss: 0.5890 - val_loss: 1.9382\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.869 - ETA: 0s - loss: 0.547 - 0s 864us/step - loss: 0.5688 - val_loss: 1.6590\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.371 - 0s 883us/step - loss: 0.5762 - val_loss: 2.0363\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.449 - 0s 959us/step - loss: 0.5984 - val_loss: 1.7807\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.461 - ETA: 0s - loss: 0.403 - 0s 988us/step - loss: 0.5795 - val_loss: 2.0420\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.612 - ETA: 0s - loss: 0.692 - 0s 931us/step - loss: 0.5712 - val_loss: 2.4338\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.569 - 0s 931us/step - loss: 0.5507 - val_loss: 2.4712\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.667 - 0s 902us/step - loss: 0.5563 - val_loss: 2.4150\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.698 - 0s 874us/step - loss: 0.5813 - val_loss: 2.2612\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.920 - ETA: 0s - loss: 0.587 - 0s 950us/step - loss: 0.5458 - val_loss: 2.5624\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.444 - ETA: 0s - loss: 0.501 - 0s 940us/step - loss: 0.5345 - val_loss: 2.3917\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.557 - 0s 959us/step - loss: 0.5444 - val_loss: 2.5553\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 25\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.30 - ETA: 0s - loss: 0.6977 - 1s 9ms/step - loss: 0.9158 - val_loss: 0.0394\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.052 - ETA: 0s - loss: 0.864 - 0s 883us/step - loss: 0.8297 - val_loss: 0.1441\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.734 - 0s 883us/step - loss: 0.7551 - val_loss: 0.2799\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.843 - 0s 874us/step - loss: 0.6986 - val_loss: 0.3959\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.808 - 0s 836us/step - loss: 0.6874 - val_loss: 0.5293\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.814 - 0s 845us/step - loss: 0.6337 - val_loss: 0.5680\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.594 - 0s 893us/step - loss: 0.6148 - val_loss: 0.6864\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.633 - 0s 883us/step - loss: 0.5805 - val_loss: 0.6788\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.585 - 0s 855us/step - loss: 0.5730 - val_loss: 0.8860\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.035 - ETA: 0s - loss: 0.727 - 0s 883us/step - loss: 0.5601 - val_loss: 0.9757\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.670 - 0s 864us/step - loss: 0.5619 - val_loss: 1.0378\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.426 - 0s 874us/step - loss: 0.5543 - val_loss: 1.0680\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.691 - 0s 950us/step - loss: 0.5285 - val_loss: 1.1317\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.559 - 0s 921us/step - loss: 0.5169 - val_loss: 1.1381\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.558 - 0s 931us/step - loss: 0.5378 - val_loss: 1.1856\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.652 - 0s 874us/step - loss: 0.5344 - val_loss: 1.2047\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.244 - 0s 921us/step - loss: 0.5120 - val_loss: 1.2161\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.607 - 0s 893us/step - loss: 0.5203 - val_loss: 1.3019\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.542 - 0s 883us/step - loss: 0.5159 - val_loss: 1.0940\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.598 - 0s 826us/step - loss: 0.5099 - val_loss: 1.2374\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.399 - 0s 883us/step - loss: 0.4837 - val_loss: 1.2271\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.692 - ETA: 0s - loss: 0.523 - 0s 1ms/step - loss: 0.4995 - val_loss: 1.4135\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.625 - 0s 969us/step - loss: 0.5019 - val_loss: 1.3662\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.458 - 0s 902us/step - loss: 0.4823 - val_loss: 1.2708\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.632 - 0s 855us/step - loss: 0.4818 - val_loss: 1.3398\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.593 - 0s 883us/step - loss: 0.4966 - val_loss: 1.3538\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.406 - ETA: 0s - loss: 0.249 - 0s 912us/step - loss: 0.4676 - val_loss: 1.2031\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.563 - 0s 931us/step - loss: 0.4808 - val_loss: 1.3132\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.604 - 0s 902us/step - loss: 0.4765 - val_loss: 1.3122\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.663 - ETA: 0s - loss: 0.585 - 0s 836us/step - loss: 0.4846 - val_loss: 1.3101\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.354 - 0s 883us/step - loss: 0.4844 - val_loss: 1.3090\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 26\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.78 - ETA: 0s - loss: 0.6526 - 1s 10ms/step - loss: 0.9398 - val_loss: 0.0550\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.768 - 0s 950us/step - loss: 0.8151 - val_loss: 0.2340\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.753 - 0s 864us/step - loss: 0.7079 - val_loss: 0.5124\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.664 - 0s 893us/step - loss: 0.6506 - val_loss: 0.8883\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.708 - ETA: 0s - loss: 0.697 - 0s 874us/step - loss: 0.6158 - val_loss: 1.3112\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.444 - 0s 845us/step - loss: 0.5826 - val_loss: 1.3795\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.628 - 0s 836us/step - loss: 0.5914 - val_loss: 1.4399\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.832 - ETA: 0s - loss: 0.581 - 0s 893us/step - loss: 0.5980 - val_loss: 1.3219\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.602 - 0s 959us/step - loss: 0.5735 - val_loss: 1.6176\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.574 - 0s 884us/step - loss: 0.5791 - val_loss: 1.7450\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.542 - 0s 902us/step - loss: 0.5939 - val_loss: 1.7826\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.606 - 0s 902us/step - loss: 0.5588 - val_loss: 1.7724\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.352 - ETA: 0s - loss: 0.611 - 0s 883us/step - loss: 0.5798 - val_loss: 1.8912\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.538 - 0s 902us/step - loss: 0.5645 - val_loss: 1.9363\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.622 - ETA: 0s - loss: 0.620 - 0s 950us/step - loss: 0.5659 - val_loss: 1.9026\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.559 - 0s 845us/step - loss: 0.5571 - val_loss: 2.0380\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.472 - 0s 864us/step - loss: 0.5612 - val_loss: 2.0957\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.349 - ETA: 0s - loss: 0.464 - 0s 864us/step - loss: 0.5640 - val_loss: 2.0369\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.282 - ETA: 0s - loss: 0.549 - 0s 1ms/step - loss: 0.5623 - val_loss: 2.1637\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.554 - 0s 817us/step - loss: 0.5553 - val_loss: 2.1728\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.088 - ETA: 0s - loss: 0.610 - 0s 883us/step - loss: 0.5769 - val_loss: 2.1710\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.605 - 0s 826us/step - loss: 0.5525 - val_loss: 2.0703\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.542 - 0s 798us/step - loss: 0.5882 - val_loss: 2.3381\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.722 - ETA: 0s - loss: 0.628 - 0s 893us/step - loss: 0.5574 - val_loss: 2.2081\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.561 - 0s 940us/step - loss: 0.5606 - val_loss: 2.0968\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.553 - 0s 845us/step - loss: 0.5590 - val_loss: 2.1124\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.322 - ETA: 0s - loss: 0.611 - 0s 798us/step - loss: 0.5642 - val_loss: 2.3465\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.613 - 0s 865us/step - loss: 0.5552 - val_loss: 2.1396\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.575 - 0s 902us/step - loss: 0.5562 - val_loss: 2.1260\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.638 - ETA: 0s - loss: 0.481 - 0s 826us/step - loss: 0.5461 - val_loss: 2.3712\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.453 - 0s 921us/step - loss: 0.5674 - val_loss: 2.4162\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 27\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 1.87 - ETA: 0s - loss: 0.8383 - 1s 10ms/step - loss: 0.8887 - val_loss: 0.0944\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.747 - ETA: 0s - loss: 0.805 - 0s 921us/step - loss: 0.8048 - val_loss: 0.2134\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.728 - ETA: 0s - loss: 0.644 - 0s 893us/step - loss: 0.7693 - val_loss: 0.3645\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.666 - 0s 893us/step - loss: 0.7459 - val_loss: 0.3520\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.693 - 0s 874us/step - loss: 0.7272 - val_loss: 0.3706\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.817 - 0s 883us/step - loss: 0.7204 - val_loss: 0.3286\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.111 - ETA: 0s - loss: 0.513 - 0s 950us/step - loss: 0.7158 - val_loss: 0.3877\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.779 - ETA: 0s - loss: 0.786 - 0s 931us/step - loss: 0.7175 - val_loss: 0.3301\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.139 - ETA: 0s - loss: 0.720 - 0s 874us/step - loss: 0.7205 - val_loss: 0.3300\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.829 - ETA: 0s - loss: 0.642 - 0s 921us/step - loss: 0.7093 - val_loss: 0.3357\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.734 - 0s 921us/step - loss: 0.7029 - val_loss: 0.2975\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.737 - ETA: 0s - loss: 0.683 - 0s 931us/step - loss: 0.7036 - val_loss: 0.2776\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.545 - 0s 903us/step - loss: 0.6983 - val_loss: 0.2666\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.516 - ETA: 0s - loss: 0.710 - 0s 940us/step - loss: 0.7028 - val_loss: 0.2985\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.866 - 0s 931us/step - loss: 0.7020 - val_loss: 0.2746\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.693 - 0s 836us/step - loss: 0.6942 - val_loss: 0.2501\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.484 - ETA: 0s - loss: 0.603 - 0s 931us/step - loss: 0.7021 - val_loss: 0.2568\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.225 - ETA: 0s - loss: 0.760 - 0s 931us/step - loss: 0.6988 - val_loss: 0.2336\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.754 - 0s 921us/step - loss: 0.6906 - val_loss: 0.2250\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.702 - 0s 950us/step - loss: 0.6931 - val_loss: 0.2165\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.641 - ETA: 0s - loss: 0.686 - 0s 883us/step - loss: 0.7002 - val_loss: 0.1920\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.985 - ETA: 0s - loss: 0.514 - 0s 893us/step - loss: 0.6914 - val_loss: 0.1908\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.823 - ETA: 0s - loss: 0.673 - 0s 921us/step - loss: 0.6826 - val_loss: 0.2309\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.417 - ETA: 0s - loss: 0.663 - 0s 950us/step - loss: 0.6830 - val_loss: 0.2277\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.738 - 0s 903us/step - loss: 0.6875 - val_loss: 0.2083\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.950 - ETA: 0s - loss: 0.610 - 0s 864us/step - loss: 0.6897 - val_loss: 0.1957\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.705 - 0s 921us/step - loss: 0.6802 - val_loss: 0.1964\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.768 - 0s 1ms/step - loss: 0.6744 - val_loss: 0.1879\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.504 - ETA: 0s - loss: 0.779 - 0s 883us/step - loss: 0.6939 - val_loss: 0.1920\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.093 - ETA: 0s - loss: 0.695 - 0s 922us/step - loss: 0.6784 - val_loss: 0.1885\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 2.178 - ETA: 0s - loss: 0.821 - 0s 950us/step - loss: 0.6857 - val_loss: 0.1829\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 28\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 1.01 - ETA: 0s - loss: 0.9028 - 1s 9ms/step - loss: 0.9229 - val_loss: 0.0596\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.911 - 0s 941us/step - loss: 0.7838 - val_loss: 0.2190\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.747 - 0s 864us/step - loss: 0.7094 - val_loss: 0.4541\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 0.640 - 0s 883us/step - loss: 0.6606 - val_loss: 0.6912\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.635 - 0s 969us/step - loss: 0.6278 - val_loss: 0.9757\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.683 - 0s 1ms/step - loss: 0.6276 - val_loss: 1.1466\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.948 - ETA: 0s - loss: 0.676 - 0s 941us/step - loss: 0.6202 - val_loss: 1.2827\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.684 - 0s 912us/step - loss: 0.6031 - val_loss: 1.3733\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.710 - 0s 940us/step - loss: 0.5862 - val_loss: 1.4416\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.672 - 0s 950us/step - loss: 0.6085 - val_loss: 1.5718\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.560 - 0s 931us/step - loss: 0.5807 - val_loss: 1.5758\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.519 - 0s 902us/step - loss: 0.6122 - val_loss: 1.3470\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.703 - ETA: 0s - loss: 0.507 - 0s 978us/step - loss: 0.6037 - val_loss: 1.5355\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.920 - ETA: 0s - loss: 0.562 - 0s 978us/step - loss: 0.5888 - val_loss: 1.7603\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.178 - ETA: 0s - loss: 0.509 - 0s 902us/step - loss: 0.5925 - val_loss: 1.7032\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.516 - 0s 931us/step - loss: 0.5780 - val_loss: 1.7786\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.692 - ETA: 0s - loss: 0.702 - 0s 902us/step - loss: 0.6039 - val_loss: 1.8698\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.688 - 0s 902us/step - loss: 0.5885 - val_loss: 1.6891\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.174 - ETA: 0s - loss: 0.637 - 0s 950us/step - loss: 0.5956 - val_loss: 1.7607\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.654 - 0s 931us/step - loss: 0.5882 - val_loss: 1.8839\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.562 - 0s 921us/step - loss: 0.5825 - val_loss: 1.8586\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.529 - 0s 912us/step - loss: 0.5744 - val_loss: 1.8102\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.319 - ETA: 0s - loss: 0.575 - 0s 941us/step - loss: 0.5878 - val_loss: 1.7778\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.723 - ETA: 0s - loss: 0.579 - 0s 997us/step - loss: 0.5939 - val_loss: 2.0099\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.528 - 0s 959us/step - loss: 0.5885 - val_loss: 1.9145\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.968 - ETA: 0s - loss: 0.448 - 0s 940us/step - loss: 0.5739 - val_loss: 1.9327\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.572 - 0s 1ms/step - loss: 0.5946 - val_loss: 2.3403\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.690 - 0s 1ms/step - loss: 0.5778 - val_loss: 2.1101\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.545 - 0s 940us/step - loss: 0.5956 - val_loss: 2.2910\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.505 - 0s 978us/step - loss: 0.5893 - val_loss: 2.2902\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.506 - 0s 931us/step - loss: 0.5827 - val_loss: 2.2280\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 29\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.53 - ETA: 0s - loss: 1.0580 - 1s 9ms/step - loss: 0.9107 - val_loss: 0.0653\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.599 - ETA: 0s - loss: 0.676 - 0s 912us/step - loss: 0.7397 - val_loss: 0.2528\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.673 - 0s 921us/step - loss: 0.6325 - val_loss: 0.6515\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.401 - ETA: 0s - loss: 0.578 - 0s 931us/step - loss: 0.5633 - val_loss: 0.9596\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.369 - ETA: 0s - loss: 0.529 - 0s 940us/step - loss: 0.5329 - val_loss: 1.2507\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.482 - 0s 893us/step - loss: 0.5201 - val_loss: 1.4014\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.481 - ETA: 0s - loss: 0.551 - 0s 950us/step - loss: 0.5210 - val_loss: 1.4029\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.412 - ETA: 0s - loss: 0.606 - 0s 950us/step - loss: 0.5131 - val_loss: 1.4131\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.822 - ETA: 0s - loss: 0.535 - 0s 883us/step - loss: 0.5033 - val_loss: 1.2179\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.416 - ETA: 0s - loss: 0.469 - 0s 950us/step - loss: 0.5104 - val_loss: 1.2982\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.452 - ETA: 0s - loss: 0.433 - 0s 1ms/step - loss: 0.5094 - val_loss: 1.2089\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.486 - 0s 940us/step - loss: 0.4978 - val_loss: 1.3405\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.466 - 0s 959us/step - loss: 0.5028 - val_loss: 1.3071\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.305 - 0s 845us/step - loss: 0.5078 - val_loss: 1.2028\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.589 - ETA: 0s - loss: 0.410 - 0s 845us/step - loss: 0.4914 - val_loss: 1.3172\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.509 - 0s 883us/step - loss: 0.5070 - val_loss: 1.4920\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.105 - ETA: 0s - loss: 0.576 - 0s 931us/step - loss: 0.4990 - val_loss: 1.2804\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.537 - ETA: 0s - loss: 0.499 - 0s 912us/step - loss: 0.5007 - val_loss: 1.3149\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.417 - 0s 845us/step - loss: 0.4985 - val_loss: 1.2272\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.486 - ETA: 0s - loss: 0.464 - 0s 921us/step - loss: 0.4954 - val_loss: 1.3673\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.135 - ETA: 0s - loss: 0.579 - 0s 922us/step - loss: 0.4931 - val_loss: 1.3657\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.390 - 0s 931us/step - loss: 0.5032 - val_loss: 1.2853\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.446 - 0s 978us/step - loss: 0.4975 - val_loss: 1.3497\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.964 - ETA: 0s - loss: 0.460 - ETA: 0s - loss: 0.441 - 0s 1ms/step - loss: 0.4984 - val_loss: 1.3677\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.404 - ETA: 0s - loss: 0.633 - ETA: 0s - loss: 0.524 - 0s 2ms/step - loss: 0.5076 - val_loss: 1.2796\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.766 - ETA: 0s - loss: 0.524 - 0s 921us/step - loss: 0.4948 - val_loss: 1.3950\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.551 - 0s 883us/step - loss: 0.4992 - val_loss: 1.2840\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.419 - 0s 874us/step - loss: 0.4920 - val_loss: 1.3292\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.305 - ETA: 0s - loss: 0.451 - 0s 940us/step - loss: 0.4878 - val_loss: 1.4822\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.562 - 0s 1ms/step - loss: 0.4950 - val_loss: 1.4185\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.463 - 0s 931us/step - loss: 0.4924 - val_loss: 1.3506\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 30\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 1.32 - ETA: 0s - loss: 0.9316 - 1s 10ms/step - loss: 0.9589 - val_loss: 0.0409\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.347 - ETA: 0s - loss: 0.875 - 0s 950us/step - loss: 0.8251 - val_loss: 0.1422\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.806 - 0s 874us/step - loss: 0.7470 - val_loss: 0.2600\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.558 - ETA: 0s - loss: 0.668 - 0s 893us/step - loss: 0.7079 - val_loss: 0.3480\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.765 - ETA: 0s - loss: 0.758 - 0s 893us/step - loss: 0.6823 - val_loss: 0.4582\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.765 - 0s 931us/step - loss: 0.6749 - val_loss: 0.5367\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.655 - 0s 940us/step - loss: 0.6642 - val_loss: 0.5521\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.448 - ETA: 0s - loss: 0.712 - 0s 902us/step - loss: 0.6635 - val_loss: 0.5567\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.354 - ETA: 0s - loss: 0.659 - 0s 874us/step - loss: 0.6516 - val_loss: 0.5537\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.398 - ETA: 0s - loss: 0.720 - 0s 902us/step - loss: 0.6462 - val_loss: 0.5423\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.505 - 0s 950us/step - loss: 0.6492 - val_loss: 0.5251\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.523 - ETA: 0s - loss: 0.628 - 0s 940us/step - loss: 0.6603 - val_loss: 0.5668\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.902 - ETA: 0s - loss: 0.734 - 0s 893us/step - loss: 0.6554 - val_loss: 0.5436\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.583 - 0s 921us/step - loss: 0.6443 - val_loss: 0.4944\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.766 - 0s 912us/step - loss: 0.6352 - val_loss: 0.5115\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.412 - ETA: 0s - loss: 0.617 - 0s 1ms/step - loss: 0.6328 - val_loss: 0.5141\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.708 - ETA: 0s - loss: 0.576 - 0s 1ms/step - loss: 0.6400 - val_loss: 0.4885\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.366 - ETA: 0s - loss: 0.675 - 0s 950us/step - loss: 0.6317 - val_loss: 0.4555\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.786 - ETA: 0s - loss: 0.575 - 0s 940us/step - loss: 0.6314 - val_loss: 0.4553\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.873 - ETA: 0s - loss: 0.720 - 0s 931us/step - loss: 0.6291 - val_loss: 0.4529\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.780 - ETA: 0s - loss: 0.587 - 0s 874us/step - loss: 0.6175 - val_loss: 0.4474\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.626 - 0s 922us/step - loss: 0.6194 - val_loss: 0.4825\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.762 - ETA: 0s - loss: 0.613 - 0s 931us/step - loss: 0.6046 - val_loss: 0.4526\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.456 - 0s 931us/step - loss: 0.6140 - val_loss: 0.4475\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.524 - 0s 940us/step - loss: 0.6191 - val_loss: 0.4607\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.498 - ETA: 0s - loss: 0.685 - 0s 988us/step - loss: 0.6347 - val_loss: 0.4859\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.610 - 0s 950us/step - loss: 0.6267 - val_loss: 0.4590\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 2.389 - ETA: 0s - loss: 0.691 - 0s 959us/step - loss: 0.6164 - val_loss: 0.4860\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.860 - ETA: 0s - loss: 0.471 - 0s 950us/step - loss: 0.6236 - val_loss: 0.4631\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.665 - 0s 931us/step - loss: 0.6187 - val_loss: 0.4848\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.963 - ETA: 0s - loss: 0.688 - 0s 883us/step - loss: 0.6217 - val_loss: 0.4665\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 31\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.29 - ETA: 0s - loss: 1.0820 - 1s 10ms/step - loss: 0.8946 - val_loss: 0.0631\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.741 - 0s 874us/step - loss: 0.7091 - val_loss: 0.1842\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.559 - ETA: 0s - loss: 0.567 - 0s 855us/step - loss: 0.5614 - val_loss: 0.3424\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.580 - 0s 950us/step - loss: 0.4774 - val_loss: 0.5759\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.530 - ETA: 0s - loss: 0.493 - 0s 883us/step - loss: 0.4408 - val_loss: 0.7011\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.309 - 0s 864us/step - loss: 0.4168 - val_loss: 0.7130\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.452 - 0s 874us/step - loss: 0.4226 - val_loss: 0.6763\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.350 - 0s 941us/step - loss: 0.4140 - val_loss: 0.7123\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.446 - 0s 912us/step - loss: 0.4030 - val_loss: 0.7808\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.441 - 0s 883us/step - loss: 0.4050 - val_loss: 0.7545\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.414 - 0s 827us/step - loss: 0.4017 - val_loss: 0.6715\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.447 - 0s 836us/step - loss: 0.4043 - val_loss: 0.6767\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.410 - 0s 1ms/step - loss: 0.4094 - val_loss: 0.6674\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.486 - 0s 1ms/step - loss: 0.4087 - val_loss: 0.7237\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.331 - 0s 950us/step - loss: 0.3975 - val_loss: 0.6463\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.439 - 0s 940us/step - loss: 0.4118 - val_loss: 0.6129\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.378 - 0s 950us/step - loss: 0.4089 - val_loss: 0.6345\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.466 - 0s 950us/step - loss: 0.4035 - val_loss: 0.6504\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.470 - 0s 941us/step - loss: 0.4248 - val_loss: 0.6531\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.453 - 0s 893us/step - loss: 0.4050 - val_loss: 0.6234\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.435 - 0s 864us/step - loss: 0.4057 - val_loss: 0.6773\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.366 - ETA: 0s - loss: 0.381 - 0s 921us/step - loss: 0.4071 - val_loss: 0.6346\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.742 - ETA: 0s - loss: 0.330 - 0s 1ms/step - loss: 0.4137 - val_loss: 0.6578\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.297 - 0s 969us/step - loss: 0.3988 - val_loss: 0.6679\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.241 - 0s 893us/step - loss: 0.4104 - val_loss: 0.6343\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.334 - 0s 941us/step - loss: 0.4019 - val_loss: 0.6815\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.354 - 0s 874us/step - loss: 0.3945 - val_loss: 0.6606\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.441 - 0s 950us/step - loss: 0.3984 - val_loss: 0.7181\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.929 - ETA: 0s - loss: 0.469 - 0s 940us/step - loss: 0.4100 - val_loss: 0.6856\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.463 - 0s 969us/step - loss: 0.3906 - val_loss: 0.5568\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.449 - ETA: 0s - loss: 0.445 - 0s 988us/step - loss: 0.3985 - val_loss: 0.5719\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 32\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.33 - ETA: 0s - loss: 1.1998 - 1s 10ms/step - loss: 0.9461 - val_loss: 0.0208\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.900 - 0s 931us/step - loss: 0.7712 - val_loss: 0.1054\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.434 - ETA: 0s - loss: 0.688 - 0s 922us/step - loss: 0.6640 - val_loss: 0.2206\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.597 - ETA: 0s - loss: 0.743 - 0s 950us/step - loss: 0.5852 - val_loss: 0.3852\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 4.648 - ETA: 0s - loss: 0.649 - 0s 998us/step - loss: 0.5517 - val_loss: 0.5176\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.571 - 0s 912us/step - loss: 0.5258 - val_loss: 0.6522\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.392 - 0s 1ms/step - loss: 0.5238 - val_loss: 0.5756\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.340 - ETA: 0s - loss: 0.409 - 0s 988us/step - loss: 0.5222 - val_loss: 0.6679\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.611 - 0s 931us/step - loss: 0.5171 - val_loss: 0.7681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.660 - 0s 988us/step - loss: 0.5022 - val_loss: 0.7902\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.934 - ETA: 0s - loss: 0.562 - 0s 874us/step - loss: 0.4953 - val_loss: 0.7560\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.394 - ETA: 0s - loss: 0.370 - 0s 921us/step - loss: 0.5162 - val_loss: 0.6923\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.581 - 0s 921us/step - loss: 0.5010 - val_loss: 0.7652\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.277 - ETA: 0s - loss: 0.353 - 0s 959us/step - loss: 0.4891 - val_loss: 0.6636\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.473 - 0s 884us/step - loss: 0.4960 - val_loss: 0.8062\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.575 - 0s 912us/step - loss: 0.4919 - val_loss: 0.7820\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.357 - 0s 921us/step - loss: 0.4971 - val_loss: 0.7430\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.558 - 0s 931us/step - loss: 0.5035 - val_loss: 0.7711\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.621 - ETA: 0s - loss: 0.390 - 0s 1ms/step - loss: 0.5186 - val_loss: 0.7090\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.529 - 0s 931us/step - loss: 0.5047 - val_loss: 0.8353\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.414 - 0s 893us/step - loss: 0.4946 - val_loss: 0.6691\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.500 - 0s 912us/step - loss: 0.4911 - val_loss: 0.7246\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.357 - ETA: 0s - loss: 0.553 - 0s 940us/step - loss: 0.5067 - val_loss: 0.7538\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.376 - 0s 921us/step - loss: 0.5181 - val_loss: 0.6444\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.350 - 0s 931us/step - loss: 0.4821 - val_loss: 0.7651\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.585 - 0s 893us/step - loss: 0.4805 - val_loss: 0.7697\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.478 - 0s 2ms/step - loss: 0.5120 - val_loss: 0.8343\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.654 - 0s 997us/step - loss: 0.4904 - val_loss: 0.6829\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.795 - ETA: 0s - loss: 0.523 - 0s 959us/step - loss: 0.4830 - val_loss: 0.7197\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.623 - 0s 931us/step - loss: 0.5001 - val_loss: 0.7657\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.558 - ETA: 0s - loss: 0.378 - 0s 959us/step - loss: 0.4820 - val_loss: 0.7523\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 33\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.19 - ETA: 0s - loss: 1.4460 - 1s 10ms/step - loss: 0.9871 - val_loss: 0.0271\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 1.272 - 0s 921us/step - loss: 0.9137 - val_loss: 0.0850\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.882 - 0s 893us/step - loss: 0.8690 - val_loss: 0.1607\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.429 - 0s 893us/step - loss: 0.8435 - val_loss: 0.2660\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 1.221 - 0s 893us/step - loss: 0.8233 - val_loss: 0.4131\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.188 - ETA: 0s - loss: 0.862 - 0s 969us/step - loss: 0.7777 - val_loss: 0.5386\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 5.470 - ETA: 0s - loss: 0.445 - 0s 1ms/step - loss: 0.7585 - val_loss: 0.6531\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.100 - 0s 1ms/step - loss: 0.7456 - val_loss: 0.8046\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.417 - 0s 921us/step - loss: 0.7248 - val_loss: 0.9161\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 10.84 - ETA: 0s - loss: 1.0272 - ETA: 0s - loss: 0.770 - 0s 1ms/step - loss: 0.7116 - val_loss: 1.2474\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.713 - 0s 3ms/step - loss: 0.6834 - val_loss: 1.2849\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.767 - 0s 2ms/step - loss: 0.6832 - val_loss: 1.5226\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.138 - 0s 902us/step - loss: 0.6454 - val_loss: 1.7410\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 8.733 - ETA: 0s - loss: 0.792 - ETA: 0s - loss: 0.687 - 0s 1ms/step - loss: 0.6401 - val_loss: 2.1387\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 1.064 - ETA: 0s - loss: 0.759 - 0s 2ms/step - loss: 0.6602 - val_loss: 2.2783\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.787 - 0s 921us/step - loss: 0.6350 - val_loss: 2.3729\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.988 - 0s 988us/step - loss: 0.6677 - val_loss: 2.5832\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.527 - 0s 902us/step - loss: 0.6065 - val_loss: 2.7567\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.192 - 0s 2ms/step - loss: 0.6216 - val_loss: 2.7203\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.393 - 0s 969us/step - loss: 0.6082 - val_loss: 3.0347\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.556 - ETA: 0s - loss: 0.755 - 0s 979us/step - loss: 0.6009 - val_loss: 3.3353\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 3.529 - ETA: 0s - loss: 0.801 - 0s 988us/step - loss: 0.6184 - val_loss: 3.4538\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.805 - 0s 902us/step - loss: 0.6517 - val_loss: 3.3842\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.621 - 0s 1ms/step - loss: 0.5889 - val_loss: 3.4335\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.394 - 0s 950us/step - loss: 0.5994 - val_loss: 3.3325\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.185 - 0s 950us/step - loss: 0.5957 - val_loss: 3.5334\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.702 - 0s 874us/step - loss: 0.5619 - val_loss: 3.9626\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.555 - 0s 893us/step - loss: 0.5592 - val_loss: 3.7877\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.209 - 0s 874us/step - loss: 0.5784 - val_loss: 3.6042\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.346 - 0s 817us/step - loss: 0.5348 - val_loss: 4.6076\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.460 - ETA: 0s - loss: 0.981 - ETA: 0s - loss: 0.651 - 0s 1ms/step - loss: 0.5693 - val_loss: 4.5540\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 34\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.86 - ETA: 0s - loss: 0.9597 - 1s 10ms/step - loss: 0.8932 - val_loss: 0.0489\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.463 - ETA: 0s - loss: 0.892 - 0s 883us/step - loss: 0.8046 - val_loss: 0.1476\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.599 - ETA: 0s - loss: 0.600 - 0s 874us/step - loss: 0.7348 - val_loss: 0.2212\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.585 - ETA: 0s - loss: 0.534 - 0s 884us/step - loss: 0.6972 - val_loss: 0.3034\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.636 - 0s 874us/step - loss: 0.6732 - val_loss: 0.4164\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.426 - ETA: 0s - loss: 0.627 - 0s 912us/step - loss: 0.6642 - val_loss: 0.4425\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.468 - 0s 902us/step - loss: 0.6401 - val_loss: 0.5152\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.684 - 0s 978us/step - loss: 0.6211 - val_loss: 0.6413\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.071 - ETA: 0s - loss: 0.745 - 0s 921us/step - loss: 0.6147 - val_loss: 0.6619\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.503 - ETA: 0s - loss: 0.685 - 0s 1ms/step - loss: 0.6085 - val_loss: 0.7415\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.678 - 0s 921us/step - loss: 0.6107 - val_loss: 0.7444\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.740 - 0s 940us/step - loss: 0.6075 - val_loss: 0.7717\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.734 - ETA: 0s - loss: 0.686 - 0s 874us/step - loss: 0.5926 - val_loss: 0.7846\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.649 - ETA: 0s - loss: 0.695 - 0s 874us/step - loss: 0.6028 - val_loss: 0.8015\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.485 - 0s 902us/step - loss: 0.5748 - val_loss: 0.8855\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.596 - 0s 902us/step - loss: 0.5929 - val_loss: 0.9195\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.689 - 0s 922us/step - loss: 0.5841 - val_loss: 0.8865\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.512 - ETA: 0s - loss: 0.720 - 0s 883us/step - loss: 0.6140 - val_loss: 0.8186\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.838 - ETA: 0s - loss: 0.541 - 0s 950us/step - loss: 0.6028 - val_loss: 0.7629\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.598 - 0s 912us/step - loss: 0.6014 - val_loss: 0.8046\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.542 - 0s 931us/step - loss: 0.5822 - val_loss: 0.8016\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.042 - ETA: 0s - loss: 0.591 - 0s 874us/step - loss: 0.5903 - val_loss: 0.8908\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.608 - 0s 883us/step - loss: 0.5790 - val_loss: 0.8571\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.646 - 0s 893us/step - loss: 0.5740 - val_loss: 0.8639\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.509 - 0s 893us/step - loss: 0.5767 - val_loss: 0.8462\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.549 - 0s 883us/step - loss: 0.5767 - val_loss: 0.8960\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.622 - 0s 940us/step - loss: 0.5791 - val_loss: 0.8864\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.729 - ETA: 0s - loss: 0.511 - 0s 959us/step - loss: 0.5727 - val_loss: 0.8322\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.633 - 0s 950us/step - loss: 0.5776 - val_loss: 0.8595\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.603 - 0s 969us/step - loss: 0.5678 - val_loss: 0.8665\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.553 - 0s 902us/step - loss: 0.5749 - val_loss: 0.8672\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 35\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 10.656 - ETA: 0s - loss: 0.9662  - 1s 10ms/step - loss: 0.9683 - val_loss: 0.0687\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.631 - 0s 921us/step - loss: 0.9340 - val_loss: 0.1687\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 6.303 - ETA: 0s - loss: 1.233 - 0s 893us/step - loss: 0.9059 - val_loss: 0.2182\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 1.211 - 0s 950us/step - loss: 0.9040 - val_loss: 0.2611\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.391 - ETA: 0s - loss: 0.937 - 0s 1ms/step - loss: 0.9055 - val_loss: 0.2149\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.894 - 0s 940us/step - loss: 0.8991 - val_loss: 0.1857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.458 - ETA: 0s - loss: 0.644 - 0s 921us/step - loss: 0.9000 - val_loss: 0.2016\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.877 - 0s 931us/step - loss: 0.8916 - val_loss: 0.1387\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.387 - ETA: 0s - loss: 0.245 - 0s 1ms/step - loss: 0.8866 - val_loss: 0.1366\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.294 - 0s 931us/step - loss: 0.8866 - val_loss: 0.1573\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.442 - ETA: 0s - loss: 1.121 - 0s 902us/step - loss: 0.8877 - val_loss: 0.1432\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.827 - 0s 873us/step - loss: 0.8882 - val_loss: 0.1320\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 1.124 - 0s 978us/step - loss: 0.8843 - val_loss: 0.1486\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.042 - ETA: 0s - loss: 1.096 - 0s 1ms/step - loss: 0.8756 - val_loss: 0.1379\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 1.116 - 0s 874us/step - loss: 0.8855 - val_loss: 0.1382\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 1.134 - 0s 893us/step - loss: 0.8835 - val_loss: 0.1337\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.562 - 0s 940us/step - loss: 0.8823 - val_loss: 0.1280\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.658 - ETA: 0s - loss: 0.567 - 0s 941us/step - loss: 0.8844 - val_loss: 0.1275\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.279 - 0s 883us/step - loss: 0.8841 - val_loss: 0.1300\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 1.091 - 0s 893us/step - loss: 0.8791 - val_loss: 0.1334\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.186 - 0s 874us/step - loss: 0.8704 - val_loss: 0.1330\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.617 - 0s 921us/step - loss: 0.8734 - val_loss: 0.1452\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.799 - 0s 884us/step - loss: 0.8720 - val_loss: 0.1328\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 1.061 - 0s 997us/step - loss: 0.8744 - val_loss: 0.1459\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 1.137 - 0s 883us/step - loss: 0.8747 - val_loss: 0.1467\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 1.050 - 0s 893us/step - loss: 0.8698 - val_loss: 0.1443\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.276 - 0s 931us/step - loss: 0.8644 - val_loss: 0.1499\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.286 - 0s 940us/step - loss: 0.8670 - val_loss: 0.1738\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 1.152 - 0s 874us/step - loss: 0.8781 - val_loss: 0.1723\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.841 - 0s 941us/step - loss: 0.8687 - val_loss: 0.1658\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.336 - 0s 893us/step - loss: 0.8757 - val_loss: 0.1674\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 36\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.11 - ETA: 0s - loss: 0.3170 - 1s 10ms/step - loss: 0.9558 - val_loss: 0.0513\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 1.134 - 0s 921us/step - loss: 0.8934 - val_loss: 0.1627\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 1.086 - 0s 883us/step - loss: 0.8482 - val_loss: 0.2692\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 1.157 - 0s 893us/step - loss: 0.8266 - val_loss: 0.4131\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 1.092 - 0s 940us/step - loss: 0.8154 - val_loss: 0.4877\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.205 - 0s 921us/step - loss: 0.7990 - val_loss: 0.4982\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.993 - 0s 855us/step - loss: 0.7705 - val_loss: 0.6168\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.996 - 0s 912us/step - loss: 0.7494 - val_loss: 0.6081\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.154 - 0s 940us/step - loss: 0.7553 - val_loss: 0.6522\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.990 - 0s 960us/step - loss: 0.7410 - val_loss: 0.8077\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.211 - 0s 1ms/step - loss: 0.7606 - val_loss: 0.7933\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 1.149 - 0s 960us/step - loss: 0.7317 - val_loss: 0.8770\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.955 - 0s 864us/step - loss: 0.7353 - val_loss: 0.8769\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.953 - 0s 921us/step - loss: 0.7317 - val_loss: 0.8344\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.968 - 0s 893us/step - loss: 0.7292 - val_loss: 0.8794\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.944 - 0s 902us/step - loss: 0.7171 - val_loss: 0.7604\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.861 - 0s 931us/step - loss: 0.7045 - val_loss: 0.8348\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.213 - 0s 912us/step - loss: 0.6973 - val_loss: 0.8119\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.168 - 0s 950us/step - loss: 0.6949 - val_loss: 0.8641\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.866 - 0s 969us/step - loss: 0.6886 - val_loss: 1.0019\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.924 - 0s 931us/step - loss: 0.6899 - val_loss: 1.0112\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.559 - ETA: 0s - loss: 0.246 - 0s 931us/step - loss: 0.6736 - val_loss: 0.8646\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.930 - 0s 902us/step - loss: 0.6991 - val_loss: 1.0141\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.215 - 0s 921us/step - loss: 0.6863 - val_loss: 0.9845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.875 - 0s 826us/step - loss: 0.6788 - val_loss: 1.0715\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 11.60 - ETA: 0s - loss: 0.8866 - 0s 883us/step - loss: 0.6641 - val_loss: 1.0458\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.191 - 0s 817us/step - loss: 0.6745 - val_loss: 0.9228\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.836 - 0s 864us/step - loss: 0.6800 - val_loss: 1.1553\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.990 - 0s 902us/step - loss: 0.6923 - val_loss: 1.0581\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 1.018 - 0s 1ms/step - loss: 0.6594 - val_loss: 1.0512\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.847 - 0s 883us/step - loss: 0.6417 - val_loss: 1.1213\n",
      "Epoch 00031: early stopping\n",
      "(105,) (15,)\n",
      "model compiled 37\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,701\n",
      "Trainable params: 41,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 105 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - ETA: 19s - loss: 0.89 - ETA: 0s - loss: 0.7691 - 1s 10ms/step - loss: 0.9087 - val_loss: 0.0431\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.695 - ETA: 0s - loss: 0.708 - 0s 950us/step - loss: 0.7620 - val_loss: 0.2087\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.707 - 0s 902us/step - loss: 0.6639 - val_loss: 0.4444\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.658 - 0s 902us/step - loss: 0.5920 - val_loss: 0.7200\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.595 - 0s 959us/step - loss: 0.5635 - val_loss: 0.9743\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.522 - ETA: 0s - loss: 0.505 - 0s 883us/step - loss: 0.5350 - val_loss: 1.2501\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.917 - ETA: 0s - loss: 0.534 - 0s 874us/step - loss: 0.5392 - val_loss: 1.3719\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.478 - 0s 988us/step - loss: 0.5119 - val_loss: 1.3747\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.393 - ETA: 0s - loss: 0.597 - 0s 902us/step - loss: 0.5291 - val_loss: 1.6598\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.574 - 0s 931us/step - loss: 0.5203 - val_loss: 1.5888\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.559 - 0s 1ms/step - loss: 0.5046 - val_loss: 1.7975\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.756 - ETA: 0s - loss: 0.522 - 0s 959us/step - loss: 0.5050 - val_loss: 1.7059\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.423 - 0s 921us/step - loss: 0.5025 - val_loss: 1.8089\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.987 - ETA: 0s - loss: 0.518 - 0s 969us/step - loss: 0.4966 - val_loss: 1.7651\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.474 - 0s 931us/step - loss: 0.5142 - val_loss: 1.9250\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.872 - ETA: 0s - loss: 0.412 - 0s 969us/step - loss: 0.5079 - val_loss: 1.7795\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.433 - 0s 931us/step - loss: 0.5037 - val_loss: 1.8482\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.567 - ETA: 0s - loss: 0.608 - 0s 912us/step - loss: 0.5116 - val_loss: 1.9667\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.433 - 0s 874us/step - loss: 0.5312 - val_loss: 1.9324\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.938 - ETA: 0s - loss: 0.607 - 0s 959us/step - loss: 0.5033 - val_loss: 2.0154\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.278 - ETA: 0s - loss: 0.407 - 0s 988us/step - loss: 0.4962 - val_loss: 1.8832\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.415 - 0s 883us/step - loss: 0.4941 - val_loss: 1.9470\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.698 - ETA: 0s - loss: 0.520 - 0s 940us/step - loss: 0.4991 - val_loss: 2.0161\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.085 - ETA: 0s - loss: 0.442 - 0s 902us/step - loss: 0.5131 - val_loss: 2.0340\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.035 - ETA: 0s - loss: 0.555 - 0s 931us/step - loss: 0.5212 - val_loss: 1.9561\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.327 - ETA: 0s - loss: 0.494 - 0s 1ms/step - loss: 0.4990 - val_loss: 1.9453\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.527 - 0s 1ms/step - loss: 0.5056 - val_loss: 1.9699\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.593 - ETA: 0s - loss: 0.513 - 0s 969us/step - loss: 0.5114 - val_loss: 2.0213\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.422 - 0s 960us/step - loss: 0.5060 - val_loss: 1.9916\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.710 - ETA: 0s - loss: 0.462 - 0s 931us/step - loss: 0.4995 - val_loss: 2.2544\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.473 - 0s 940us/step - loss: 0.4882 - val_loss: 2.0947\n",
      "Epoch 00031: early stopping\n",
      "MSE: 889.391622\n",
      "RMSE: 29.822670\n",
      "MAE: 21.447358\n",
      "MAPE: 25.661680\n",
      "\n",
      "Quantile 1, between 39.99999999999999 and 77.5\n",
      "MSE: 508.965255\n",
      "RMSE: 22.560258\n",
      "MAE: 15.844559\n",
      "MAPE: 29.494504\n",
      "\n",
      "Quantile 2, between 77.5 and 87.5\n",
      "MSE: 1080.115300\n",
      "RMSE: 32.865108\n",
      "MAE: 22.880584\n",
      "MAPE: 27.717324\n",
      "\n",
      "Quantile 3, between 87.5 and 98.00000000000001\n",
      "MSE: 602.658348\n",
      "RMSE: 24.549101\n",
      "MAE: 19.602713\n",
      "MAPE: 20.993765\n",
      "\n",
      "Quantile 4, between 98.00000000000001 and 130.00000000000003\n",
      "MSE: 1356.226626\n",
      "RMSE: 36.826982\n",
      "MAE: 27.420436\n",
      "MAPE: 24.179901\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAAF1CAYAAAAJAjeKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOydd3hU1daH35WEBEKAhDQSgoQOAkoLioWigiC9KaGjNLkgiF4VwYYgxftdiigCKr2LJSCIhCLoBUGaUqVLSQgQIAlpJLO/P2YS0zNzBkiG7Pd5zsPMOWedvc8M+Z01a6+9tiil0Gg0Go1Go9FoihJOBd0BjUaj0Wg0Go3mXqOdYI1Go9FoNBpNkUM7wRqNRqPRaDSaIod2gjUajUaj0Wg0RQ7tBGs0Go1Go9FoihzaCdZoNBqNRqPRFDm0E+yAiEhzEblwj9paICIT7kVbdwNH779Go3FsCkqDRCRYRJSIuNzrtu8Ejt5/jWOgneC7iIicFZEEEYkTkUiLGHrchXaGi8jvIpIkIguyHEsTkrgM2zt3ug/3ChHxEZFfReSaiNwQkZ0i8nhB9ysvRGSLFnONpvCSRasvi8j8u6HVObQbICJhInLJohHBWY4vEJHkLPrtfLf7dbcQkVEiclpEYiz3PK0w66KINLN8LzqQcp+ineC7T3ullAdQD6gPjLkLbVwCJgBf5XGOp1LKw7J9eBf6cK+IA14EfAEvYAqwtrAKqYj0Agpl3zQaTSbStLoBEAKMy3rCXdAZE/Aj0DWPc6Zm0G4PpVTqHe7DvWQt0EApVRqoAzwMvFKwXcoZESkGzAB+K+i+aO4e2gm+RyilIoGNmJ1hAETkURH5nyWieVBEmmc4NkBEjopIrOWX85A8rv2NUuo74Jq9/RSR+iKyz9LuSqB4hmNeIrJORK6IyHXL6yDLse4isjfLtV4Tke8sr58TkSOW614UkdeN9E8plaiUOq6UMgECpGJ2hsvmYeYlIj9Y2v5NRKpk6GNNEdkkItEiclxEnrfsr2LZ18DyPlBErmb8jvJDRMoA7wFv5HCspYgcE5GbIjJLRH4WkYHWXluj0dwdlFIXgQ2YnTQskcB/icgJ4IRlXzsROWDR7v+JyENp9nlpaA5tXVZKfQbssbffIuIsIv+x6NRpoG2W47k+U0TkkIi0z/C+mOU69USkuIgsyTD6tkdE/I30USl1Sil1I60ZzD8CquZj1ktE/rb0Z2yGPjqJyFsicsrSt1UiUtZybLaIfJ3h3CkisllExIbuvgb8BBzLuFNESlgi9Nctz7R/yz1KT9TcebQTfI+wOIttgJOW9+WBHzBHcMsCrwNrRMTXYhIFtANKAwOAaWkOmUHOicgFMQ/z+eTSR1fgO2CxpU+ryRyhcALmAxWBB4AEYJblWBhQSURqZTi/t+VaAF8CQ5RSpTA/XLbYcS+IyB9AoqXdL5RSUXmcHgp8gNlZPglMtFyjJLAJWAb4Wc77TERqK6VOAW8CS0XE3XLfC5RS22zo5kfAbCAyS999gDWYI00+wCmgUKd0aDRFBRGpADwH7M+wuxPwCPCgRYe/AoYA3sAcIExE3KzQUCMMs/wg3ysieV1rEOZnRn2gEdAty/G8nimLMOt1Gs8BEUqpA0A/oAxQAfP9DsWs/YYQkZ4iEgNcxRwJnpOPyRNADeBp4N0Mz5hXMH8vzYBA4DrwqeXYa8BDItJfRJ4EXgL6KaWUlX2siHnEcXwOh98Dqli2ZzF/PhpHRSmlt7u0AWcxD9/HAgrYjDktAcwO1uIs52/E/Iea07W+A0ZaXjcHLuRwzgTMjlrGfR6YBdEF8Ae+Bjbm0kZTzKkVkmHf/4AJuZxfD7ie4f1sYKLldW3MouRmef835odG6Tv4+RbH7Ljm+JlZzlmA2UlOe/8ccMzy+gVgR5bz5wDvZXgfBvwJ/JF2L1b2rRFwwPK5B1u+fxfLsb7ArgznCnABGFjQ/2f1preiuGXQ6hvAOeAzoITlmAKeynDubODDLPbHMTtjNmlohnNcLO0EZ9nfALPj6WLRrljg8VyusQUYmuF9q4y6k8P5GZ8pgZZrl7a8/xp4w/L6Rcs9PHSHP/NqwIdAuVyOp+lmUIZ9u4EeltdHgaczHAsAbmfQ2cZAtOX7DLWxb98DL1heL8j4/QGngdYZ3g8mh+ex3hxj05Hgu08nZY5+NgdqYo78gTma2t0yvHRDRG5g/sUbACAibURklyUCcAOzAOYYwc0LpVScUup3pVSKUuoyMBxoJSKlczg9ELioLH/ZFs6lvRARdxGZIyLnLL/ktwOe8s9EjYVAT8uQUx9glVIqyXKsq+UezlmG/pvk1F8ROSz/TAB5Mp97S1RKLQfeEpGH8zg1YyQ2HvMPAzB/B49k+Q56AeUynD8Pc+T6kwz3krXPvTL0eYOIOGF+iI5USqXkYBIInM9wHyrje41GUyB0Ukp5KqUqKqWGKaUyRjsz/n1WBF7LohsVMP9d56mhtqKU2qeUumbR7/XAUqBLLqdn0pWs7eb1TFFKXQJ+BbqKiCfmUculFtPFmAM0K8Q8mW2qmPNlyXL9TDpoxb2dAA5j1sq8yEu/v83wHRzFnB7nb7n+bswOqwCrcrt41meOJS2klFJqZS4meX7OGsdCO8H3CKXUz5h/Uf7Hsus85kiwZ4atpFJqsoi4YR4u/w/gr5TyBNZj/mO2uyuWf3O6VgRQPkve1AMZXr+GeVjqEWWe2NA047WUUruAZOBJoCf/pEKglNqjlOqIOe3gO3IRJaVUbfXPBJAdVt5TMaCyledm5Dzwc5bvwEMp9TKAmGeHT8ecyvF+Wr5ZDn1emqHPbTAPNzYCVopIJP/k+12wOPYRmB+aWNqRjO81Gk2hI6NTex7ziFdG3XC3/CDPT0PvRD9yew5k0pWM7Vr5TFmIOSWiO7BTmXOjUUrdVkp9oJR6EHgMc0pF32wdy66D1uCCOa3ACOeBNlm+h+Jp/RaRfwFumCPz2eZlZOh31mfO00AjMVd0isQ8YjhKRL63mOT6OWscD+0E31umAy1FpB6wBGgvIs+KeUJDcTHX/w0CXDH/8V4BUkSkDeahrRwRERcRKQ44A2nXcrEce0REaoh5EoE3MBPYppS6mcOldgIpwCuWa3bBPKSURinMuWA3LA7hezlcYxHmPOEUpdQvlj64WqIEZZRSt4EYzL/YbUbMkwmfsFyzhIi8ifmXv5EZvOuA6iLSR8wTQYqJSEiGnLMZwF6l1EDM+dufW3ndm5ijBfUs23OW/Q0t/fwBqC0iXSzf0ytkjj5rNJrCyzxgqEVbRURKikhbESlF/hqaDYt2u1neulnepx3rJiIeFv1uhdlJDcvlUqss7QaJiBfwVoZj1jxTvsOcfjESs46n9aGFiNS1jPjFYE45MKrfA0XEz/L6QczVkjYbuRZmPZ4o5vxdRMRXRDpaXlfHnB7YG/Oo5BuW5641vANU5x/9DsP8nQ+wHF8FjBHzRPEgYITB/msKAdoJvocopa5gFpd3lFLngY7A25iF6Tzwb8BJKRWL2TFahTmvtie5Cx+YJ1glYBa93pbXaeV9KmMuwRMLHAKSMOfR5tS/ZMxDbf0t7b4AfJPhlOlACcwTGnZZrpuVxZjTBxZn2d8HOGtJoxhK5kkYtuCGefLDNeAiZgezrWU4zyYsn3MroAfmaEEk5pJrbhYxbW3pK8BooIGYS57ld12llIpM2zB/vwCXlVLJSqmrmKMtky33UQ3zUKRGoynkKKV+xzwJbRZmnTyJWTOt0dCcSMCcjwzmSgQZ0zBGYta5G8DHwCCV++TceZjTFg4C+zK2a80zxZL+sQaolKXP5TDnCMdgTjn4GXMQxwiPA3+KyC3Mkej1mJ+BRpiB+R5+EpFYzM+kRyyBhSXAFKXUQUvaxdvAYktEPE+UUrFZ9DsBuKWUirac8gHmFIgzmKtHZH3WaRwIyZy6pNHYh4iUwDwLuYFFfDRWICLbgCVKqS8Kui8ajaZoIiLvAtWVUkaDFEUOMZfNXKKUCirovmhsRxfx19xpXgb2aAdYo9FoHAdLittLmEftNJoigXaCNXcMETmLeaJFpwLuikaj0WisREQGYU53W6yU2l7Q/dFo7hU6HUKj0Wg0Go1GU+TQE+M0Go1Go9FoNEUO7QRrNBqNRqPRaIocDpUT7OPjo4KDgwu6GxqNJhf27t17VSnla8S2qpRU8cbKjwIQQdJGpVRrwxfQ3HG0Zms0hZuC1GwoeN12KCc4ODiY33//vaC7odFockFEDC8hmkAqL1PRcNvv8pfNy4pr7i5aszWawk1BajYUvG7rdAiNRqPRaDQaTZHDoSLBGo3m/kb/KtdoNBrHwdE1WzvBGo2mUCA4vqBqNBpNUeF+0GztBGs0mkKDowuqRqPRFCUcXbMdvf8ajUaj0Wg0Go3N6EiwRqMpNOhf5RqNRuM4OLpm59t/ESkuIrtF5KCIHBaRD3I4p7+IXBGRA5ZtYIZj/UTkhGXrl2F/QxH5U0ROishMEZE7d1sajcbREDs3jRmt2RqN5l5gr2YXBgGxJhKcBDyllIoTkWLALyKyQSm1K8t5K5VSwzPuEJGywHtAI0ABe0UkTCl1HZgNDAZ2AeuB1sAG+25Ho9E4Mo4eVSgkaM3WaDT3BEfX7Hz7r8zEWd4Ws2zKyus/C2xSSkVbRHQT0FpEAoDSSqmdSikFLAI62d59jUZzP+Fkx6YxozVbo9HcK+zR7MKg21b1QUScReQAEIVZIH/L4bSuIvKHiHwtIhUs+8oD5zOcc8Gyr7zlddb9Go1Go7ETrdkajUaTP1Y5wUqpVKVUPSAIaCwidbKcshYIVko9BIQDCy37c0r5UHnsz4aIDBaR30Xk9ytXrljTXY1G44Ck1Zx01IhCYUJrtkajudvYq9mFQbdt6oNS6gawDXMuWMb915RSSZa384CGltcXgAoZTg0CLln2B+WwP6c25yqlGimlGvn6+trSXY1G42A4spgWRrRmazSau8l97wSLiK+IeFpelwCeAY5lOScgw9sOwFHL641AKxHxEhEvoBWwUSkVAcSKyKOWGcZ9ge/tvhuNxgHYtGkTc+bM4fTp0wXdlUKFjgTfGbRmazR3lkOHDjF79mx27dqFOSVeA/dHJNia6hABwEIRccbc51VKqXUiMh74XSkVBrwiIh2AFCAa6A+glIoWkQ+BPZZrjVdKRVtevwwsAEpgnmGsZxlrHAKTycTXX39NXFwc7dq1w8/Pzyq76Oholi9fTrNmzXj66acJDw8nPDycxx57jDp1so5WF00KgyjeB2jN1miysH37do4cOUKzZs2oVauWVTa3b99m+fLlBAQE8PLLL3Pw4EHmzZtHlSpVeOqpp9BVAh1fs8WRftU0atRI/f777wXdDc19QEJCAitWrMDV1ZUOHTpQqlQpq+zOnDnD2rVr6dq1K76+vvzwww9cu3aNZ599lgoVKuRqFx4ezqVLl+jRoweurq7p+5VS/Prrrxw5coR69erRuHFju++tIBGRvUqpRkZsK0pxNYaKhtt+mb8Mt625O2jN1twplFKEhYURFRXFc889R/ny1s3LjI2NZenSpYSEhNCgQQN27NjBsWPHaNCgAY0a5S4XR44cYevWrfTo0QNvb+9Mx06ePMnWrVsJCAigdevWuLg47rpjBanZUPC6rZ1gTZFj79697Nmzh169egEQFhbG7du3ad++fTaxS8NkMrFmzRqKFStGx44dM0UAUlNT+emnnzh//jzNmjWjRo0a6ceuX7/O8uXLadq0ab7R3v3797Nnzx6qVatG8+bNHTLKYK+gjrVDUIdoJ7jQoTVbcye4dOkSa9asoX379gQFBfHjjz9y8eJFnnrqKapVq5ar3Y4dOzhx4gShoaGUKFEi07Hff/+dffv2UbNmTZ588sl0vb19+zYrVqzAz8+PZ599Ns9+XbhwgY0bN+Lp6Um7du1wc3Oz/2bvMQWp2VDwuq2dYE2BcurUKX788UeqVavGM888g5PT3RtcSUxMZNmyZVStWpWmTZtmO7Z27Vpu3rxJmzZtMkUZzp07x/fff0/Xrl3zjD4opfj555/566+/aNSoETdu3OD8+fOEhoZmiv7mx/Hjxzlw4AAvvPCC7TdZwNgjqMFSXI2zQ1AHaSe40KE1+/7j5s2brFixAk9PTzp06JDNubyTpEV/ExMT6d69e6bng1KKLVu2cOLECZo0acLDDz+cfiwuLo4lS5bQsGFDQkJC8mzj6NGj7NixgwceeIAHHniALVu20KNHD3x8fKzu59WrV/n6668ZOnSo7TdZwBSkZkPB67bjxvA1Do3JZGL16tUUL16cYcOGcfr0ab744gv8/f157rnnKFas2B1tb//+/fz222/07NmT0qVLZztevHhxunfvTkpKChs2bGD9+vW0aNGCAwcO4OzszIgRI/KNzIoIzZs3p3nz5ixZsgQ3Nzf69euXp01O1KhRg927d9tsdz/g6PllGs39zPbt2zl58iT9+vUjPj6eVatW4eTkRIcOHShTpswdbSsiIoKvv/6atm3bUrly5WzHRYSnn36ap556il27djFnzhzq1q2LUopjx47Rr18/qxz0WrVqUatWLc6dO8eCBQt49913bR6F8/HxoWTJkjbZ3C84umZrJ1gDwMaNGzlz5gxPPfUU1atXv6ttnTlzhnXr1tG1a1cCAwMBqFKlClWqVOHSpUssWrQIDw+POxJlSExMZPny5VSuXNmqX+kuLi60b98ek8nEJ598QsOGDXniiSdsbvfpp5/m559/NtJljUajyZc///yTHTt2UKdOnUzD+XeDmJgYli1bRkhICC+++CJgDhz069ePuLg4wsLCSEpKol27dthbFk8pxdq1a0lISGDYsGE4Ozvneb6I0KRJE5o0acLu3btZu3YtH374oc3tVqxYkVKlSjlkGprGONoJLuJcu3aN5cuX89RTT9GqVSt+/vlntm3bRkhICPXr17+jbaVVVXBzc2P48OE5ik1gYCAvvfQS0dHRrFy5EhcXFzp06JBj9NYaVqxYQefOnfH09LTJzsnJiccff5yLFy8aatff35/o6Oj8T9Skk1ZuR6PR5E5ycjLLly+nfPnyDBs2jKNHjzJv3jwqVqxIy5Yt73hK2Y4dO/jrr79yjax6eHjQs2dPkpKSWLduHdHR0bRu3TrPicJ5sW/fPsqWLWso+NCoUSO+/fZbQ+2C+bPVWM/9oNnaCS7C/PTTT0RGRjJkyJD09IO04fzdu3czZ84cHnzwQZ544gm7fx1bm1ebRtmyZenfvz9xcXF8+eWXjBw50lC7JUqUMBxNDggI4H//+58hWycnJ27fvm3Itijj6IKq0dxNDh06xPbt2+nRowdly5YF/hnOP3v2LF999RW+vr53JKUsNjaWZcuW0bBhQ1566aV8z3dzc6Nr166kpKTw3//+l9deey3fKG5OeHl5kZCQYKTLODk52VWpwR4n2N3dnVu3bhW5tAhH12ztBBdBoqOjWbZsGc2bN6dVq1Y5ntO4cWMaN27MoUOHmDdvHsHBwXZNXAsLC7MqrzYrHh4e6WJvBD8/P65cuUJQUFD+J2fB19eXy5cvG247KSkp/5M0mdADkRpNdjLWqx02bFiO5wQHBzNw4EAiIyNZvHgxJUuWtCulbMWKFfTt29dmexcXFx5++GGuXbtmdQ31jPj7+7N//36b7dKwx/m3J3Dh5+fH5cuXc8xfvp9xdM12dCdeYyPh4eGsW7eOwYMHW7VAQ506dRg8eDBVq1Zl1apVhtv19PS0K5pstIqJn58fkZGRhmxtqeiQE/YIqrOzMykpKXa172joFeM0muwcOXKEefPm0a5dO1q2bJnv+eXKlePFF1+kVatWLFiwwHC7xYsXN+xA+/n5ERUVZci2ZMmSxMfHG7IF+3Q7JSUFk8lkyLZcuXKG79lRuRcrxonIVyISJSKHMuyrJyK7ROSAiPwuIo0t+0VEZorISRH5Q0Qa5Hd9/ewoQly8eJGbN2/St29fm4WicuXKBTa8X6pUKWJjYw3Z+vv72yVM9giqPUNrPj4+XLlyxbC9RqO5P9i8eTPDhg2zeUTMy8sLDw8Pw+26urqSmJhoyNae4IO92KPZnp6eXL9+3ZBtWiRYc8dZALTOsm8q8IFSqh7wruU9QBugmmUbDMzO7+LaCS5CeHl5OeS65/Y4smXLlrVrglpBDa15eHgUSSdYR4I1mszYOqn3TmGP7qaloRUE9mh2YGCgYUfW1dXVsAPtyNztSLBSajvmpd0z7QbSZsuXAS5ZXncEFikzuwBPEQnIr/+aIoK7u7vhCQf24uTkRGpqqiFbe6IKTk5Odjn+9gjqrVu3+Pvvv22ySSsPdO7cOWrXrm24bUdEp0NoNHceo/rn7+9v2CEsVqxYgaVzubu7ExcXZ8i2TJkyHDx40Ga7ffv2sXDhQrp06WKoXUflDqVD+FhSGtK2wVY0PQr4WETOA/8Bxlj2lwfOZzjvgmVfruhnh+ae4O3tzbVr1wzZ2pvSYA+JiYmcP38+/xOz2IwdO5bHHnuMxYsXM3HiRE6fPp2vXVRUFLNmzaJmzZqEhoYamlnt6GgnWKO5c5QuXZqYmBhDtvY4wQVJmTJlWLt2rc12M2fO5Ny5c9y8eZN3332XX3/9Nd8fEImJiXz11VfExsYydOhQw6U8HZk74ARfVUo1yrDNtaLZl4FXlVIVgFeBLy37c5p4lOeXqKtDaO4JaRMljMwW9vDw4NatW4baVUpx4cIFEhMTKV68uNV2t27dYvr06QQEBPDjjz9y6tQp+vfvT82aNfO027RpE1u3bmXUqFHp95qSksLMmTOJiYmhW7du2SYkKqXYsGEDN27c4OWXX7arxI+jo51ZjebOkebIGlnNzcvLixs3btyFXuXP1atXuXr1qk1LF5tMJhYsWMDFixepW7cuY8aMoVmzZrRunTWdNDOnT5/m008/pW/fvpmWXl69ejVjx46ladOmPPvss9kmdh84cIBdu3YRGhp6x1fLcyQKSLP7AWl1U1cDX1heXwAyFqgO4p9UiRwpuk9bjc24urqSlJSEm5ubzbb+/v4cO3bMqooUWUlMTOTkyZOkpqbaFB29fPkyq1evplGjRnzwwQdUrFiR3r175ztZZMeOHYSFhfHmm2+mi3BSUhKrV69m/vz5dOnShUceeSSTTVJSEuPHj6dhw4ZMnDgxk2C6uLgwevRoTCYTX3zxBatWraJNmzY0adKEK1eusHLlSlq1anXXV+rTaDSOR1oqmZGRIX9/fy5cuGBIW0TErlSykydPEh8fj7u7u9U2iYmJLFu2jOrVq/Ppp5/i5uZG79698y1xefbsWWbNmkVoaGj6inYdO3Zk8+bNjBs3jtq1axMaGprNbtasWZhMJiZNmpRtQl337t3p3r07mzZtYsyYMYSEhNCxY0dSU1NZvnw5wcHBVq1CqrkrXAKaAduAp4ATlv1hwHARWQE8AtxUSkXkdSHtBGusxp6au+7u7mzbto0WLVrYVGt448aN7Nixg86dOzN//ny8vb1p27ZtnjOAlVKsX7+e2NhYXn75ZZydnWnZsiURERFMmjQJX19f+vbtm222dXx8PDNmzCAwMJCPP/4407E0MU5NTSUsLIy33norfZW9LVu2sGnTJkaOHEm5cuVy7ZeTkxODB5vTnVasWMH3339PjRo1GDp0aJGO/qZxP6w+pNHcaXx8fOyqubtv3z5D7ZpMJo4cOWKzI5sWWe3SpQvffvstqamptG/fHi8vrzzt9u/fz2+//UbPnj0pXbo0bdu2JTExkenTp3P79m169OhBtWrVsvVx0aJFnD9/nqlTp2Z6togIzzzzDM888wy7du3ivffeIzAwkEGDBnH+/HlmzpxJ7969810ZtWXLlrRs2ZI9e/YwduxYvL29GTx4cIFNWCxM3AvNFpHlQHPMucMXgPeAQcAMEXEBEjFXggBYDzwHnATigQH5Xt+RqgU0atRI/f777wXdDYdm8eLF9O7d21DN3t9++41ixYrRoEG+pfcy8b///Y8jR45Qp04dfvzxRzw9PRk+fHiejl9iYiIffvghjRo1olOnTun9vXz5MuvXr8fd3Z327dtnE+eoqChWrlxJ69atswlmGtevX2fGjBm4u7vTq1cvypcvz6+//sq3337LG2+8YdXDRinFpk2b+P7772nRogVdu3a1+TPdvXs3pUuXzjfFwpEQkb1KqUZGbKtKcfV/VDTcdif+yrdtEfkKaAdEKaXqWPZ9DLQHkoFTwACl1A3LsTHAS0Aq8IpSaqPhDhZBtGbbz/79+ylWrJihUbSEhARWrlxJ//79bbI7c+YMa9eu5bHHHmPr1q3Exsby6quv5uvIzpw5ExFhyJAh6YGKhIQE1q5dS1xcHG3btsXf3z+TTVJSEsuWLaNy5co0a9Ysx+umpqYya9YsoqOj6dSpE/Xr1+fcuXPMmjWL7t2707hxY6vu6/DhwyxcuJASJUrw9ttv2zyqmZqayooVK+jVq5dNdoWZgtRssE637yY6/FTEKF26NKdOnaJq1apW26RVLEhISKBbt25W2926dYulS5dSr149Bg4cCMCjjz7KyZMnmTJlCiLC6NGjs+XqhoeHs2XLFkaOHJlNMP39/RkwYAA3btzg66+/RkRo3749ZcqUsTqv1svLi/fff5+EhASmTZtGdHQ0Dz74IP/5z3+svjcRoVWrVvz999+0a9fO0I8Kf39/zp07d185wfZwjyLBC4BZwKIM+zYBY5RSKSIyBfNM4zdF5EGgB1AbCATCRaS6UspYmRONxgDlypXj559/ttkJ3rt3L3v27LHJYTOZTKxZs4ZixYqlr/DZqFEjoqOjWbRoEZcvX+Zf//oX5ctnnnB/9uxZPvnkE/r06UO9evUyHStRogTPP/88ycnJbNiwgaioKFq2bElwcDAHDx5k586d9OjRI8/IqrOzMyNHjsRkMrFw4UIWLVpEmTJlmDx5sk1pIrVr12bQoEH88ssvhtL6nJ2dDS+mcT9yP4zeaSe4iNGuXTs2btzIli1baNGiRa7R0jQuX76cnsNqi+O8c+dODh8+TO/evbNFa6tWrcrYsWO5ePEiM2fO5NatW4wePZoSJUrwwQcf5JhXmxVPT0/69u1LfHw8a9eu5TgKKo0AACAASURBVOjRo4SGhlKjRg2r+5gWDXjvvffS88hsJTAwkOPHj2eaUGEtfn5+/Pbbb4ba1RhDKbVdRIKz7Pspw9tdQNovvY7ACqVUEnBGRE4CjYGd96CrGg0AAQEBVKlShblz51KrVi2eeOKJPLUxLa+2atWqNuWsnjt3jrCwMLp06ZLNyS1btiwjR44kLi6OZcuWcfr0aQYMGECNGjWYNWsWqampOebVZsTV1ZWOHTtiMpnYtGkT8+fPp1mzZjb10cnJiQEDBjBlyhQGDhxoKE/az8+Pixcv2mynuT/RTnARw9nZmeeeew6lFNu2bWPLli088sgj2X69Z8yrHTZsmNViEx8fz9KlS3nooYfSo7+5Ub58ed544w2io6NZsGABhw4d4sMPP8wzrzYr7u7uvPDCC0yePNkmBzgj9ixqUb58ecNOcIkSJUhKSjLc9v1IIYgqvAistLwuj9kpTiPfmpMazd0gJCSEkJAQDh8+zLx58wgODuaZZ57JNr8ia16tNZhMJr755hucnZ0ZPnx4ng62h4cHgwcPJikpiVWrVjFjxgwGDRqUb15tRpycnHj22WfZvn07Tz31lNV2GQkKCuLy5ct4e3vbbFu6dGnD1YY02SkEmm0Xjt5/jUFEhBYtWjBkyBCSkpKYM2dOel3EqKgoPv30U6pXr06PHj2sdoBjYmKYM2cOPXv2zFY9IS/SogzlypWzyQHOiD2ObLFixQyvVV+uXDkuXLhguG1NZgqg6Ho6IjIWSAGWpu3K4TTHmUShue+oXbs2gwcPplq1anz55Zd8//333L59m6SkJObPn09MTIzN9WpnzZrFo48+SufOna1O63Jzc6NPnz5UrFjRJgc4IyJieGn5qlWrGo7miohdSytrMnO3V4y72+hIsIZHHnmERx55hD///JN58+bh4eFhqGKBk5MTlSpVomTJkob6UVBLFKctlVmpUiWbbX18fIrk8sZ3AyFnr9MGrhqdYCEi/TBPmHta/TNb2OaakxrNvaBSpUoMGjSIiIgIFi9ezO3bt3nhhRcMVSzw8vIyVPEHsMuZ9PX15cqVK9lSL6yhVq1arFmzxnDb9jxrNP9wBzS7wNFOsCadunXrUrduXcP2Hh4ehperBPsENTk5GaWUoQlqlSpVIiIiwpAT7OzsXCRXdrtbFERkQERaA28CzZRSGYcEwoBlIvJfzBPjqgG7C6CLGk2OBAQEGJ7PcCewR7ODg4OJjIw05ATbm9JgjxPs7OxMSkqKLmtpoTBEc+3B0fuvKWTYU3LPHmEqU6YM169fN2Rbo0YNu1Ia9NCa42CpObkTqCEiF0TkJczVIkoBm0TkgIh8DqCUOgysAo4APwL/0pUhNJp/8PLyIjIy0pCtvbprNJUC7NNsHx8frl69atheU7jQTrCm0GCPMKVNlDBChQoV7EppMOq8f7tiGWUP/491H43lzwP7Dbd/P3G3c8uUUqFKqQClVDGlVJBS6kulVFWlVAWlVD3LNjTD+ROVUlWUUjWUUhvu2I1qNPcBQUFBHDt2zJBt1apVDTvQYJ8TbFSz//jjINFXj/PH/o1s3fKTXUGf+wVHzwkuDH3QaABztQSjE9SqVavGpUvG0jWdnJwM5xSHh4dz+fJl3nzzTf744w+rbGJiYpg79t/U/3U1rU//QpsDYZT54n3WT3iT3Tu2G+rH/UBazUlHFVONxhEREcO1bwMDAzl16pQhWxcXF7uq4xjV7OPHj3Ps2DHeeustvv/+e6tsTCYTc+d+QmriEV7oWJlWzXypVyuRnzcvZcOG70lNLZoDRPZqdmHQbZ3Uoik0pJUbMzLbuFatWnz33Xc225lMJiZMmEBKSgpjxoxh+PDhVuWo3bx5k+nTp1OrVq30ZT3XrFnD0qVLad++PU888USOdt+vWoHbvp/pc+43XFP/EfHyUacpH3WaqIt/8tPWMFwbNqPZc8YW4XBk7LpdHZTRaGymbNmyXL9+3VC5MX9/f8PBBzAezV24cCEJCQm8+eabdO7cmUcffTRfm9TUVL788ktu3ryZvrLd9u3bGTduHNWrV6dv37452h069CeH/tjGCx2CKVP6n/kfXmVcaP5YWWJvpbBzxwquxxTj2dadilx6nN2PqALWbe0EawoN5cuX58iRIzY7wTExMUyaNInExEScnJx4/vnnrVoN6Pfff2fFihUMHz6c4ODg9BXuTp06xYABA3JdyS08PJyffvqJsWPHUqZMGcA8vNajRw+ef/551q1bx5gxY3j88cdp164dYFk9b9L7PBNzioqRJ3Ltk9+18zx97TzRf+9n6+7NpNRtQqtuL9j0eTgyTmKHImonWKOxGT8/P8M1dz08PAyN3plMJj7++GOioqKYNm0affv2tar9q1evMnnyZDp16kS/fv1ITU1l7dq1vPXWW7Ro0YJnn302R7sTJ04wZ84c+vfvn2nlvWbNmtGsWTP27dvH+++/j4+PD8OGDUuvvzx37ic0etiHFzpWzjUgUaqkC0809iIhMZV9v60h8qqizXNdDa1I54jYpdlQ4LotjpTTotehL/ysWbOG1q1b21wm7dy5c8yYMQN3d3c8PT0ZNWqUVbNvv//+e/bt28eoUaPw8vLi+PHjLFiwgODgYHr16oWHh0c2G5PJxMSJE6lQoQJ9+/bNVnA+rRD8oUOH6NatGyEhIYDZ2Z4xYwbVq1fnhRfydkzTFiMJDw+ndHFXHroVSbO/d+OWYlvk44/gBjw06SuHqUBhzzr0NaS4muv0gOG2m5tOFOga9JrsaM0u/Jw/f56TJ0/SokULm+wSExOZMGECzs7OpKamMmrUKHx8fPK1O3LkCF9++SWDBw+mRo0a3Lx5k2nTpuHu7k6vXr1yHYlbvHgxFy5c4JVXXsn2fFFKER4eztatW3nooYfo0aMHYI7+zp8/n2vXrvHvf/87m9Zn5dixY6xevRqTyUS1yl4817ISnqVt094bN2/zxwlPmjZtZpNdQVGQmg0Fr9vaCdbcMc6fP8+iRYuIj4+ndevW+S7vmcbs2bNJTExk2LBhuLm5cfr0aVasWEFqaiqvvfZatmWXAeLi4pgwYQLNmjWjTZs22Y5fvHiRzz77DD8/P/r06UPZsmUB2LdvH8uXL2fYsGH5lkRLSUkhLCyM3bt388ADD3D27FnGjBmDl5eXlZ+ImVXvvU7XoxttsknjbEAN3N/6hICAAEP29xp7BLWmFFdznY0LarNU7QQXNrRmF25u3brFokWLOHXqFI0bN6ZLly5WBR/Cw8PZsmULI0eOxN/fnxs3brBkyRIuXrzI0KFDqVixYjYbk8nE//3f/1GqVCkGDhyYrZ3ExESmTZtGSkoKoaGhVK1aFYDo6Gg++ugjOnToQNOmTfPt286dO9mwYQOenp5ERETQt29fm0t/rlyxiG5tvXF2tn2sPyVVsWFrAu07dMv/5EJAQWo2FLxuaydYYzdKKb799luUUnTu3BknJye+/fZb9uzZw5NPPknr1q1zdIbPnz/P9OnTCQ0NpVGj7H8DERERLFmyhJiYGF599dV0R3bdunXs2bOHkSNHpu/LjevXrzNt2jQ8PDxISEigQoUK9O/fP9+IQNb7Gzt2LB999JHVNhlZOeUDuu392pBttIcPZ4dOIaRxY0P29xp7BXWeHYLaVDvBhQ6t2YWXXbt28eeff9KrVy/c3d3Zt28fq1atombNmvTo0YPixYtns0lOTmb8+PHUr1+fLl26ZNP1+Ph4li9fzl9//UWfPn3SUw+OHz/O3LlzGThwILVq1cqzXykpKXz66adcv34dT09Pbt26xciRI3Mc1cuLqVOn8vrrr9uk9Wls3RJOSN14PEoayxj9Zv01unTrY8j2XlOQmg0Fr9s6J1hjFxcvXmTNmjV07Ngx06//zp0707lzZzZv3syYMWMICQmhY8eO6b/+58yZQ1xcHBMnTsxRbMFcCP7f//43169fZ8mSJVy6dInbt2/TokULPvjgA6v65+Xlxfjx44mPj+ezzz4zVFheRGyO/mZElSyNwtjKOiUTY7h2/iw4iBNsL2JvfplGo8mT+Ph4lixZQt26dRk0aFD6/gYNGtCgQQNOnTrF+++/n55SVqpUKQC2bNnCpk2b0pe4zwl3d3deeuklkpOTWbNmDYsXL6ZYsWKUL1+eKVOmWBVldnFxYeTIkZhMJt59910mTJhg6D49PT1JTk7O9fmSFwGBQdyIOWLYCXZzLToTmh1ds7UTrDGEUip93frhw4fn+mv76aef5umnn2bv3r2MGzeOypUrc/z4cV544QUaW+nYeXl5MWLECK5cucLs2bNp27atzf11d3e3q9KCXQt5BAaRVKw4xW8n2mzrmpJM/BVj9Y81Go0mI7t37+bAgQP06tUr13kbVapUYfLkyURERDBlyhTKli3LtWvXqF+/Ph999JFVOurq6kpoaCjPP/8877zzDi+//LLNfXVycrJrLkRgYCAnTpwwtApqhQoVOHV0P0EBtjvQAK5FyAl2dApDmTaNgxEZGcmsWbOoV68e3bt3t2q4qWHDhkyePJnk5GQGDBhgtQOcER8fH8N1hMH+4upGa2mWq1SVuOKlDNkK4JxofHlQh0LM5XaMbhqNJmeSkpKYN28eKSkpDB482KqJywEBAUyYMIF+/foRGBhIt27dbA4kODs725zGkBGjtYDBvJDHX3/9Zci2ZMmSxCcYb7vIOMF2anZh0G3tBGtsZvPmzQwZMoTg4GCbbRs0aGB4ZTcRsasGoz2C6uvry8WLFw3ZBgUFcc3d03DbbgYiyI6KI4upRlNY+fvvv6lVqxaPPfaYzbbe3t52LQZhzyiai4sLCQkJhmz9/f05d+6c4bYTE43fs2uxoiNI2gnWFDl8fX25du2aIdtatWrZtV68PU6wPZHgoKAgjh49asjWx8eHWCdjw2oArsnGHgKOhmCuOWl002g0OePn52dYs8E+7bRHswMCAgwHTXx9fbl69arhtpOSjY38Abi5Fg3Xyl7NLgy6ne83JSLFRWS3iBwUkcMikuuMJBHpJiJKRBpZ3vcSkQMZNpOI1LMc2yYixzMc87tzt6W5m5QrV46oqChDtl5eXsTGxhpu256oQmpqquGIRkBAAGfOnDFkKyIkuhh7EJjEiWtx8Rw7dsyQvaMhdmwaM1qzNVkpXbo0MTExhu3tGUWzJ5WsUqVKREZGGrJ1cXExVBkijeTbxh202NhEtm7diiNV3zKKPZpdGHTbmv8hScBTSqmHgXpAaxHJtkahiJQCXgF+S9unlFqqlKqnlKoH9AHOKqUOZDDrlXZcKWXMq9Lcc9JWGDKKvbm5RrEnMuDv709ERIQh28iICG64lSTeLXu947y46hXIssotaPKvN7l8+TJz585l7969hvqgKVJozdZkwt7l1+1xgv38/Pj7778N2dasWdOukUOjz4vExERiYuK4et22+45PSOW7DX+jXCoQEBDAvHnz2LBhg13pJJq7S77VIZT5p0yc5W0xy5bTz5sPganA67lcKhRYbqCPmkKGj4+PXcNM9giqPUNrQUFBREZG4u/vb7NtREQEZ8+e5fr16zaVS/th+UK8/thMm+jDHHjgYW6nOvFg5DFKx9/M1cYkwu7gEE4E1aX/iFHpfW/WrBl79uxh7ty51KxZkyeffNLuh1th4z67nQJBa7bmTmNvKtmxY8cMzSF54IEHWL9+vaF2b968yaVLlzh9+jSVK1e22u733TuJOvcz3Z6As3+f4ejRUlStWpYAv7yXQT5+Kp5tv/zNiwNHpDvfNWvW5O+//2b+/Pl4e3vTtm1bu55hhRFH12yrSqSJiDOwF6gKfKqU+i3L8fpABaXUOhHJTVBfADpm2TdfRFKBNcAElcPYgYgMBgaD+Q9CU/C4uLjY9cvWHifYw8PDZkc0DWdnZ7777juqV69OiRIlrLIxmUwsWLCAyMhI/vOf/zBt2jRKlSpFnz59cq2VCXA5MpKtn3/M0zf+oGyCORev4dUDmIDDgbWJNRWn5tXTlI3JHEy76hnAj961aPHyazyew//3kJAQQkJCOHLkCHPnziU4OJiWLVvaNexXmHB0QS0saM3W3ElSU1NJSUmxqs5vVsqVK8f+/fsNtXvp0iUOHTpEVFQUfn7WZ9+Eh4fz008/MWnSJBYsWEBMTAzdunVLX7wjJ5KTk1mzYg4hNZJo+Ih5X/XyiVQnkXNRN/n5r1IEB3vzQHm3TMGHhMRUftp2EY8yVRjy8uhs133ggQcYOHAgly9fZunSpZQoUYIOHTrkuBKqI+Lomm3V/2ilVCpQT0Q8gW9FpI5S6hCAiDgB04D+udmLyCNAfJqNhV5KqYuWIbk1mIfeFuXQ9lxgLphXH7LqrjSFGnuiCjdv3mTlypUMGjTI6hqSCQkJzJo1Cy8vL/r06cP48eOpUKECvXv3pnTp0rnanT59ms8++4yePXvSoEEDAMaPH09cXBzTp0/HycmJnj17ZotwrF+xmDIHw+kafQinLAE4J6DutcMAHPeuzlHfKlS9/je+1y+xJziE4+VrM+CV7EKalQcffJAHH3yQM2fOMGPGDF599VWrPovCjIhy+MLrhQWt2Zo7SVoqWV4//HPjypUr7N+/n7i4OKvLpSmlWLFiBcePH2fixIl88sknuLq60rNnzzx/WMXExDB9+nSqV6/O1KlTARg9ejQmk4l58+axatUq2rRpQ5MmTTLZ7dv7GxGnttL5cXDLIYOiol8yFf2ucfnGTbb/UprAoLJUDS7BybOJbNl+lv4vDsfNLe9Isb+/PwMGDODGjRt88sknDB8+3KpSdYWZ+0GzbfpZp5S6ISLbgNZAmjiWAuoA2yy/jsoBYSLSQSmVtl5mD7IMqymlLlr+jRWRZUBjchBUzf1FamoqUVFRbN26lebNm1s9nH/t2jWmTZtGkyZN8PHx4e2336Zu3bp07949T/HZvXs3q1atYvTo0QQGBgIwadIkIiMjmTp1Kt7e3vTp0wcfH590G5PJxKJFi7hw4QJTp07NFmX18PBg3LhxJCcnM3PmTG7dukW3bt3w8/Nj8+wpPHX9D7zj808XqXHjL2oAZ0tX5JuyzQkZOIrHbBi2A/PEkYx9d3ScHDyqUNjQmq25U6xZs4bBgwdbnWebnJzM559/DsAHH3zApEmT8PX1pW/fvnkud3/x4kVmzJhBhw4dCA0NBeDdd98lOTmZ6dOnk5SURPfu3alZs2Ymu61bt7JhwwbGjBmTbaTQycmJIUOGALBixQrWrl1LixYtaNasGd+snEvDaonUz5Y1nx1/zxT8PaO5HnuDdT+6U6J0NYa8/JpVn0canp6ePPHEE0RERFC1alWbbAsjjq7Zkt/sRRHxBW5bxLQE8BMwRSm1LpfztwGvp4mpJerwN9BUKXXass8F8FRKXRWRYpjFNlwp9XlefdHr0BcOEhISeOedd2jatCkdOnSw2u7UqVN8/vnn9O7dm3PnzrFz504ee+wx2rZtm+dw/g8//MAvv/zCO++8k2kI6dixYyxYsIDKlStnWwEpMTGRTz/9FA8Pj3Txy4mbN28yffp03Nzc6N27NykpKcyaNYvQ0FAaNmxo1X2ZTCbmzp2Lz7l9dIzemy36aw2bAx6j1fuzbLYDWL58Od26dbNr0uCdwp516B90dlNLSgQZbrvhrdMFugZ9YUFrtiYrSik+/vhjXF1dGTFihNWjaGmR1Vq1alGzZk2WLFlC9erV6dmzZ54pZX/++ScLFy7k5ZdfpkqVKun7r1+/zowZM3B3d6dXr16UL18+Ux9XrlzJkSNHePfdd3NNvTCZTMyePZuoqCg6dOhAjRo1mD59OpUqVaJXr15WfiLw448/cvb4r/Rv64Gbq+2affCMG3UeH2EoReTkyZNERUUZqtt8pylIzYaC121rnOCHgIWAM+bR3FVKqfEiMh74XSkVluX8bWQW1ObAZKXUoxnOKQlsxzxhwxkIB0ZbhvByRQtqwbNnzx727t1Lr1692LdvH+Hh4VStWpV+/frlamMymVi4cCGRkZG8+eabmRzeHTt28MMPP1CvXj26du2ayZGLjo5m2rRphISE5OlsX7hwgdmzZ+Pn50ffvn05deoUK1asYNSoUQQFWfcHmpiYyNSpU7l69Wp6qoOtbH5nMM2jjP3//F9ACE++P8eQbXh4OLVq1cr0QCko7BXUpe7GBbVBnHaCQWu2JjMXL17km2++oUOHDiQnJ7Ny5UqcnZ159dVXKV489/rlmzdvZuPGjYwdO5YyZcqk7z979ixz5swhKCiI3r17Zzp2+/ZtPv/8c1JSUvJM0YqPj2f69OmYTCZCQ0MpUaIEM2fOpHXr1jRv3tzqe1u0aBHbt29n6tSpeUaXc+Ob5dPo/HiSzXYApyLdSPZ4lgcffNBm29jYWDZv3kynTp0MtX0nKUjNhoLX7Xyd4MKEFtSCIyEhgWXLllG9enWefPLJTMf27dtHWFgY3t7e/Otf/8rkQJ45c4bPPvuM0NDQ9LzanDh48CDLly+nRo0a9OjRg23btrF9+3bGjh1rdR5ZdHQ0EyZMwMPDg/Hjx9t8jyaTiTlz5hha5x5g/fuv8GzEL4Zs9/vXo9H4rwzZHjp0iNu3b1O/fn1D9ncSewV1mbtxR75+3BntBBcytGYXHEopwsLCSE5OpmvXrpl0+cKFCyxbtoz4+HheffXVTI5sbGws06dPp1q1avTo0SPX61+5coUZM2akz7W4cuUK8+fPZ8iQIVSrVs2qPqakpDBz5kz+/PNP5syZY6hywpQpU3jzzTdttgNYuXgmzzeLN2R7LdaZ7ccr06VLV5ttlVIsWbKEPn36GGr7TlKQmg0Fr9u2x/E1RY69e/eyZ88eevbsmeNEsgYNGtCgQQOOHTvGxIkTcXV1ZeTIkaxYsYILFy4wZcqUfCOrDz/8MA8//DCnT59mxIgRtG7dmkmTJtnUz7JlyzJ8+HDCw8NtskvDycnJrkl7ycXynhiRFyVMyURERBAQEGCzrZ+fH/v27TPcdmFBcPyZxhpNYeDSpUusWbOG9u3b51iaLCgoiDfeeINr167x1VdfcfXqVUaMGMGxY8dYv359jnm1WfH19WXChAnExMQwefJkEhMT+e9//2tTP11cXBg9ejTjxo0zXDrMnmpDOBVHqXhDulPSzcTVqEuGmr1fSlveD5qtnWBNriQmJrJs2TKqVq3K0KFD8z2/Zs2avPPOO5w/f55Ro0YxcOBA+vfvb1OblStXpkOHDoYnDNizqAXYV7nidjHryq7lhGfiTQ4fPmzICfb29rZrSVSNRnN/kBb9TUxMzDYqlxPe3t68+uqrxMbG8tFHH/HAAw+kV1WwltKlS/P666+nT4Izgj3zGexxgkuV8SHp9nWKG8gJdiumuJ1kfPVTTeFAO8GaXPniiy/o06dPpqEya6hQoQJ16tShUSNjIxxBQUH89ddfhnKtSpYsSVKSsRwvsE9Qb7sar/tYKvEGF06fAp6x2dbZ2dnwsqSFDUePKmg0BcmqVasICQmxaXEIgFKlStG+fXvi4uLyPzkHvLy87FqW2Z4FJJKSklBKGYqu+gc8QFzCKYq72l73XgSK31/rXhjC0TX7/qiwr7krlClTxmYHOI2SJUty82buq6Llhb+/P2fPnjVkCwUXVXD39idFrJt1nZUStxO4dTXScNv3hRMs4CTK8KbRFHVSU1OpWLGiIduAgADOnDljyFZE7HJk7dHskiVLEhtrLCIbFFSB6FjjXpyHu/E4otbswqHb2gnW3BUqVKjA0aNHDdn6+vpy5coVw23bI8b2pEN4VwjmVjFj0WAnTKi4Gzbb3bp1i7lz5+a5EpIjIWJ802iKOvYsae/v78+lS8ZyXME+R9YezQ4ICCAqKir/E3PA19eX2HjjzqhbMdttTSYTq1atMlTNojBij2YXBt3WTrAmT4xWDwkMDOTEiROGbO1dW93eerlGHeGAoApcd7N9OedEl+Ks830U9+AajBkzho0bN1plt2vXLpYtW0bv3r2trmlcmBE7N42mqOPn52fYIXR3d7drJMxeJ9io7lauXNmw8+7k5ERcgu3POJMJdvwh3Ewsw3vvvcfnn39uVWT33LlzzJo1i8cff5z27dsb6XKhwl7NLgy6rXOCNblSqlQpYmNj81xaODfKlStnV1ShoIbWvL29iYiIsHlI8dixY3zxxRfUKRfCWTdvHrlxjBIpCfna/eVZmT8CQ+j6r9dwcXFBKcXmzZsZN24cderUybFEUXx8PEuXLqVu3boMGjTIpn5qNJr7Fz8/P44cOULdunUN2RdkNPf06dPZVoGzhho1arB582aaNm1qk11aHfoAf0/iElJ45EEnfErnnxt85aYLW/YpWrZ7kabe3gAcPnyYCRMm4O7uzqhRo7ItoGEymfjmm29wdnZmxIgR9011iPsB7QRrcsXf35+oqChDTrCXlxc3btg+vJ+GPYKamppKfHx8ptXlrOGrr74iNjaWzz//nMDAQPr06YOnp2eeNikpKXzxxRfExcWlL7F8+/Zt1i7+EvdTe3k09gSlkrJPGEl0cWOzV32CO73ICxkmEIoIzzzzDM888ww7d+7k3XffJSgoiIEDB+Lk5MTu3bs5ePAgPXv2dPh153PC0deh12gKkoJMJbPH1tfXl/3799vsBP/yyy+EhYXh5ubGRx99RGhoKJUqVcrXbv369Wzfvp1x48bh4eGBUoqtW34k9tAfNKrhTKB3SjYbkwn+d0iId65Cj36dMx2rXbs2tWvX5syZM0ydOpXU1FRGjx5NyZIlOX/+PN999x2dOnWiQoUKNt2fI+Domq2dYE2u+Pn5GV7fXEQMLSeZRnJyMiaTyaaV20wmEx9//DHFixdn1qxZ3Lp1i9GjR+c7uS8qKoqpU6fSuXNnXnzxRQCuXr3Kf//7X8qUKUOvXr0oV65cNrvjx48zd+5cXnrppUyVLIoVK0aXF4eSmprKj6uXow79yiNxJymbYC5jdqJMyxmCPwAAIABJREFUJQ4ENqLrv17PM/LSpEkTmjRpwqFDh/jwww/THeT7OfqrAyQajXGKFStGSkp2B84We6O4uLgQGRmZo1bmxapVqzh27Bhly5blzTff5MUXX6RGjRp52qSkpDBhwgSqV6/OlClTEBFSUlKYMWMGcXFxdOvWjdq1a2ezu379OtOnT6d+/fpMnjw5fb+I8NTTbYA27Nq5g9927KR+NScq+qcgAldvurB5n+KZtgPw8fHJtV+VKlXi7bffJiIigtmzZ3Pz5k3q1atnVbk6R8XRNVs7wZpc8ff35+DBg4btjfzRJyYm8sEHHxAcHMyECRMoUaIEI0eOzDfKcPjwYebPn8/gwYOpXr06ANeuXWP+/PlcuXKF4cOH51iDd+HChURERDB+/PhMkWMfHx/Gjx9PXFwc06ZNw9nZmZ49exIcHExqaipffvkl169f5+OPP871Pp2dnWnbozdK9WLzuu+J270JUlOo2OlFejR+xOrPpE6dOtSpU4fFixcXirXm7yZODi6oGo0j4+TkZHPwAWD69Oncvn2bZcuWce3aNUaOHImfn1+eNjdv3mTixIm0atWKd999FzDr/+rVq/nqq6/o1q0bISEh2ex27drFt99+y4gRIwgK+mfJXhcXF1577TVMJhPz5s1j5cqVPPfcczz6qHn17x9//JGtW7cyduzYPEc3H23yJDR5kj//PMia7T9R2j0Fk1swPfp1s/rzCAgI4PXXX2fx4sV07Wr7inKOxN3WbBH5CmgHRCml6mTYPwIYDqQAPyil3rDsHwO8BKQCryil8pxko51gTa6ULFmSyEjby3bFxcUxY8YMkpKSGDt2LK1ataJp06b55kFt2rSJrVu38uqrr6YL6Llz5/i///u/9LXosy6hnBb99fT0ZMqUKTg7/1OizNvbm1GjRhEbG8vSpUs5e/YsL730EtWqVePq1atMnjyZjh070q9fv1z75OHhwTvvvENycjIzZ84kNjaW2NhYBgwYYHXenYjwTPtOJD/7HPPnz6eTDQ5wUcI8W9ixh9Y0moLGSLkwk8nEwoULuXz5MmPGjKF+/fp07do138jw6dOn+fTTT+nXrx8PPfQQADExMSxZsoTz588zePDgHNMTVq9ezdGjRxk7dmymkbrixYvTp08fUlJSCAsLY82aNenpYSkpKUycOJEqVaowefLkXJ8nTk5ODBkyBIDly5ezdu1aTCYTDRs2ZMqUKVZ/JnXrPkzdug8zdepU3njDege4KHGPNHsBMAtY9E+70gLoCDyklEoSET/L/geBHkBtIBAIF5HqSqlck721E6zJkcjISFavXk2pUqUYM2YMjz/+OO3atcvXbvv27axbt4633norvQTMt99+y9tvv80TTzxBmzZtskUZEhMT+fDDD2nUqBETJ07MJG4VK1ZkzJgxXL58mTlz5nDjxg1GjhyJj48Px44dY968eQwaNCjPXLJSpUoxdOhQEhMTWbVqFdOmTaN8+fJ88MEHVufVurq68vrrrxMeHk6JEiUMTTxxdXW1ayEPjUajyY2EhASWL19OyZIlef/99/Hx8WHYsGH5RnXPnj3Lp59+SmhoKAMGDADg4MGDjBs3jho1ahAaGkqJEtlXw/zkk09QSjFp0qT/Z++8o6I6tz78nGHoXXpTFBAwFqJGxd4RC0bssZdEY8xN4k2x514jlhQV/UwiGmOJYo8aa6yxVyyIYkcBpYmAdGbmfH8gc0XazJkkCuFZKysyM3vecwZmn332u/dvF9ups7CwYMKECeTk5LBhwwZ+/PFHhgwZQsOGDcnIyCAkJIROnTqps7+lIZfLCQ4Opk+fPvz+++98+umnKJVKPvnkE2rWrKnxZzJ48GD1sfbrJy2Q1UU2U09PD4VCoVNp4D8dURSPCYLg/tLD7wPzRFHMe/6aIkmU3sCG54/fFwThDtAMOF3W+1f/ZioBubm5qFQqrRu9pCCKIrt27SIrK4sJEyagp6eHKIocPXqUGTNm4OPjw5AhQ0rYZWVlsXjxYlxcXEqM3ezTpw99+vThyJEjTJ06lcaNGxMcHIxcLufw4cMcPHiQjz76CAcHhzKPy8HBgX//+9+kpaXxyy+/EB0dTf369Zk/f77GDsbIyIjhw4fz+PFjxo4dK6mxzNfXlxMnTmhtV4QuDvWfQHU1RDVVAZVKRUZGRoWNtX8WERERnDt3jnfeeUe91R8dHU1ISAgGBgZ88sknJUrKVCoVa9asIT4+nvnz5xcLlhs1akSjRo24d+8es2bNws3NjaFDh2JhYUFMTAxLlixh2LBh+Pn5lXlMxsbGjBo1ioKCArZu3UpYWBg2NjZMmTJF489FEAQCAgJwd3cnOjpaqwD4RXSaBKqDbVGjYmmleFWFV+Sz6wJtBEEIAXKBT0VRPA+4AGdeeF3c88fKpDoI/pvIyMjA2NhY68aDkydPcu3aNUxMTFCpVAQFBWFtrb0WrSYkJiayadMmAgMDizXDCYJAhw4d6NChAxcuXODLL7/E3t6e999/H5lMxokTJ9i+fTuTJ08ut2mg6D0iIiKYPn06CoUCf3//Etnf8rCysmLixIlMmzaN8ePHSzpPV1dXEhMTsXkub6MNTk5OksXoQTeHCkgeD1pZqMKnVk0lQ2ry4f79++zatQtbW1syMzPp1KmT1mOMtTnG9evX4+npWcIf+vj4MGPGDGJjY1mwYAF5eXl88sknWFhY8ODBA5YsWcLAgQMZOXJkme9fp04d5s6dS2JiIl9//TUFBQU4OjoyZ84cDA0NNTpGfX19Bg0axLNnz+jQoYOkGwMPDw+OHDmitV0RuiQfdLF1cHAgMTGxagfBuvtsW0EQLrzwc5goimEV2MgBa6AF8BawSRCEOpQek5dbr1EdBP/FiKLIzp07efr0KSqVCjMzM3r16lXq9tKLZGZmsm7dOho3bqyub8rJyeG3334jMzOT7t27a92FW94x7tmzh/T0dHX2tyyaNm1K06ZNuX79OrNnzyYtLY0GDRrw7bffarxe48aNady4MSEhITo1DeTn50uS5fH09CQ+Pr6YooOmFEmgSUUXh2ppaUlaWtpfdhP06hGra4KreS24cOECFy5cwNTUFKVSSY8ePbCzsyvXRqVSsWXLFgwNDZk4cSKCIKBSqTh06BAHDx6kZcuWf+pkx8uXL3PmzBkGDx5crgKOm5sbkydPJikpiRUrVvDo0SPMzc3Vko6a4ODgwOzZs5k8eTIfffSRpKZnDw8PEhISJKkNyeXyVxbIGhoakpWVJWnnUNfm8tefP8Vnp4ii2LTilxUjDtgmFk7zOicIggqwff74izp0rkC5Awv+cUFwamrq3zau8PHjx2zZsoXu3bvj4eGhXn/Tpk3o6ekRFBRUapfq6dOniYqKYvjw4cWCZWNjYwYMGEBBQQF79uwhKSmJLl264O7uLvkYlUolS5cupVu3bmpVBU2oV68eM2fO5L///a+6jkxbdHFMRdtMLi7l7nSUio+PD9u2bZO89qvaWivSba66QXA11ZTk6dOnWFpa/i0SUzk5Oaxfv566deuqM6t5eXns3r2b1NRUAgICStV6jYmJYefOnfTt27eYT5LJZHTp0gVRFDl16hRhYWH4+fnRrFkznY5z48aNODg4aLUbZm9vz6RJk1iwYAGTJk2StG6NGjVITU0td8evLHx9fTl69KikdUG364UuftfJyYnExERJ2XxbW1uddJurKZPtQEfgqCAIdQEDIAXYCawXBGEBhY1xXsC58t7oHxMEFykEWFpakpGRwRtvvEGrVq3+kq3lorra7OzsEpnVGjVqMGLECDIzM/ntt9/Iy8ujZ8+e2NrakpWVxbp162jUqBFjx44t8/319fXp3bs3KpWKgwcPsm/fvgqzAWUhCAI1atTQKgB++VikootTc3d3JyEhQVIQbGlpSVZWluS1dTlupVIpuVHCwcGBhw8fVqihWVkRqJZIq+Z/KJVKNm/eTH5+Prm5udjb29OjRw+dx6KXxcWLFzl//nyxuloozAQGBwejVCrZv38/e/fupX379tStWxeVSsXWrVuRy+XlTgITBIFWrVrRqlUrLl++TFhYGG3btpU0IQ0KfVD79u0l2ery+bm4uJCQkCApCHZwcCA1NVXy2romH6SWktWuXZuEhARJQbCenp5G45QrK3+HzxYEIRxoT2HZRBzwJbASWCkIwjUgHxjxPCscJQjCJuA6hdJpH5SnDAH/kCD4xIkT3Lx5kxEjRqgzq9euXSMsLIzatWvTuXPnPy3LkJiYyObNm+nWrVu52z5mZmYMHjyYvLw8du3axYMHDzA1NWXYsGEa16DJZDK6du3KnTt3uH79Ov7+/lofr0wmo/BvRxq6OFRdtpm8vb25ffs2TZo0kbT2q9pas7OzIyUlRetSFlEU+eOPP4iJicHGxqZUIfiqQHVNcDVQKL21e/du+vXrp66nfPz4MWvWrMHMzIygoKAKS8o0JTc3l/DwcDw8PMrNrOrp6dG9e3f1d/HAgQOkpaUxYsSIYnq1FeHn54efnx9r166VHATrgomJCZmZmSXkJjWhbt26PHr0SFJZh0wme2V+19LSkqdPn0raBfb19eX06TLFBcrl5MmTXL9+ncOHD9OhQ4cq2c/xV5+SKIqDy3hqaBmvDwFCNH3/qjnC5DmZmZn8+OOPGBoaMmbMmGJOs379+owbNw4PDw9WrFjBjh07dG5aOnnyJIcPH+b999/XuO7J0NCQvn374u7uTufOnSUpQNjb25OYmKi13Z+BLqMynZ2dJR+3p6cnjx8/lrz2q9hay8/PJzk5maVLlxIbG6uxXVxcHF988QUeHh7MnDmT1NRUwsLCOH/+vKTjeG0RinQnpf1XTeVHpVKxceNGIiMjSwy4cXJyYsyYMXTp0oVNmzaxdu1a0tPTdVrv4cOHrFy5kr59+9K2bVuNbARBoH379gwcOBB9fX2tAuDXATc3N27evCnJ1tvbm/j4eMlrv6p+iqJmaG0RRZFz585x5MgRrfxtdnY28+bN49atW8ydOxc3NzdWrFjB7t27USrLTUxWLnT02a+D366ymeBTp05x/fr1Ytnf0vDw8MDDw4NHjx6xZs0a3N3d6dSpk6Q17927x7BhwyTZurq6Eh0dra4d1gZzc3NJAul/Bvr6+pImDEFh5/Hjx48lbTPp6+uTm5urtV0RUp1xbGwsT548YcqUKbz77rsaH/uVK1dYu3YtEyZMwNnZmdDQUPLz8+nfv3+Z2SBRFNmwYQPR0dHMmTNHXULRpk0b2rRpQ0REBMuWLcPb25t27dpViSyDUH4jbzVVmCJVheDg4HLLnIpKyrKysti5cycqlapU2UZNePjwId26dSt3glh5x/H06VNJ675KnJ2duXr1qqRdNAsLCzIzMyWvrUsQLIoiBQUFWu8+5uXlcenSJSIiIggKCqJDhw4a2SUlJbFo0SI6dOjADz/8wC+//MKOHTvo2LFjuVnd06dPs23bNj777DP10CcvLy+8vLyIi4vj559/xtramp49e2qssPE6U9l9dpUMgletWkW9evXKrat9GWdnZ8aMGcPatWslryuXyyV9SQEcHR05d67c+u0yeZXBj4ODA/fv35cUvOuquatLIBsXF8fKlSvLFIIvjR9//JHs7GxCQ0MRRZGNGzcSFhbG4MGDadSoUak2+fn5LFu2DJVKVUxB44svvkClUvH999+zYcMGgoKCaNy4sfr5+Ph4lixZQo8ePdSC7y9TpLJRNDTE1dWVgICActU9qqnmdeT3338nJydHraqgCaampgwePJi1a9dKrvd0dHSU3PQkk8le2RAEU1NTySUNDg4OxMXFSV77VWRzs7OzycjI4Ntvv2XUqFEal5MVKXN8/vnn2Nvbs3//fqZMmUKzZs3o06dPqTaiKLJ9+3YuXLjAzJkzMTIyAmDo0MLd999++42pU6fi7+9Pz5491QmgnJwclixZgo2NDd98802p7+3q6srYsWNJTk4mPDwcQ0NDgoKCJJUEVvPnUCWDYD09PZ27b6VQ1Anq7Oysta29vb2kEcWvGjc3N8kZbF00d7dv305sbCxz587lk08+UTuq8hBFkfXr13P79m2WLl1KXFwcs2bNwtXVlaFDh5bZWBgfH8+CBQsYNGhQsVn2I0eOpKCggG3bthEeHk5gYCDt2rVTPx8ZGcmqVauYMGFCqZ+PTCZj4sSJAKxevZpff/2Vzp07k5CQQFRUFLNnz9boIuvj44OPjw8PHjxgy5YtDBw4sEKb15UqkMyuRgKJiYmSd9Gsra0l13va29sTFRUlaV3QrSdCF4pK4KQEwTY2Njo1qEkNZK9fv05ycjLTpk0rUepSHidPnuTXX39l5syZmJiYsGjRImQyGe+8806Zykj5+fnMmjULPz8/5syZo75BCgwMpFu3bhw/fpwZM2ZQp04dRowYoQ5kk5OTCQ0NpXXr1oSElF5W2qtXL3r16sXx48fVI6ZdXV3Zvn07n376qUYBup2dHSNHjiQjI4O1a9dK1rx/HajsPrtKBsGvCkdHR5KSkiQFwUZGRjo1qOnCq8pgHzp0iKioKA4dOkTHjh01yuRkZmYye/Zs2rZtS2hoKHFxcYSGhpKTk8Mnn3xSbiAbGhpKUFCQeuvU3d2duXPnkpSUxLfffou1tTVDhw5Vb2EBhIWFkZGRwezZs0vNGOvr6zNw4ED69+/Pnj17mDJlCm+99RaPHj2ioKCA7777TqPPYsSIEQD8/PPPPH78mFmzZmlk9yK1atXi2LFjWtu9LgiAUC0PUY2WFA0kkBIEm5ubk5GRIXltXXoiQPrwm6JrjZTkgy4Z7OjoaGJiYli/fj39+vXT6PxVKhXfffcd5ubmLF68mJycHNatW0dMTAxjxozBy8urVLvs7GyWLFmCvb19sV206dOnk5+fT2hoKNnZ2fTv37+Y5vuRI0fYv38/H330UamBtiAItG3blrZt2xIREcGsWbOwtrbG3d2dM2fOMGPGDI16c4rK0q5evcrixYtZsWJFhTYvY2FhUamzwFXBZ1cHwS9hZGRETk6OpM5je3t7Ll++LHntV5VVsLOzIykpSWu5MaVSybolP1KQmMrGX9YxcKhmdXnp6emEhoZSr149vv/+e3bv3l3q9tLL7Ny5k4sXL/L555+rL3iurq588cUXPHnyhJUrV/LkyRMmTpyovhsvKlt4ua72Rezt7fnqq6/IyMhg4cKFGBgY0LlzZzZu3Ej//v1p3rx5heckk8no2bMnPXr0YNGiRTRp0kTjRpsX6d27N8uWLdParkoggFClW3WrKQs9PT2dpAPv37+Pr6+v1ra6lpLp4rOLaoqlZrCvXbsmad11v/xMQ19rwpYtYdTo8Rqdg1KpZMWKFWRkZPD9998THR3NzJkz8fT05J133ikzaIyOjmbFihWMGTNG/fsxNzdn/Pjx5ObmsmnTJn766ScGDBhQrBzs9OnTbN26tczMqoGBAZ999hkqlYply5axYcMGAgIC2LdvHw0aNGDu3Lka/W6LSsouXrxIeHi4VkOfimjYsKEkqc4qQRXw2dVB8EsUZRWkDKCwtbXVaaTuqwiCc3JySL5xhqTLB2nSYzjevppNUTtx/AR/hK2DvRcQn2Rw//xd5u47gW2LBoyZML7MQPbgwYMcOHCAqVOnqrO2PXr0oEePHpw4cYKpU6fi5+dH37591Z9HdnY2X331Fa1bt+a///1vqe9rY2PDJ598otaDjomJoXfv3uzYsYPu3bszaNCgCs/JwsKCL7/8kvz8fMaPH8/SpUu1vhkSBIHmzZtLbh6xtrbWueO9mmoqG0W+U8oUTHt7e86cOfMXHFXF6OKzi0rgtA2CRVHk+JFd5D69w7GjprRt30Uju8TERPbsXE1ge0ccbex4lgXHD6zgZkwOI0e9X6avu3XrFmFhYYwcOVItjVavXj3mzZvHgwcPmD17Nk5OTgwbNqzYSORvv/0WU1NT5s2bV+rNjZGREcOHD0ehULB9+3Y2bdpEhw4duHr1KtbW1hoFpDKZjPfffx+Af//73wwZMqRYMK0pjRo1YsuWLVrbFaHrjkA1r44qGwRL3Wayt7cnKSlJUhCsqzC21C/SgwcPOH/+PAqFgqCgIGxsbDSyu3D6OJkROwlWXUSur+DugVg273HBo01fGjcrPfupUqn4eupM5GfvoDh6Sf14XvRDiH5IyonrfHPyCkYNPZj4+b/VTVoZGRksWrQIb29v5s+fX+p7t27dmtatWxMZGcmMGTPw8vLCysqKK1eu8Omnn2p0XkVZhpycHKZMmcLXX3+t9edqYGCAg4OD5Auck5MT+/btk2QrCMI/2qFW9vqyaqRRlHyQEgQbGRmRl5f3FxxVxZiYmJCdna21tGVubi5btmxR+5mySgJeJi42lstnttLaNwvLBiqS0s+xa8MF5BY+BAS+XeY1L3zdKlzschn+tgNFfbPmptCxhSn+fiZcOLGGa7cyGPTOWPVESqVSycqVK0lNTS1zxHKtWrWYM2cOKSkpLFiwAAsLC/z9/dm2bRujR4/WSM9cLpfTr18/goOD+fLLL3n//fcllRQGBARIVgySy+U6NRTr4rMNDAzIzc3VqK/ldaSy++wqGQRbWFiQnp5e7K5UUxwcHLh9+/ZfcFTl8/DhQ2JiYpgxYwYff/yxxoHs0qVLyc/P55tvvlFPqktPT6dbt25lbtHk5uayd+0SGhdcpbHigfpxTzEWT3ksD8884Nfjrti+2Y02Hf+XZThz6jSHflwDeyPIS0kr9b3zHyTAgwQUh6/wbcRtZD5uNGjVjMOHDzNlyhSNRv42aNCAefPmERMTQ1hYGHPmzNHos3gRY2NjatasKdk5SR1qAYV/Q7o0Ob6qsphXj1A9Mu4fir29PZGRka/6MLQiKyuLmzdvMn36dEaPHq3xAIkDBw5w5MgR/v3vf2NnZ8fhw4c5fPgwLVq0KFNlRhRF9u7ahItZHN0bP1MHHvaWBfRoBmlZUezbcoN8vVr0CBqozrwmJyfz268/E9jeESfb0gN1YyOBNk2Mad7QiEtXtnLlRir1GrRk+/btjBgxggYNGlR4Tra2tsyaNYvMzEw+/fRTlixZorUfk8lk+Pn5Sb6h8fb25uzZs5JsQbdAVhef7eDgQFJSEjVr1pT8Hq+Oyu+zq2QQXNQ0ICUItrKyIi2t9ACvIq5cucLNmze1VixYt24dd+/eJTQ0lMzMTNauXcujR48YP348tWrVKtUuNjaWRYsWldj+6devHwqFgr1797J79246duxYbHDHhTMnyLi4g+6Ki+ijKPW9a4qPqSl/TGLUfXZG7MbQqzWXz1xF79wtFIcvlWrzMoqkpyi2HENmacrR5GS+/nGJRnYv4u7uLmkUdBG6ODU3NzfJmSkTExOdZIR0caiCIKBUKiunTFoVqC+rRhpFN51/NwkJCdy/f19rxYLjx4+zY8cOJk+ejIWFBVu3bmXt2rX06tWL1q1bl2qTl5fHrFmzaNKkCSEhIeqsbadOnejYsSNnz55l2bJlNGjQAH9/f/Xz8XFxXDq9pTD7a1K6X7EyVdCtCWTm3uPozq9JK3AgJ0/AzT6f4X0ckGvgDgz0BZo3NKDJGw6s33myzOxveZiZmWFrayvZhxXp5deuXVtrWzc3N3bt2iVpXdDN7+qil1+k9FEpg+Aq4LOrZBBsb29PbGwsdevW1dp2//793Lhxg2PHjtGmTRuNSiry8vIIDw+nVq1azJ49m/j4eBYvXkx2dna5igWxsbEsWbKE4OBgtQahlZUVH374IdnZ2YSHh3P79m2GDRtWbFtp2bJlZGVlERISUmqgLZfL6dWrFyqVisOHD3Po0CGaNm3Kw4gjvFlwlcaKGI0+CwdVCj31U7j/IJHLe9LIunpPI7sXUaVnYS6T7lx0CSYtLCxITU2V1HhSt25d4uPjy8zMVIQuDlWX4N3W1pbU1FTs7Owkv8erpCoM/KhGe+RyueRJWtevX+fKlSuYmZnRo0cPjb4/oiiye/duMjMzmTZtGjk5Oaxfv5579+4xatQovL29S7XLzs5m8eLFODs7F6tZHTx4MAMHDmTXrl1MnjyZNm3a0KNHD/Xzhw8f5sCBA3z00Uel3lgLgkCLFi1o0aIFkZGRLF++nNq1a6PITcXJLJ7ujTM02nY2M1LSyU9Jbn4cVx470aKR9hNI5XoCDvYWkgI6KCyjkNrk6OjoSEREhKR1ZTLZK0s+2NnZER8fj5ubm9a2jo6Okscyvw5Udp9dJYNgJycntm3bhrW1tUZbOQBpaWksXLhQrSt448YNli9fTs2aNenatWuZDiEyMpITJ04waNAg9Va/i4sLn3/+ebmKBeHh4dy6datMxQITExPGjBlDfn4+W7duZc2aNbRq1Yo//vijhF5tWchkMjp37kynTp34bvYM3rc4izHabzW5iInomWmfVS9CL1/6mEhdR2XeuHGDVq1aaW3r7e0tWfoNdK8R08U2OTm50gbB1fxzKSgo4MCBA3Tq1EmjAEyhULBixQoyMzP5+uuvSUlJYf369RgZGREUFFRmnW5SUhKbNm0iICBAXYtrZmbGe++9R25uLps3b2blypUMGDCg2FS1U6dOsW3bNvXghZeRyWQEBQXRq1cvjhw5wrRp0/D29ubWrVsl9GrLo0GDBjRo0ICjR4/iYvgAX9fsCm1exshARE+HDJ2psfSdJHt7e5KTkzXOqr9s+/jxY8lr6xIE6+J3XV1duXnzpqQgWE9PTyfd5mp0o0oGwUZGRkyaNInTp0+zbNkyGjVqRPPmzct0QL///juHDh1i2rRp6vGZvr6++Pr6EhMTw8qVK7G1taV79+7qL0p+fj7r16/Hzc1N3Z36MqUpFvTq1YudO3fSq1cv3nnnnQrPxcDAgMGDBzNgwAA+/PBDvvvuO0mKBXYu7igzLyBlwqEBCmSm0sc7CnnSHVNBQYHkJkdnZ2fOnj0rKQguksqTii4ONTc3l5slqOONAAAgAElEQVQ3b5aZjSoNlUrF1q1bMTAw0Hgs6OtGoebkqz6Kal4Vo0eP5s6dO/z00084OjoSGBhYZjaxaEriu+++qx47bm9vz8iRI0lPT2fbtm2IokivXr3UZXGiKLJ3717S09MZP358mYoFw4YNQ6FQsGPHDjZv3kzbtm2JjIwsoVdbFoIg0LFjRzp27MicOXPo27cvb775ptafR8uWLblx8rzWdkUoFNKTD0aG0r+INWvWJDExUVIQbGxsLHlHAHRLmhgaGpKfny/JdxsYGHDkyBE6d+6sld3x48e5deuWRrHA60hV8NlVMgiGQkfUsmVLWrZsyZUrV1i+fDkeHh7FhjKkp6ezaNEi6tWrV6Zigbu7O2PHjiUxMZF169ZhYmKCh4cH586dY9CgQRpttb+oizh58mS+/fZbrbeK9PT0sLS0lLxlU6uOJxnXrDBTap9VAJCbSP9TkeVKD4LNzMx0anKMj4+XvLYuWYX09HStA9mcnBy+//57TE1N2bVrFxs2bKBfv34Vdlg/ePCAnTt3EhwcXPn1Kiv51lo1uuHp6Ymnpyfx8fGsWbMGCwsLevbsqS77Ksr+Pnv2jG+++abUjLGlpSVDhw4lJyeHnTt3kp2dTfPmzTly5AhdunTRqExOLpfTt29fgoODmTlzJh988IGk/oDGjRtLViwwMDAgV3pMh0opXanIxEiPJ0+eaNyg/SJ169blwYMH+Pn5SVpbl7IEXTPBmzZtUpcmaoIoimzYsIHo6Gjq1q3L1KlT6dChA507dy43cVOUGGvatCljxoyRfMyvBZXcZ1fZIPhFGjVqRKNGjbhz5w4rVqzAyckJQ0NDDh48WEyvtjwcHBwYNWoUaWlphIaG8uWXX2p9HEZGRtSsWVPytB5dFAsaNGhAQpQZ2gvPFCI3lv6nIugQBLu4uJCYmCi5yVHqNCiFQsG1a9e0Li1ITU1l4cKFvPnmm5w7d46VK1fSv39/mjZtWq7dhQsXCA8PZ9KkSepAVqVSERYWxqZNmwgMDKRFixbFbFQqFdu2bUMulzNx4sRKX5tVFZosqvlzcHFxYfTo0aSmprJx40b09fXx9fVl3bp1jBo1SiPpLWNjYwYOHEh+fj5Tp04tU6+2PARBoGHDhpIDWR8fH86fl57Nzc6TPkVUpZKeUbUylxEZGUn79u21tvXy8tKpxlWXIDg2NpYrV65o1ctRUFDAjz/+SEFBAfXr12fatGk0atSIAQMGlGtX1PvTvXt3Bg8erH58//79TJkyhebNmxMUFFSiSfnEiRNER0czYsQISUO5XiuqgM/+RwTBRRRlGR4+fMiqVavKzP6Wh5WVlU56fq9KscDGxoZ7Kh2atUx0UBvIyZesg+jp6cmjR4+0yqgWkZubS3JystY6nsePH2f37t18/vnnbN68mdjYWMaNG1ehdvSePXs4duwY06dPx8zMDCgMpnfu3MmWLVvo3Llzie2yvLw89VCOl0csy2Qy9Uz59evX89tvv9GhQwc6depEbGws27dvJzg4GFdXV43P7XWnso/grObPpUaNGowYMYJnz54xb948SYoFBgYGGBsbS04+uLi4EB0dLUk7vmbNmuzZs0fSugA5usgfi9IzwRZm8CDmHtBea1sjIyPJNw2iKJKamkpsbKxW9bXx8fEsWLCAsWPHEh0dTXh4OIGBgbRr165cu6ioKFauXMn48ePV9eFdu3bl9OnTzJw5Ezc3N8aMGVPsb04URTZv3sy1a9f46quvSlzTAwICCAgI4OzZs0ybNo369eszYMAA8vPz+eWXX2jSpAljx47V4lN5vansPvsfFQQXUbNmTUkSLEXosuViampKenq6JOkvXRULcpEeBBvqEASL6VlER0dL2h4zMjLi2LFjtG/fXqtM57lz59i0aRMTJkxg6dKlPHv2jE8++aRcnWKFQsFXX32Fj4+Peuymr68vOTk56i2voUOHlmi2TE1NZdGiRTRu3Jh58+YVe04ulxMcHEyfPn3Yv38/U6dOpUmTJvTt25eIiAjWr1/PRx99VKHDL6oZ27NnD5999hlNmzblww8/rPzZ32qq0QBzc3N8fHwkKxYoFArJ0oHOzs5cuXJF0rq6KhZk50oPZPX1VCiVInp62vsIYyPIzHwqad3MzEzu37+vdeKjKLMaGBjIvn37uHv3LiNHjlTXfJdFWFgY6enpzJ49G2NjY/z9/enfvz979uxhypQp+Pv7ExQUVMymoKCAsLAw8vLySiQfAPz9/fH39ycyMpKvvvoKc3Nz/vWvf5GcnExoaCgBAQHMmjWr3ONq3rw5zZs358aNG0yfPh17e3smTJig9WCVav5aKgyCBUEwAo4Bhs9fv0UUxVJrAQRB6AdsBt4SRfGCIAjuwA3g5vOXnBFFcfzz1zYBVgHGwB7gI1EUpe/9/I3oUnzv5ubGjRs3Smxta4KuigXZSr3CSnYJGJlID7aUz7L5fe9vWgfBixYtQi6X8+abbzJ16lTeeustevfuXe6FLDc3l6VLl2JmZqZuYmnSpAmpqamsWbOGxMREPvjggxK1s6dOnWLHjh18+OGHJTKrxsbGjBo1ioKCArZu3covv/yi1gPdt28fR48eLZb9LQ1BEOjWrRsBAQGcPHmSiRMn4uvrq/Ws+u7du2NlZYW9vX2VDICr4Cn97VT77JLY2Njw5MmTUlUdKsLBwUEnxQJdrheZ2dJLGqxN88nOLZwMpy2iCMqCLAoKCrQqT9i1axdnz55l0KBB/Oc//8Hd3Z2hQ4eW6xtFUWTjxo3cuHGDkJAQdcY+Ly+PzZs38/PPPxMcHEzz5sWnmCYkJPDNN9/Qv3//EtdTmUxGz5496dGjB8eOHWPGjBnUrVuXYcOGcf36dX766Sfee++9CncYi5Q67t69y/Tp08nNzdV6Cqmvry+zZ89m27ZtVTIAruw+W5NMcB7QURTFTEEQ9IETgiDsFUWx2LB2QRDMgX8BL49suSuKYmnRzw/Ae8AZCh1qN2CvtifwKtBVseDixYuSgmBdFQuyVXogMaFrZCZHz8YC5RPtamzNOvnRoJcTjbzi2bliCikqF0aOnVBuRufevXssXbqUESNG0LBhQwACAwM5e/YsU6dOpUGDBvTv3x9Dw+KKFUV1tR9//HGJzGqNGjX46KOPyMzMZN26ddy7d4/Ro0fj4eFBSEgInp6ezJs3r9zfqb6+PoMGDWLAgAHs3r2biRMn0rFjxxLZ3/IQBIHWrVsTFRUluSGiSFz9xSEoVYIqUF/2mlDts1/C1dWVhIQESUGwsbExCkXpg4U0QZcgOCtHeibY3EhBfJISn9rabfjGJ4ncvpfOsAADju9dxM2HKoaPmoipadnRdHZ2NrNmzaJ169Z89dVXQGEmND4+nrlz52JnZ8fw4cNLNJI/evRInf0dNGhQsecMDQ0ZOnQoSqWSnTt3MnnyZDp27EjXrl3V0qNF2d+yEASBdu3a0a5dOy5evMjEiROpVatWmU2VZeHh4cHw4cO5dOmSZPUIXXYEXluqgM+u8Nvx/E4/8/mP+s//K+3u/yvga+DTit5TEAQnwEIUxdPPf14DvE0lcagmJiY8e/ZMLaemDY6Ojjx69Ejy2rp8kdKyIc/cAEM0d8oqBC4ofDG9GE+DZu7EGJqRfeE2+XHJ5drJbS2x6d+UvoG52BoXZlF6uT8lrSCZ31dP42FWDUaN+7hElmHx4sUIgsDcuXNLOJui7aXo6GhmzpyJh4cHQ4YMQS6X8/3332NoaFjq1taLmJmZMW7cOHJzc9m0aROhoaFMmTJFq/ozmUxGr169uH37NsHBwRrbvYijoyN37typcKuvNBwcHCrdiFmNqeT1Za8DVdVnGxgYSO4tKColK7qp1pZXpVigQo/UTBk1zLR7j1uPjEmNeYqDcJJjsfWoXdcGVweh3Jt8hVLkfGQutiaZtHujsKa3QyMl/vVkXDz2A5F38uk/+D1sbW2L2e3evZvTp0/z6aeflnjOxcWFkJAQnj59SmhoKKampgwZMgQnJye2bNlCZGQks2bNKjew1NPTo0+fPrz99tscOHCA8ePHM3z4cFq2bKnVZ9KkSRPu3LlD69atJZXVODo6EhcXp7VdlaeS+2yNbhEFQdADLgKewFJRFM++9PybgJsoirsEQXjZodYWBOESkAFMF0XxOOACvPjXFPf8sdLWfo/C7MNrM1bQxcWFpKQkSUGwtbW15LHMSqWS2NhYsrKyyr0rf5nHjx9zaP4cWl8+z9kGPhi6G9NA/x4mYvlZ5WQ9OyIeO+C08gjmubk4UPgH8LBJA+685UVW5APy7pSUIDPr0Ij6QU50rZ9Q4jkr/WcEuD0jU/GY4+EzuJlqzvB3PyY5OZklS5YwbNiwCssmfHx8mD9/PnFxccyZM4fU1FQmT55c5ojp0jAyMmL48OEkJCRIEjiHP0dcXUoQbGZmRmZmZsUvrOYfS1X02UVDGKR8X318fLh0SbOR76Why3f9yZMnxMfHayVfmJuby68rFxBoFs2jCDNuWDjj7SHD1rz8BEZOvoxzUXr45lylrt4TAGrnxPPokh3HjBvg6mFHHVdZiWD4UbLIrbvptPJ+xsv9g0b6KlrVy6NZXbh8aTXbbuXStccQ7O3t+eqrr/D392f27NnlHpe1tTX/+c9/yM7OZtGiRTx+/Jjg4OAK62pfRBAEunbtyrFjx7QOgIsoKkeUIiWpy7W7mtcXjYJgURSVgJ8gCFbAr4Ig1BdF8RqAIAgyYCEwshTTx0BNURSfPK8n2y4IwhuUXplaam2ZKIphQBhA06ZN/7T6M0EQJM/6rlOnDvHx8ZK2o3Nzc8nK0r7W6u7du/zwww/06NGDkJAQHBwcGDZsWIU6xb+tWoXJ77vpevkCAmB1KBEVcLm1P3haUt84FgtV8RIHFQIXlb5k/5FMzSMlEz01L0ZSE0is582NRq3IvPmI3Gv3kdewwGbAW/TtnoutcckA+EXM5Dl0cL5PCwd9zm6bxYHrpWd/y8PV1ZWQkBCmTZumVQD8Iro2OWZkZEjeETh58qSkdatiLTA8F16vmqf2t1MVfbaDgwOJiYmSgmATExOys6VppIuiSFZWltYNzRkZGSxcuJDmzZuzevVqVCoVgwcPxsPDo1y7C2dOkHhmE29b3MZQKCj8lNNvcSfCnetmtfCsLce5RknZiNuPjUm5n0Yb4RKyl8renGXJOOcdJjXSgj9uv4ljLXvq1pGjUsGFyFxqvJD9LQt9ObxVN5fGnhAVs5VVPz1m0qQvtJKQNDExYerUqcyfP1/yUB+lUim5ydHR0ZGDBw9KWlcQBMkKI1WVquCztfqNiqKYJgjCUQprwa49f9gcqA8cfX5xdgR2CoIQJIriBQrr0xBF8aIgCHeBuhRmEV7sPnIFpNcISKBGjRqkpqaW2L7RBEdHR3bt2kXbtm21CkjOnz9PREQE//rXv1i7di2mpqYEBQWVW9OkUqlYtWoVCQkJanmgLl268PTpUxYuXIiZmRlDhw7F2bm4AnBCQgIH582hRcQ5LFMSiz0nA7xPnIYTcPOtJuT5uuFjloCt8gkperZcTHDC8efDmGWX7xQdrt/E4fpNUmvX5OqwDtT2MyKg/mO0ua8w1iugtcNDolMbSc626JKl0XUsc3R0NM2aNdPa1s7OjsTExIpf+E9CECq93M7rRlXy2Q4ODpw5c6biF5ZCXl4ejx49Ii8vr0QvQXnEx8ezbds2RowYwb59+8jOzqZHjx4V1hYfOnSI/fv3M23aNHXgrFAo+L//+z/S0tLo06dPCZWf/Px8tv70HU3lN2hsWfKj9SQGz8wYYiOdOGbiSc1aRtSyyyG3QMb563p4Z1/F63n2tyxq6GXQruAPnt0y5ti9JihMLGlTLwMDLSIBPRk0rJ3L9Tq2ksez61JeomuToy7liLocd5WkCvhsTdQh7ICC587UGOgMqAV2RVFMB2xfeP1R4NPnncZ2QKooikpBEOoAXsA9URRTBUF4JghCCwqbMoYDS/7ME6sIZ2dnIiIi6Nq1q1Z2+/fvJyUlhe7du7NixQpcXFwICAgo9640JyeH9evX4+3tzbhx44BC/dvU1FQ2bdqETCajd+/eJTKK9+7d4/vvv+edd96hcePGxZ6ztrZm1qxZZGdns3DhQgAGDx5MnTp12LV6NUa/76Hr5fMIFTRv1zl/Ec5D7Bv1uezXCP2Ix9Q8rJ2uZY37D6lnb0XTepZaBcBFyGUqKMjS3vA5ugbBujQ5Xr58WVIQrMsxV2Uqe5PF60BV9dnW1tZER0cTGBio1Q5eZGQkx48fZ8yYMYSHh2NgYEBQUFCFigU7duygoKCADz74AJlMhre3N/n5+ezevZsnT57QtWvXEuUeGRkZhIaG4uXlxddff13sOblczscff4xKpWLlypVs2bKFbt260bJlSyLOn+bx6Y28bX6rMPtbDm48xi37McnXrTlqXB89RS6tZRElsr/lYS7LoY3iBNctOmsVAL+IqbF02UxdS8kSExMlBcGmpqbk5UkXX9bluHXZfX6dqew+W5M/fydg9fMaMxmw6Xkd2SzggiiKO8uxbQvMEgRBASiB8aIopj5/7n3+J7ezl7+5Kc7Pz4+zZ88SFhZGvXr1aNWqVbmB0JMnT9iwYQMdO3YkICAAgHr16hEbG8vPP/+MtbU1PXv2LJFluHjxIufPn2fIkCGYm5sXe65ICD4rK4udO3eSm5tLz549sbGxYfXq1cTHx1coDm9iYsK0adNQKBQsXrwYecw9Aq9ewCpJuyyjW9Q18vQMsTh8USs79XE8fEyGwhkzuTQHY2oofddUl7tzExMTMjMzS/xuNEHXRonqQLgkVbXU42+mSvpsmUxGcHAwP/30E/b29nTv3r3c735+fj7h4eG4uroyYcIEoDD5kJGRoQ5wg4KCSlUs2Lp1K7169SoxIMPAwIA+ffqgVCr5/fff2bt3L+3bt8fb25sjR46wd+9epkyZUq4euUwmUw9L2Lx5M7Onf8pA92TetNAuQ2knPKVJzkWSZLbIJASyMhkoCqT7XRNj6d9VXfXy4+LiSui1a4ouflcXW112n19nKrvP1kQd4irwZimPzyzj9e1f+PdWYGsZr7tA4ZbcK6NIbeDatWssX74cd3d3OnfuXCLo/P3330lMTOS9994r4XTd3NwYO3YsycnJhIeHY2hoqB6VuG7dOry8vNRTv8rC1NSUwYMHk5eXx65duzh16hSDBw9m1KhRGp+LXC5n0qRJbPriU60D4CKMc7PJNzHBQELtnEniE9JzjXCWOEzPVPMdyhLo4picnZ1JTEyUFARbW1vz9Kk0QXmQHryvXr2a+/fvs2zZMlq1akX9+q/0a1TNa0ZV9tm1a9fm3XffJSEhodySsmvXrnHs2DEGDRpUIsi1sLBgyJAh5OTksGvXLjIyMggMDMTJyUmdjCjK/paFnp4egYGBiKLIsWPHWLduHd7e3iWyvxXRv39/tuQ+pU7BVa3sijAml1Sl9r6riIJ86TrExobSgx83Nzeio6NLaP9qgo+PDxEREZLX1uV6IdVnnz59mqNHj/LgwQO8vLzo2LFjlcsIV1aqq7yB+vXrU79+fe7du1csy/Ds2TPCw8Np165dhWUTdnZ2jBw5koyMDH799Veys7MZOHCgVne6hoaG9O3bl+zsbJo2bSrpXAqMpYtxm6U+IcHZFoM7D7W2lalU5OVL/1KbGEjPSBgYGKBQKCQ1LXh4ePDo0SNJTY4ymUySUxRFkW3btpGSksIXX3zB22+/jb+/f4V2KSkpzJs3j969ezNixAhEUeTUqVOEhYXRqFEjSReU1wqBwrzlX7mEIKwEegJJoijWf/5YDWAj4A7EAANEUXwqFKY4QoHuQDYwUhRF6Vffav40HB0dGT16dLGSsqCgIExMTAgPD8fR0VGd/S0LY2Nj+vfvT0FBAXv27CE+Pp6AgIAKG9depEiHNjY2liFDhkg6lxoObuTEGWGK9hrwckFFvkJ6MKookK5DbGyA5O39Ir18KT7LxMREJ718qYHsxYsXuXPnDtOmTaNevXoa/b4VCoVah37+/PkIgsCdO3f46aefcHR0JDAwsHI32/0NPvuvphJ/+n8+derUoU6dOjx69Ii1a9eir6/Pu+++q9Wdo4WFBUOHDv0Lj7J8BAtLRKQNhjN+lkF2zVpYSwiCAfLzdNha05du6+TkJFlz18fHh8OHD2ttV1BQwNa1S2jnkc2GFXNp33MUjo6OFdolJSWxaNEiOnTowIIFC1Aqlfz2229MnjyZDh06qEttXmbt2rXExcXx3//+Vy2PJwgCrVq1olWrVly+fJmwsDA8PT3p0KFDpdyi+ps6jVcB/weseeGxycAhURTnCYIw+fnPXwCBFNbEegHNKRwWUcnvNKoWL5eUpaen079/f2xsbDR+D319fXr37q3TcehS7+nk5k56rBmmgvTATiqqAgVSJyiZGAnEx8dLUut4VXr5e3Zv5U1vfTavW4R3/fY0bFTxFNO8vDy+//57jIyMCA0NBeDs2bPMnDkTJycnxo0bV+rv/ezZs2zdupWJEycWqx339PTE09OT+Ph41qxZg4WFBT179pSkf/2q+cepQ/xTcHZ2ZvTo0a/6MCRh6eJCgaERBnnlKzuUhn5eLvnm0jPJeTnSA1ljufSJTC4uLpI0dzMyMjiyZzW2ZtlsXLuU7m8P16gs4urlC8Sc30LvOgkY6RWgEhOIPD6fP57WoFnnd6hdp2QmSRRFtm/fzsWLF5k5c6ba4enp6fH222/Tu3dvDh06xPTp06lfv756elJqairz5s2jZ8+eDBs2rMxj8vPzw8/Pj9u3b7N8+XJq1apVZkD9OvNXdxqLonjs+WjgF+kNtH/+79XAUQqD4N7AmufDJ84IgmAlCIKTKIrSZ+hW85dQVFL2qtCl3tPV1ZX7ShOcJWbUdPnGiArpQbCVqYLLUVGSgmCpmrtKpZJtG3+iYZ0CNqxZiH/bPtR6qW67NJKTkzm87xc6tTDF1soUUYQHCWfZtuEITjWb4t+yTal2ly5d4pdffuFf//pXMRnOolLKqKgo9dS6jz76CAMDA1QqFSEhIbi7uzNv3rwyb4xcXFwYPXq0ut/IwMCAfv36VbpekSqvDlFN5cKhjgfZ5haSgmABdJr+osjWYWtNrpSkuXv9+nXCw8MxMzMjKiqKSZMmaXRHffTwXpSp5+jfIgs9mUiB8gEXji7gYaoFnXsMK/ViplAo2Lp2CY2sYujllaJ+XCZAI7skGtomcfPKYjYftKJey768Ub9wOlVycjKhoaG0bt26TFF5QRDo3LkznTt35vTp08ycOROFQoGZmRkzZ84st5P9Rby8vPDy8mLVqlWSVS9eGbqP4LQVBOHCCz+HPdesrQiHosBWFMXHgiAUtZ27ALEvvK5oQER1EFxNMRwcHEhISJAUBJubm5OlkBfO9ZOADOl+F6UCkNaQYW6sJD4uRmu79PR0QkJCMDMzY+rUqXzwwQcaDa+4dSuaaxd2EPiWHqZGAqJYwI0HW9l4XKTRW93x8alXqt3e3dsw14+nf4Cp+vImCODuJODuZMqjlBvs3HwBU2tfOnYKQBAE8vPz+eGHH9DT0yt3Cukbb7zBG2+8QUxMDN9++y3p6emoVCr1eGZNsLGxYeTIkVy+fJmoqCjefLNEOf/ryz9hbHI1fz+mpqZkZmZqHPi8iKubG3E1bLBKSZK0tp4g3aFKDYLTFWZciZfzS0iIOhCsKHhTqVR89913WFpaMn/+fORyOfHx8SxevJisrCwmTZpUaj32s2fP2LXlB9r6ZOJc/3/bj/p6Iv5emTRTZXLl/P9xMNGUlh0GUPO5I7t29RJ3z26kV50EjPVK34oTBPCxTsHbKoWYe8vZdtqKdMGV2/ceMH36dExMNMuy+/v74+/vz4wZM5g6dapGNi9jY2PD06dPKxymUsVIEUVRWjF96Wg8IKKafzYODg7cvHlTsn2uKP1SLNchCDZW5aJQmiLXMhlcoIDjkXpERd1g7dq1DBgwQCP95c2bN3Pjxg21fnJmZibr1q3j/v37jBo1Cm9v7xI2SqWSXzf9TF2XNN5uLSA8v0YJAtRzV+JbC+493svWdXtw925Lk6aFkpUpKSkc3LuGTi3MsLMq+9icbSGovSlP0h+y99dFPM2y4NKVG0ycOLGEOkhZuLu7M3XqVGbOnMlnn30mqdG6SLa1mr+X6iD4NaRoMpKUINje3p6bBtJri2QSr/HPvNy5nSqgeFiH5o5PqGGQXqGNKMKVtJqcT7Bh7MTPkclkaomhFi1a0KtXr1L1l6Ojo1mxYgVjx44tVgLh4uLC559/zpMnT/j5559JTk7mww8/VNfqHjuyn/yUM/RvXpj9LQ09GTSunYWfexY3bv/MxiNGKJVy/KzjCPJK1uizEASobZFKbYtUtkQrmDNnrkZ2LyNWoPFcHo6OjiQlJVW+IPjVZK4Ti8ocBEFwAoruIOOAF/d6//YBEdVUDuzs7Dh+/Lhk+3yJ2VgAfZm0IDhLNOV2ksDd4wLN6xlS0y5Xo6/fwxRjfj+TzcBh4+nW14IbN27w5ZdfUqdOHYYMGaLuWXiRjIwMQkJC6NSpEzNn/k+kxMzMjHHjxpGbm8vmzZtZuXIlAwYMoEmTJgDcuXOLq2e3EdBMjplR6ecpCODhrMTDGeKST7A9/A9ylJa42uUyoKuZxtr1NpbQvY0pJy7nMGTYt5oZvYSrqytJSUmSguCiISCVjsq021gK1UHwa0hREKxNl3IRMpmMPC0mIr3ITX9/zOz1iR3RBatDkZjHlT/6GEClp8edLq3JbNOcDya8j0qlYt3qFVgo7tPMOQ1Hw9RS7TIUZuy9a4dPy4G81/9/2z+BgYEEBgZy6tQppk2bRsOGDdV1UqIo8t1332Fqasq8efPK7Kq1sbHh448/5tmzZ+osQwNPa9rVz8a1gWbNJzIB3nDNop5LFpejZPjW0CwAfhlLE+mC8nqPYIgAACAASURBVKIoaj1euwh7e3tiYmIkNQu+Sl7R1tpOYAQw7/n/d7zw+ERBEDZQ2BCXXl0PXE1pGBgY6DR9Mg9pdaD39OqQVdOf4wVv4vjkMl6yuxXaiCLcKPDgeIo9730yA5lMxqGDBzgZeY63fI3xdMopNa4pUMAfVwzIFF1494OB6sd9fX2ZN28esbGxhISE4OjoyLBhw9Rayb/++itXr15lypQpWFlZlXpMRkZGDBs2DIVCwY4dO9i8eTNedexpWlfF221k6uxvRbjaKXC1g5PX82jdWFoyyNxEXye1oYSEBEnXbj09PVQqHUpbXhHV5RDV/OnY29sTGRmptZ1CoWD58uU8s7JB3rw1PpfOoa+BY860tOJO2+Y0zbqFRWIqIvAgwIM7ygaYnb6N1c2YUu2eeboT0awBfaZPwcnJCSgMwoeNeg+AbZs3oIy9SjOXZ7gZJSEIhQ44Mr0mZx/VYOzEL8psGmjZsiUtW7bk2rVrzJgxA2dnZ2JjYxk9ejT16pVe+/Uy5ubmjB8/nri4ODJvL8fVWvvua0EAhcTGEQBjfelOzc7OjuTk5BIjsTXB3t6es2fPSl77lSD89U0WgiCEU9gEZysIQhzwJYXB7yZBEMYAD4H+z1++h0J5tDsUSqRpLtxdTTUasnv3biLi8jB2bcBbJjFY8KxCG4VKxinzLni2eJPWroVZx5QUL05euY9lciT1heul2mWJJuxPro1T07cZP+J/zWCdOncBunD+/DnW/H6Apr7G+LrmqLOocU+M2Hc6h/5D3i0zkHVzc2POnDmkpqaycOFCzMzMSElJoWPHjnz55ZcafRZyuZy+ffsSHBzM0d++oaH2saTOmJnIdFIbOnHixF9wVK8pf4PP/qupDoJfMzIzM1m/fj13794lNTWVwYMHa1RLGh0dzfLly3n33Xfxef99nj59yrpvv8Ez8RFvXLmIUVZmqXa3WrTA1F5Oh6Qz6gJIAXBPvYs7d4lv6catVp0xuPQIm0uFjlWlJ+Nu59aktXyLCf+aWOYxBfcfBAzi9/17OXHzOI2c8rieqI9niwG8169xmXYvUr9+febPn8/ixYsZP368JD1fZ2dnIq5K/6IqdRBCNNaXLkZfs2ZNEhMTJQXBxsbGOo0HraqIoliWhECnUl4rAh/8tUdUTWVHpVKxZcsWbt68yTfffMPQoUPVSYHySE1NJTQ0lCZNmvCfuQsLxyn/sAhnVSxvmcVjQ+m7aPdltUl0b0frVh7I5f/zTba2xrTuVI/0dE9OX/LDMPEGfqpLyGSFyYdoRR2OJ9kz9pMZpZaZAbz1VjPeeqsZN2/e5Oe9W3jT24jUZyrS8h159wPNdJBr1KjBrFmzSE5OZseOHXTr1k0juxcRBIF8hQwk1jvrUkpmaS5yPPKGpCDYycmJlJSUil9YzWtDdRD8GnH69GmioqIYMWIExsbGPHjwgNmzZ+Pk5MSwYcNKvQNXKBSsWLGCZ8+e8c0336gzq9bW1kwMmUNOTg4/fz0ft/iHNLh2BdO0QseaaW7J3fbNaZx1C8uksqeeuaTF4kIsyfUduNGkM7lxOdy3sqb39MkadfQCdA0IhIBAPv74Y7777usyHXB5vPHGGyQkJEgeapFToEMQLOowBERfRWJiIg4ODlrbent7c+/evcrVLawDVUFzspp/Fg8ePGDnzp0EBwczYMAAsrKyWLhwIYIg8M4771C7du1S7fbu3csff/zB9OnT1b0fMpmMsR9MAuCXlWGYPbtJM4sEHCmcAKpQCZwy74JH8zdp6Va2io6lpQEt23uTlV2HC5caooy7SULKM+z9ejFueHuNzsvb2xtv72mEh4fj5uZGl9attfhUCrGzs9NpqEV+gfQgWEBEFKX5EzMjSEmWVvovk8kkaxhXRqqCz64Ogl8DsrKyWLduHY0aNVLPlAeoVasWc+bMISUlhQULFmBpacmQIUPUjV43b94kLCyMUaNGlTk619jYmAlf/oeCggJWLvgO+/t3sRLzsLIWaZ90VmONSbvMROwyE/nDowUTZi+RdJ5OTk6kpaVpJWRfRL169fjjjz8krQuQVyA9kBV1+JZbGeRw7do1SUGwl5fXP2trDaHSb61V889ApVKxbds25HI5EydOVKvZmJqaMn36dBQKBYsXLyYjI4N+/fqp/fPTp09ZtGgRfn5+zJs3r8z3Hzq6sKRs+5aN5D88R20rFfl1mtCqZR309TVLIpia6NO8VR1uRFviYvqGxpJdL9K4cWNu3LihtV0RugSE+UrpPtvMSCS/AAwllFobGkB+bsUlKWXxTwqCq4LPrg6CXzFnzpzh6tWrDB06tMyyB1tbW2bNmkVmZiaLFi1CJpNhbm5OdnZ2sexveejr6zPui8moVCoOfDmRhvGnJB2vKdK/4K6uriQmJkoKgh0cHHTqnM1T6NCgpsOMd3N5NrEP7lHKbnuFGBgYSC5pSE1NJfPWFc4ed6N5m/aS3uNvR0A35f9qqvkbePjwIdu3byc4OBhXV9dSXyOXy5k0aRIqlYrly5ezadMmatasye3bt5k2bZrGeuhv9xsIDGTVqv9jZDsvScdrbW3E1cibkoJgDw8PSRM1i9ClWbBAKQek2dtYKMnMkRYECwIYGUp3RFLPWaFQkJF8l72/baJr976Sdkz/dv4Gn13aqPsXnvsU+AawE0UxRcqo++og+BWRn5/P6tWrqV+/Pu+9955GNmZmZkyfPp38/HwWLVrEF198ofW6MpmMXJn0iTRGqnyysrJKlcGpiKJRkZo2tr2IrttM+UrpDsXQQIZCJSAvQ1atPEzk+WRnSFOWiIiIICoqirt372rVbXzg183on9/D2CdXSdkZze8ndqPfqC3tA3u+9sMzKnuncTVVmx07dqBQKPjwww81+i7JZDLGjRsHwPTp05k/f76kdXOypQeT5mYGxMXdl2Qrl8t1C2R18Nl6+iaoxGxJ85tsLJQ8ziyUPZOCmYm0ySVPnjzh/v37nDx5kpYtW2rsb69dvczdS9sY1e4ZStVZTv12l6dKF7r1GvTaT5D7G3z2KkqOukcQBDegC4XNzEVoPeq++pLzinj69Clubm74+/trbWtgYCBJvqWITB1+7Rb5WURFRUmy9fHxIS4uTvLaOjljlcRxTEANcyU5SmmOKC3flMSEx0RHR2tsUzR288qVKyxZsoQdO3bwn//8h2vXrpW/Vloa4bO/4I3jK2nz5AoyROwzHtEp5hANDv4fR+b+m32bw1EqpTfrVVPNP5m0tDT69u0r6WZSitShGkGOQiGtPtbYWE5OzqvZ3tfFZ1tZO5CbJ+2m3dgQsnOkNcfl5cPTtAz27dunld0vv/zCjz/+yKJFi3j06BHTp09n//795TbpKRQKNq39P2SPNxPk9xRjuQIzg3xa135EZ/dLXNz/Lds3rSArK0vSuVQFRFE8BqV2iS4EPqf4ACP1qHtRFM8AVs+138ukOhP8irC1tdVpe18X55IlSs+KWmY94fStWzRr1kx72/9n77zjpKjv//+c2V6ud+64AscV+gFHr4IUpahoAAV7i4kl7ZeoSUwIfjXF8o1Ro/kagyJiRIwUg4j03stRjivccf24wrXtO/P74+C849rurFGO7PPx8KHuzntmdm73Pa95f94lKMinH7MvztiNTnGhRF0DVFsiGRFe3OmQjauRZdhfHk2NfhjP/PZ2Pv74Y959913mz5/f5bU7fvw4K1as4NFHH22J/l69rDpr1qx2D09frf0Ucd86bq8+0eEY1ZCmi0xq2kZjxRF2n9tHXfxgZi6827cb83+Anp5f5uf6xpPUs87wxWebzUFYrE4CA7zvAS+KAhof7vS++F2Xy4UkSYquW0xsAvWWsxj13j+0l1WLVFbZscbpMXjRLjjngsSRszL3P/IMO3bs4Nlnn2XQoEEsXLiwU5va2lpeeOEFbrrpJhYvXgzAHXfcwR133MHmzZt55plnyMzMZN68eW1SHE6dOkHuoU+YPaAeg8bVbr8GtYvR8WU43RWc3FZOUUMYE6cvuOYGIH0XPlsQhLlAiSzLx696IPV61L1fBH9H+NoY26enc40eCUHRdDijs4m6su8mmqvUVpIkastKOZBtJDPF4vEEIZtDZO9JDYN0eZhV9ewtTsFoFBkUWtrllKYau5mvCiKZOPcxxlxulbR48WLcbjefffYZn376KTfccAM33nhjm3P8/e9/T2RkJC+++GK7fLDWy6offfQR69evZ/LkyYwcOZLP//IiE2uOE93Y/XATs62ecRd2Yi09xL+Lc5j7dOfFOd86Qs+vNPbjpyuUDr9JSupLfZ1NkQgGMJmVTxH1KZobHExtba2iOpDDB/cQHQzBZtB7uBAnSbAvSyDK2MDk3pc4fDIKtyaQgSlaArroNGp3wOZ9TcQkjmPBXc1BimnTpjFt2jT27t3Lr3/9a2JjY3nooYfaCPqVK1eSn5/Pr371qw6nxF3Zx8GDB3nmmWcYOHAg8+fPZ/0n7zIgspS5Q+u7/UwalcSwuAqGSJXs2PIGo2/6sUdtU78VvhmfHS4IwqFW//+2LMtvd3pIQTACzwLTOz6jdnQpdPwiuIfiiwg2hEViL9ZjcHrfvkYjuZAaO2+p1h1KHWp9fT2m+ot8/s+VzLpjkcfLkQf276Nw32qW9D6Pyurk8ME+OE1BjEyz0FVGyZkLRhor65lkPIUoNP+GxgecxiWJHCxJQ60TGRhWgUH19d9BluFAeTRV2qF875El7fapUqm47bbbuPXWW/nyyy955plnyMjIIC0tjeXLl/PII4/Qr1/3xS8LFixgwYIFfPHFF3z2Pz9nYd0hVLJ3D1QGl5UYZ/ejrb9NBPyRYD/XL+Hh4YqH3wwcOJCSot3ExXk/jhfAZFI+ltmX4INsqWLDP//O7ff8wGPhVlVVxdpP3mbmGBORwTJnLuiorpPI6Osi0NS5nqmoEcnOt5MZX9MSWc2Mq0CSKjh5NoImAklP1hN6VU1iXpHEoTMyt97xgw7zb8eMGcOYMWPIyspi2bJlmM1m7rnnHn7/+98zY8YM7rzzzm4/U2ZmJpmZmZw9e5bX/vgsP7wJjNr20d+uUIkyKVEWiouLSUlJ8cr2P8U35LOrZFke4cX2fYEk4EoUOA44IgjCSBSMuveL4B6Kw+FAlmVFuWkxiX1pyA5WJIIBTCjLKS0sLEQvVfHRyvdYcOfdHtutXf1PNMe38njjYazbTrPx2DbsKSOZvfjeTnOjJUnib68+z9iwCuZH5rY8rWbqzuJ0qjh5pC+NumAy0+3otV8LSJtLZN9xDf1150k3tU9XUYsSY8ynkSQ4XJaKW6VlYMRFnJLI5vPRjJ/9KKO76Z8sCALTp09n+vTp7Nmzh9dff53XX3/d62rgGTNmsHbvv1FdUraioHXZcLvd11YVsr9Kwc91SkJCguLhN8HBweSeUx74MBqV1TTY7XYCdHbe+ssfue/hJz0u0jqVlcWJr97j0UHFGDjHgZUXKCWemXc8TFBQ59Vqq1YuJzqwmiUztagvBxgGJjqRZMgp1nE0182AJInwwK/vQZIE+04JROgbmNj3Urt9iiIMibkIXCT7fAinnCGk9DESbJbZvK+J6ISxLLiry9qp5vMYOJCBAweSn5/Ps88+yx/+8AePu3xcIS0tja1BQRi1ylIhA3U28stKrhkRDHzrPluW5ZNA5JX/FwShABhxuTuE16Pu/SK4hxIYGEhtba2i/KD+/ftTtyOEyIYuvxsdYtGa0OtkNn+5iWk3drQa0TH/+L/XSQmt5PFpDVRZT7Bu+XNUuyK5+77vd5ov1tTUxIrf/4YbbedJqMoFQOt2ML38AJbqLLZl76U2fjCz730Yg8HQYnfo4EHO7/mIRTEFBAjtJ+VpBDcZ2nO4JZHTx/pQow5mWLqLkmodDeUNTDCeQiV0nSoiipBpykaS4Hh5X07UxXDPY7/w+qFk7NixbNq0SbEQbfJhpLPZZVF8U/bj578Rg8GAxWJRtBydkpLC+fPnFQ+/sdqURWTdbgm3w857777N4nse9Dg/d/OmjVgq9/PEXJDki+xe93vOlqhZfN8PO1z6v8L/vfZ7MoJLWRD3dfBhfGg+LrmAI58UcN7Riym3Ptimd3p1dTWfffxXZowNoFdo+4d6UYDU3g5S4qCgQsPpApG+sRJqUSY7z0FmfHWHebVXkxpZSyq1nC8N4ss8I3cs+RE6nXdR8j59+tCvXz+vBfAVJEGPWxI8ri9pjVHj4lJxiaLj9lQ6GnUvy/I7nWzu9ah7vwjuocTFxVFZWalIBB8/fhy3EEBvlQ692/M+tDmR6dSkJnDH4AaKG7bzz//bjTZ0ILfcdkenNkVFRWxZ+zY3D7ISbmiuUI40NjC7fwOX7PVs+vC3XLgUyH0PP9EmV279mk8Qj2xmSeVhdK72zt/otDCl4jD2qiwOFB6lPDqdmfc9ykfvvsbokPI20d/OUAkSg7S5SDIcPZ5CoK6GTJN3DkYUIcOUR7YtUXH7MV+K05p8mGYXZKkhv6jo2hHBAijqh+THz7dEZGQkFRUVnU6C64qUlBR2796t6LhnzpzhbHYxgwZGExzsuWirrLRw6ug5bojPpTFaxxf/fJ6iWjP3P/R4p6todrudf7z1B24YLJOccSWy6mZyWg2jk0UOb3mVrEKB2+98lPDw8K/P8fRpjn75D+bHFhMstk+1UgsSI4MLGCEXcnJjEVubohk1cwkHDuwlwlTJklk61Kquhb4gQFK0k6RoKKnScCrHwg39Kjy+HldICqsjq1TvtQC+gi8+2xQYgdVZiVnnfWRfJcq47Mo7fXzjfAs+u4tR91feT2z1316PuveL4O8QURQVL0cXFBRQWFjIj3/8Y4979jY2NrJs2TImTpzI9F+/yoblfyOo8Dija85idHbetcGqMXIoaQRpozWkBjX/AOODLhE/HMoaj/Cvf2TRJMZz190PtLF77+9v0CewnMWZlR3+ToJ1FmakWGh01rDzk2Wcq9Bz+10P8clrf2Ca9TyJVTndfiad2874ymO4Lp7ki7+UsaBPCYGCd05CFCBVW0SpHOGVXWtUkvLxoL70gbSptMgo61dusjdQXVwAo7pfCvzW8KdD+LmGiYqKorKyUpEI3rFjB8eOHaOkpMTjkfOSJPHSSy8REBDAgw8+wVebv8DpzGdkZgKRkZ1HoyVJ5tChYsy2HCYnVAEQqrYyc7CVeruVbf96gZwyLfc++ESbVbTNX35BU/k+lkxswKBpn/amV0uMS65hZJLA8f1/5V/nJabevIQtG1YzJLCEhXE53QYfREFmSEARg81FHNx+kaHpKaTEeZ/SFRvu5HyRcgGmFb3LyW2NwWDAZrOh13tfcJiY1Jc6e54iEQygUyk/7/8IPdxn+0Xwd0hycjKffPIJt956q8dPlsXFxbzyyissWrSImJgYnn/+eaKioliyZEmXUeH169dz4MABfvazn7VU6t7+6BO4XC42rFyO9ux+RtWdI8jaNqcqLzKViymJTBjSiCi2d4ox5nrmDYEaq5XPV/yG0sYQZtx8G1999jazBtmINHZf/WrWOJjSt5zRCWq2/utVlpRtR+fyblKaWnYTIVkxoawFm0GwUeswgaH7bTtCIyub7Aa+RRX0IRHYKvQYXDbvjys5sFZXKj72N44/EuznGqdXr1589tlnpKWldZnb2hqHw8HSpUsZNmwYL7/8Mq+88gqSJLFo0aIuh+BkZ2fz9ttv88ADD7QMGJox82YAduzYxp59pxgxrDdxcW2X5a9Ef0fH5qIPbC8uA3V2pva3M6afmoNfvMSpCwK3L3qYTz78K1MGyaQMa59XezUalcyIxGoy4mHnkRXMizxLmLp7u9YIAgwMKKVC7UN+qw+TGnRq5f3Se/XqRU5ODoMGDfLadtCgQZQf2UassmwKtNeSCL4OfLZfBH+HjBo1ivj4eN5//33MZjNz5sxp81R+NW+99RaNjY08//zzLU+g//M//0NtbS2vvPIKZrOZxYsXt1netlgsLF26lHHjxrF06dJ2+1Sr1cy7+wFk+X42fvIRrqPbyazPw+xo4FDiCNLGaOkX1D6v9mpCDU3MGtBEnb2BXdveYXHmRa9zngxqFzGhstcC+AohDdVYZB0BgvdRWZUg43Qr/zHrBN9EsNJemhHxSTSdD1QkggVAbbd4befHz38rAQEB/PCHP2Tt2rVYrVZuvvlmIiMjO91+69atfPHFFzz11FNER0cD8Mwzz+ByufjLX/7CpUuXuPXWWxkyZEgbu5dffhm9Xs+LL77Y4UPyxImTgckcOXKI/QeOM2RIL/okhXD4UDEmWy6TE7qfUmnUuJiQUsuoPiJbvnqLJRPrMXqQV9salQgpMXYoViYojaKDJoVDLcC3vs16jfIWpbGxsZw9e1aRCA4LC+O8wiEgAFqVf9jRN4lfBH/HxMTEcP/991NTU8PHH3+MKIrMnTu3TdJ9aWkpL730EgsWLOhw0EJISAhLly7FYrHwyiuvALBw4UKys7PZs2cPP/3pT9vkbnWEIAjMun0h3L6Q7V98TmPuDmaObkAUvRNXQTobYQEBipL+ATQaFC/vhzdWUy/1JkBUmJqg3BejFZRXbkdERFBUVERCQoLXtunp6dTtDyG8SWFE14dJUv8RevjSmp/rH6PRyMKFC7Hb7Xz++edUVVUxY8YM4uPjW7a5Ev0dMmQIL7zwQrt6AbVazVNPPYUkSbz77rt88sknzJgxg4iICN566y3uv/9+BgwY0O25DBs2gmHDRnDmzGlWr9rEnIElHUZ/u0KrlugVKnktgK8QqLdT6goiTK3MlzgdysWoSq3cYRi1KC5yjI6OZsuWLYqPbXUqF8FqwYHL5fJpauw3Sg/32dfIVfQTGhrK3XffTVNTE+vWrcNmszF79mzWrFlDXV0dy5Yt6zJKDM3O+dlnn8XlcvH8889jMplYtmyZ1+cyacZNbK3b5/FQiavxwS9h1rtoUhsxu7yPUAbZaql0pxGr8FstdNMRoiv0ovKG8nFxcWRnZysSwUlJSRxTeZ+X5hTVHIwYSu/yAjYv/SnCkHHcMPcWxcV93wiC0OOX1vz896DT6bj11ltxu91s2rSJjRs3MmnSJMrKyti4cSNPPPFEt0WnoijywAPNtRSrV6/mn//8J3/605+8TpFKT+9P9pF1bdo9eoNBCw6XiFbtvb1B46JWVri2DzidbpSFPUCtVu4vQs0uioqKSE1N9dr2Ss9npTQoiH7LMpwoj0QTFsuBvWuoqoHpM+Yqykv+xrgOfLZfBF9jmEymlijDG2+8QVpaGg8//LBX+1Cr1fz85z/n/fffV3weNpfy1lsalXIxGaSzURMQjrn2gte2ImB3K1fgKh9CwXqVW3GRY2RkJHv27GH6dM9bzgFknznFqc//RnyElr3akaTW5BNqqerWrig4kQJ1JGPO7kWNBMXHuVR4mG1HtmNNG86M79353fUO7uEO1c9/HyqVilmzZiHLMlu3bmXXrl0dRn+74/bbb+f8+fOKawQcbuW38yCjnZomLdFB3qdVqURw+iAl3E43SqWIxgcRHGxwkleqTASrVCrsdu9T4Orq6vj8g5dJDa5hx9kQEqIl4oPqui0mvGQzcKw8ihHjBhIQ2FxIbbO7OXF0PaXlDqZOm9Nl27r/KD3cZ/tF8DWKTqdj5syZNDZ2n4/bEXq9HqtVeccCpw8OVauWkCQURZJNWgf5QZHEKxDBAE6XL8tMypflArVO8vPzPZr41pojR47wwQcf0K9fP37+858zb948xo4d26WN2+1mzT/+QqrtGPNMBc0xlAg4G5rImdq+JF8qIqqh/ZAcp6jmUMRQYmoqmFDZtlVTcEMVE898RdP5A+w5vZ/aPoOZcec9ilsIKaaHL635+e9FEARuuOEGSkpKFK+o+DKi2I3yiGCAzklunUmRCAaUOfvLSE4XSqWIQSfgcAlo1d4HMPQaF+Ul5722Ky0t5c9//jP9+vXjl7/8JWlpaSxevLhbuy0bP0Ms3cIdvQsu96EvpKI6lB1lvYmNgL5hl9qJYVmGkxURuAOTmDIrru3561SMHB6GyyVx4vQmLhRZGT9hZrepj984Pdxn+0XwNUxUVBR5eXmK7X0ZrexwK+9YEKh3UmfXEWLw/klZI0pYfZiL7nb5EAkWJFyS9+kcF+wx5J/UIH34c3YPTGfGEz8kJiamS5srkX6dTsdLL70ENIvbDRs28PTTTzNhwgRuuummdnY52Wc4uf5vzDCfw6xq2wkjTVUA4VAQ2oudl+JJrCsn7lKzSC4OTuC8Joox5/ailjoX+yZbA2Ozt2PL38+R3CM0DRrHtIXtR0D78ePnm8cXn20KCMPhOq9MEKrdNFiUr/4IauW2skt5t4NAo0RlrYG4YO/S52qtBo4Vm0mPKuXj5f9DypCZDBk6rOvzlGVWr17NyZMnWbp0aUtry0OHDvHcc88RGRnJ97/ffvhTfX09Gz54mUmRBcRE1rR5L0pbQ5S2htp6Ezsu9iUiVCQtshZRgDq7nqNl0QwfN4DAwM6DEWq1yLDBoQwZIHPm3C727rEwZ273o5z9NOMXwdcwISEh1NbWKrb3JarglH0QwTob5XVGRSJYEEDWKf9aOp0ysky3y0utkWU460jEZhM4ZkvEptYyMiQXrdh1ZNgpq9hX3gfNmjP0PX4WgL4HjpB19ARfDEhjwqMP07eDyPDRo0dZsWIFTzzxRJs8YJVKxdy5c5kzZw5btmzh2WefZeDAgSxatAhJkliz/A2Sm44wz3SertKXE8VSEkNLKQ8JY9elMYg2N72qy9pFf7tC77QxMmc3O/Vm4FsSwddBux0/fnzBF58dER1Pk/0YWrX3QloQQPKhMFilFpQXFrudilYOS2s05JSoCQ1NIe9CHUMiigk2dP3ZZRlOlIUiiSqmDGpeZR2cYKXw4jrWvP9vYvqMZ8y4Se3sysvL+fOf/8yNN97YrsvSiBEjGDFiBKdPFhY1NgAAIABJREFUn2bZsmXo9XqeeuoptFotWzetRy76kjviCrqcQhqiaWKS5gRNFi27zqSiNujRRyQwZVZvj6+HSiUwMD0Il0tCluVvp77jOvDZfhF8DePrl9iXqILWEOTTaMcaq/KlOZfG+6+lhMDR2AzUGtjR0J9YQyN91Re6FcNNspGDDYkMlPJJo3nKkcul4tTFftSrDIwIPY+xg6K3Ins0Z45rSPz7Z4itIquiJJN4+AQJh09QdPQkuwakMez+exiUkYHD4eDNN99EpVK1RH87QhAEpk6dytSpU9m3bx+/+tWv6G9u5ObgHMxqz/sgRwvVRIdUc6gwkaTKfI/tWqN1Km/9pogevrTmx48vqFQqrFZrt0XQHREbG0dtvpYQk0K/78PtRkJQJGRz7XHIVhu79zYSGBnEgD5uugsqu9xw6JyWsMggJo9tvs/IqQZyC8M4XlRLekgpkeb2qYCXrHqOFQcwvG8TAca297WECCsJEVBet4W1K3diDB/K1BubezKvWbOGo0eP8pvf/KbLwUb9+/fn17/+NYWFhbz00kuEqWq5ue9FekVVe3w9TGoHEwNOsluczvBMzwVwawwGNfX19R73sfaZHu6z/SL4OsaXqEJoRBxW5wlFU21EQcauwA9bHGp2f6Um6sxZtodlEK63MaDmTLd21QFRZPVKYUzU+ZYuDbVuMzus/YnQW0nTFCBe9RQuy5DtTKTOomYCR9v8jtW4GcJZ3G6Rs1V9qRbMDA2+QJDGejn6m4TmX2fpc/Rsp+ckALEnz9Dr5BkuHj3J8kHpnDDqePzxx0lMTPT4mowePZqMjAxO/e0xzLKyQSBuH1rpaJ3K88q95jqIKvjx4wsxMTFUVlYq6hQTExPDiSzlvx8llpIEu0+biZAqOOhMARmGm851m1JmlbQcbOhLP3MlE8KaR9U7LqnZt3cQ+rAQBiVL6DpYjCyv0ZBdomFsZghazdcHEQSBfolakhMiuVAayo6CWhLN5cQHNzbn1ZaF4hJUTB7UdY1NdJCdOSPsVDcd4N8fHWX/aQsTJk71qstSQkICTz/9NF++81N66TwXwK2RHApzs4HgQA3FxcXfjgi+Dny2XwRfx7hcLuUdC6JjqanUeS2CGx1a9h424Dp0ge1jIhmU6iDU0L14O54TiHV7GSOO7EeUZRJyoCEkjB39hmIyy2RUHW/3wCkhcCx2KPpeaqaYstu8FyI2MkmfRZOkZ1dDOsFaO/1151ELEk2SgYONSfSXC0il83QTFRID5BwkWSC3NonDUjzOIhuJ765F5fKsiE4AIrNzCai8SOKbr3glgK+g0+mwSD5UX/vQs07ntGO327+9Arme7U/9+AFQvBydmJhIeXm5IhGsVqtpUqCdXJLAofPBqHGyIzuEhAg38SH13a6iFVcbKChwMdJ0Ep3oBD3YJQ3Hm5KxulWMDMjrcDRxnj2OCruJ8cFn20SOtaKL8YajuBpFDh0YiBAUxqBkAaNewu2GQzlaQsK+jv52hCAIJMRqSIiNpLQylB25l7A3XGJkPytBJs8f6MNMDmYNc9DgiGbatGke27XGLnUeNe4OUcHwoyuYzWpOZZd61Gf6G6GH+2y/CL6OMZvNFBcXe+1QDx06xKpVq+gdbWZQvJGRCZcwabuOKssynCkPoWhDBTHvf9b84mo4PmEI8s2p9E93EW1u30zd4lSz5ys1/fbsI/FiWZv3AmqryTiwC6s5gD3pQ1AHqBhRdQw1EtXmSLJ6pTIquuN0hSuYRBsT9VnYJDX7GlMRBRnR6WaCcNTjVRwRmRQ5H0hCfne7xwK4NdqGRi7m5cPUqV7bAthRnqMt+SCCA231lJWVKRLvfvz8NxIQEEBDQ0ObgUeekpiYyJEjRxg1apRXdlc6FoSHmKizBjOyn4NIc/fFYqX1JnJKNIyLL2mJ3lY0GtlxLoReoRLJ4e3bd0kS7D5jJk6uYHxASZv3dKKT4QG5uGSRU5ZE6hxaRgTmYxQd2CQtBxv60NdcyVhjW7vWqEWJ0foTSFY4eqg/toBIHIKB8ZmhaLWe+7JekWrCg8PIOtZEkEnZIA+NDyOKHSgXwTrJgtMpodF477sNehX19cp7GP+34RfB1zj19fU0NjZiNpu9snvn7VeYNlSi4sQ77N0ayOSZi1vGdnbGlY4Fer2eP/3pTy2vLX/nNRKCGxmZ1Eiwvv3TdJNTy54jBsx/3kxMcdvJZWE7j8PO4+QMTSFr/mBSBgj0DmxuBXMiL5Cm7eUMP7QPUe4899jQ2MCQg7ux6/QcHDAUW7iZiDiZKebsTm2uRi+6GK87xc6mdMYIWR7btSZYbKQ4KoKgkrLuN74KlcuNpVzhVDfAgQ+RWI3yR/WAhmpOXyj89kRwD19a8+MnKCiIM2fOeC1k161bi1pdT0ZGKKtX/52UlGEMHjy0SxtZlvn444/Jyspq07Hgow/fR+04z6h+ErHB7QWg+3L0N0TXxKTEtoIpymwhymzhkk3HjrOhRATLpEVdQhShpEZP/nk3maaT6MXOVwnVgsQQUz6SUeCMLZGLVj0atcS44GyP84ZFEYYbTpPV5CR50kivBPAVNJrmQItSNB1Esj3F6YPPDhOqaLK4CA7yXkiLogCy8vP2/oA922d3++0QBEEP7AB0l7dfLcvyc51sezvwMZApy/IhQRBuBF4EtIAD+Jksy1sub7sNiAGuqKrpsiwrVwnXMBUVFURGRnq1PCZJEqtXryYwMJC33nqLS5cu8eSTT3bbAzAnJ4cDWz/glgwnocbmSMDwpAayjr3O9jITIybcTt/k5HZ2R44cYeXKlTz++ONtIsc6nY6HH/spkiTxj3feJFJ/kZF9rEQYm3OtzlaEcOHzi0Qt/6zLyGrQsXNw7BzFfeI4c9dIBLublL37Saho38+2M3R2GwOP7OP4pEkMNhd5bNcaX4oNzXIDlvgYRSIYQGhS1vMZwC4od6hqjYgkCF0+aHSGyd7EpWJlPZu95jqYPnQt4PfZvlNbW4vBYPB6GtfOnTvJyclBFEXWrFnD4sWLGTRoUJc2TU1NfPDB/3HjjYNISooFYMCAKAoKqlmz5l1iYlIYM2ZcO7uysjJeffVVZs6c2a5jwYJFzR1dPt+wll2nj5KZAklhzSkO5fUmzhZrGNu7rMsJccF6O5OSypprNc5F4Ha4iRfLmRBQ7PH1EAWZAYbznJCTGGwu8NiuNWFCFY0WGYOCWmtBEJBF5e3bdBq3YltZbfa6U9EVwjR1XKp3KhLBAFrdt+RHrwOf7ckjkh24QZblRkEQNMAuQRD+LcvyvtYbCYIQADwB7G/1chUwR5blUkEQBgJfALGt3r9LluVDvn2Eaxer1crKlSsJCAigrq6O6OhoZs2a1e3M78LCQj777DPmz59PbGzz5bp06RIrVqygpKSERx99tMMUh7+//b8MjK1n4di6Nt9LUYDBvRsZFNdIduFyPt6lJ23YzQwaPBS73c6bb76JRqNpif52hCiK3P/QDwD4aOVyNLZ8zE4X5te2EnPBc1Fozi/G/Lti6uaPJcQLAdwalV35U65e5cbpVqHBe+emx461V9f9f7tC1eT9KOgrOEXl3TaCtY1YNUZMDu8L69SSC3vtt7i01sMrja8R/D5bIbIss3btWurqmjvFXGlb2N00rsbGRj744AOGDx/eMuHT4XCwZs0aVqxYwZw5cxg/fnw7uw0b1iMIl7jnnjHodG3FWmJiMImJwZSVNbB27XKMxmimTm2eKrl69WqOHz/O7373uy47Ftx081xgLrt27WD3zq0kRIpEm6xMTvL8N23UupiQWMbus4H00XsugFujF924JBF1N20nOyJU3UBRvZuIUGViVhB8EMEKxkhfISgsBrusQS94XyVuFq0UN9gBk6JjK4maK6aH++xuRbAsyzJwJYSlufxPRyGl3wF/AH7ayvZoq/dPAXpBEHSyLH/LfZe+fY4cOcKBAwe48847W3LDSkpKWL58OUFBQcyePbtdlEGSJNasWYNKpeLxxx9vE7UMDg7mhz/8IRaLhQ8//JCcnByWLFnCgAEDyMvLY8/m95k37Ovob0cIAqRFN5Ia1Uhh9WpWv7uO3SdqePLJJ71a7l5w5z3Y7Xa+nLMQkxcCuDVOHyYMqZS0nrhMuKoOi9tIEN7niInIYFDuUNU+TPBzqbxvmXSFMFU9jaYgRSK4MCaNwIgoxcf28+3j99nKKCsrY/Xq1cyePZukpCSgWdyuW7cOu93OnDlzCAsLa2e3e/duzp49y913392mtZlWq2XhwoV873vfY/369fziF79gwoQJ3HzzzVitVt5//22mTRtInz5dP1jHxAQwd+5Aqqst/PvfK9i37ywTJ07xqmPB+PETGT9+Il9++BwpCcp6z8uCcp8drGqk2hVAlLbOa1ud6MJqdYHCHFtfRsDrNbLiIsfI2CQaCw3otQo6LIlgtyvr7lRT68RqUR7B/m/Do2QZoflR6jCQDLwuy/L+q97PAHrLsrxeEISfdrQPYD5w9Cpn+q4gCG7gE2DZZefdo7HZbKxcuZK+ffvy6KOPtnkvNjaWBx54gOrqalatWoVWq2Xu3LmYzWaKior49NNPue2224iLi+tk72A0GnnggQdwOBx88sknfPDeO8ydEMKd4+o8XpUQBEgMbyLE7MCmHqq4Y4FDr3yJ3iUrd6gah0PxWOYwsY4aMYggSVmhhMqg/LzVFuUVv4bgSFyXVKi9jGDLQIEzlJKwWMa6XUTUe7Z6bVfr2N1vPBE3L+LGkd7lNvpED19au1bw+2zPkWWZdevWYbVaeeyxx9qIJrPZzKJFi7DZbKxfv566ujpmzpxJbGwsTU1NrFixgoyMDB544IFO9y+KYssQnK1bt/KLX/yCMWNSufvu0ej1nuerhoUZuemm/hQX1yjvWOBSLghRKfd9ZlUTBfYoRSIYwOlQvvon+tAi0myAuro6goODvbbt3bs31TlGwrX1XtsWWiPIzakkMMhA36RAj0S4JMkcOVFL7SUDc+bd5fUxFdPDfbZH3w5Zlt3AUEEQgoFPBUEYKMtyFoAgCCLwCnBvZ/aCIAwAfg9Mb/XyXbIsl1xekvuE5rFU73Vg+zDwMEB8fLwnp/udcezYMfbt28eiRYu67NEXFhbGvffeS0NDA2vXrqW0tJSkpKR20d+u0Gq1LFq0CDVNZCaeUvQ9NGlcNFxSntLn8kEEu2UfcnMbG2jARBDeRzZNWCmUIhUfW6VXfiPQ2pRFghsbG2l0WdkdOY+0ql1ESZ79zeoFM5sbU8mc/wNGJSaxfePn7Nv+OcNqcomt7bw6+0JMKseSR3PzI090udT6jSOAD8EmP63w+2zPKC8v5+OPP2bWrFkkd1ArcQW9Xs/tt9+Oy+Vi48aNvPfee4SEhHD33Xdj9HDMuyAI3HDDDfTp0wfI9UoAt8ZsVr4q5JCUd5nRa0WcsgqN4H2U0SDYqXN6dp06wuVQvvqn0SgX/qEBbkpKSrwWwZIksWPTaoLdoSCqSO2iG0ZrXJLIlvIkAvvP5Xvjp3D27Bk+/tdOBqSH0T81pFN9UFPrZNe+MsaMvYkRERFenatPXAc+26tfoSzLly4XR8wErpTYBwADgW2X/0DRwFpBEOZeLrSIAz4F7pZlOa/Vvkou/7tBEISVwEg6cKiyLL8NvA0wYsSIazbqsHr1asLCwtpFf7siICCAO++8k1/96lfMnz9f0XFDQmOwOrIx6bx/UlarZHArz1N1mZQ7Y7cPqzWm2mouumII0novgkURnLJacW9DjULdbw0KRBgSxar//Q3jb3uAuN6eTQPasWMrDkc5CxcOR6USuFDYn50nz5FQsYd4V8f5eTJwzNWXoqBM5j/8cIvjnDTzJph5Ewf37OLA56sZWpNHYlVBy6Wwq7Xs7TeekFkLuHX0WGUf1Fd6eFThWsPvsztn7969FBYWtov+doVarWb27Nnk5eUxZ84cjwVwa2JiYjh9+gSJiSFe2wKYTL6swCl/qA0z22mqMxCs8r7AVxDA7cOigdsnEaxMpbkkOFtqoPLSWuy2JoYNH+mRXW5ONie2LmdGUhlmtZ0aRyC7q9MJFK0MNBV0Wih3wRrB3po+zF3yo5bvVVpaOmlp6RQWFvDRmk2kJAczdFB4cwcImqO/x07WUlWjY+68exR9Tp/p4T7bk+4QEYDzsjM1ANNojhAAIMtyHRDeavttwE8vO9NgYAPwtCzLu1ttowaCZVmuuly4MRvY/A19pu8Eq9XKlClTFNm6XC4kSUJUsL4fE5dIfZ1GkQgGMOmVf4HdRuUiWHDJijsWGJoaKbGYSFboz10KM/nLpDCstTK5c24g8YtdqD2cyFc8YwzBwwOYyXFkSeD06nx2y3EMu/lu+qWmdWjT1NTEunWrGD++D3FxKS2vJyQGk5A4krLSdHYfzyWy9CD9nOda3m8QzHzZ2I8Rt/2Q4Ul9Otx35tjxZI4dz6mTJ1iz+n361+RjFOB48ihufvSpbzf62xqBHl9kcS3g99mekZeXx+LFixXZXhlqcaVw2Rt0Oh1Wq/Jpnkaj8t+npDIo7lgQbrJRWxOgSAT7itupTARbbBIVecVUo2FwqkyI2bP7ZGGVkeJLBsYNsKNRQ9HFrXz6wRYi4kcxfkLH93lZllmz8q/01WZzS3JlyzUO1dYzLqaeeqeRfTXpaCQHw8x5Lel8LllkS1ki5tTZLLyj4zSXhIREEhIe5uLFi3z86WfE9zaS3CeUfQcrGDl6BsMyu25/+h/jOvDZnkSCY4Dll3PMROCfl/PIlgKHZFle24XtD2nOSfuVIAi/uvzadKAJ+OKyM1XR7Ez/pvRD9HSCg4Opra3tsOiiO2JjYykuUROjcEKi0QcRLPkQCdZX1mAzmDBavHeoarcLm03ZeWc1xFJYoiKwdxL9hPMeBYQlYO+lNMJyKxh/fhsulZqzczKpchvo/eUedJ10fbAFBlC6ZBKZoUUEu5uXwwRkBnKeAcJ5cv9dyMfrY+k36Q6Gjshssdu1awdWazG33z4YdSeDLmJ6BRDTK4OqqlT2HM0lsOgoLksTBYEjmP/wox6l1QwYNJgBg/5Ifm4uX/773zzy+OMeXA0/PQC/z/YAg8FAU1MTJpP3FfipqalkZ2czfPhwRce2+9DhxmhUHgkOConC7j6NXu39UpxJ4+SCW3mXGqVaqdIaQN7ZGvSBwQwdbEaj9sz3Z52uw1ZwjimmcwjI5OXFccIZRmpfgeiQjkW1S4I950JI6iUyrv/XqfC9I5z0joCKSwdYt2o/2uB0ps+Y2+Jn8/NyObr579zYp5wAdce1H4EaC2OiLFhcOg7VpCM53URpazhQk8ScJT/26HsYERHBgkUPUl9fzzvvvM2PftRZOr8fT/GkO8QJIKOD13/dyfaTW/33MqCzElZl3uM6pFevXlRUVCgSwUFBQWT7ULdt8iHHVTYra98CYCy+iKVfiCIRfGbUKCSNipP2PgzU5nsU1XBJIv86k4x1wzk4Vci+yGCy7h5Bv0QnA1S5zZ0fOqBCCuXMhTAyTu5Hb2sWu2q3i4E5R5AEkZwbB1EkBhCz4wimqpoWu+LpowgaHsQ04QQdpdAJQD/5Av3EC1zYXcCn23oTMvRGyqtKGTs2kfj4VI+uRXi4kfE3DqasLImjRxu57abZHtm1pk9yMkYFhR//EXr40tq1gN9ne0ZUVBQVFRWXc3S9Izk5me3btys+tt2uPB/MYNAo7lgQ1SuJxsbd6NXe1yc0p5Ipy68tcPXGEZ7MXmsgmeojHrVKkyTYlJvI6c8raNz2CYVaNScfv4m0G/sxLCMQfSe9cG02ib3b8xmoySbc/HUnjGR9MX11xRQVxbAzL4z43loSor6+eV6oNlBUa2RMfwcadcf3g6hgJ7NHQ23jaTauPo1Lk4jT0kCS+gy39qvw6F5kVNsZGVmEw63io5w07v7+s90bXUVgYCDh4ddI154e7rP9E+O+IVQqFS6Xq9sewB2RkpJCaWkp/fv399pWEARsTuVC1oegAuaYaCS1GtHlXVRDAmrGpdEQoEXldhNS6Vm/4CZTAGemj2NY4AWCpSLqrYHstaeg1coM0+Z02i3iTEMv9m8XET/dguBqvvmIlZew/GkzRwOMZN83mj7JEoM1eS3dFyRgX10aITkVjDm/o8P9irJEav4JUoDzY9LJ0w0nMCufhinpjAgvJsR91qPPFS+VEa8qY3u+nvn3zVNUyBEebsTprPba7pqjhztUPz2HqKgoKisrFYlgtVqN3a48+mC3K+8/azRqKS0tVZSKERcXR/URLeEm70VwVlko1U6J845eJGpKPQw+wB55PElDU5kSq8fhiOfEqT40lRWRqTqIvpOJbFX2AD7ZEkz16v24qi93V3C4KHppLUWviGQ9Mp3UWamMGBGCyfi14z91ug5LwTkmms4hCu2FrCBAvK6MeF0ZZRcj2HkhkohoAxcteuKjVYzr71kHnxCzm5mZcPFSIZcKy+kX6H2BuVblxvQdZZ59o/Rwn+0Xwd8QERERXLx4kZgY74cppKWlsWbNGsXHtruUi2C9VpkzrqyspK4oi/L7byDwiyzMhZ4J2Ya+cdinpjGl+gT6Kis5/ZI5nNyP3hcKiSwu6NTu7MiR6FLNTOEEwuVTDpTrGeM+gcVm4KCjH7JaZIQ+mysZBC5J5LMzyVg25KA61fG+xQYLtj9vIUuvJffeMcSnqonTVpN7IZihJ/djsHVfOCgAfQrPkAQczBzH1JCTiG7vc50jXRcvz4v3XgRrNCosFmVt364ZroP8Mj89h8jISE6fPq3Y3uFhTUBHOJ3KI8HBwXpOnjzttQh2Op18svp9UhPikWvKSQv1bGCGxaFmf0EIg8OrGJhUz0WriR0VA4nR1tNPd6FTMVzoiqUkZCRjM6NbitO0WpHhGeG4Bodx+kwSl4qKGMZ+TOqvr+WXuYmc3FBJ49YtHe9Ykih7cyNlb27k1OLJpMwbxOCMEE4dLmCA5hwR5pqO7a4iRnORGM1FzlXH0T8tnrBA7/OOwwJdXJSVd9zQCsq/Q9cE14HP9ovgb4grS2tKRHBQUBBNTd53OriCXeFsdJtLRUG5k10vvcSSJUuIjPSsddiHf3+bmJJjLGk8gkp2c+HWRHJsA9DvzCPoVH6HNhJQNn8SfYObSC7/umVpv+pc+gEX4npzKGEyMWWl9Mo/15Kr22QK4MyNY8kIKiZE6rgbglG2MtJ9ErtbyzFXP2wqLUHueo7sEhDWbEVwdh+pFm0OHH/dzjlRpPqxCUw77f1SpwAE2CydplZ0R7iznPp6O0ajMqeqUe6LrxF6/ghOPz2HwMBA6uu97+F6BafCYq1mW2U+QpJk8vKq2LBhJ7169WLAgAEe2e3Zs5vykmPcfUcfTEaR6too9pytxuwqZ2BYWaeraFnloditbibF5Lf8NCMMTUxKbKLOrmVH+UDC1U2k6wtaIq8uCfbK40gYksq4uI7rRtQqgcEDg5H6B3E2J4HKghJ6W7PYvNNA1eoDuKo86yd8ccU2Lq7YRukvZ3NXZk6H0d/uiNLUUN2UQFig16bNQy0kH9pmisq/Q2q1GqfTieY7dfw932f7RfA3RGRkJCdOnFBsrzSqsGXLFrILG4gOCiAjoRGV6JkTOF8dxObjIkse/BlOp5NXX30VrVbLnXfe2Wlvz6qqKj77ywvMlM4T01DU8npCQwEJFFA2NZazk25EOFxK2P5TLe83JvbCOr0/E2tOYLjY8TJc/KUi4imiMjSCw7FTiKi4iC3IgCYtgCmcbIn+doUOB8Pdp3C5Vawr7Y/4kffF66IkITX5MJbZZsOGFj3e/z1DpDrO11qJjjYrOnZnRXR+/Phpj5Kc2tYo9dmFhYWcOHGWqCgDY8YkYTB4dhuuqbGxYcNxxo6dwcsv38pbb73FRx99xJw5c8jMzOzQRpIk/vbWq0wYHcOYWbEtnzksRM24MVHUN4Rz4Ewsams5wyKKW8SwzSWyNz+MQWFVhJs7flAI0jmYlFCCxaVmV+lAAgULQVoHpcHDGTUyBp0Ho3tFUaB/aiDpKQFs+reR8r++6tG1uBprST10fAm6xaSykXNJppvBfZ3iknwYy6xSfq+JiIigsrJSUVqMn6/xi+BviPDwcKqqqhTZ7t69m9zcXFatWsX8+fM9erJzOBwsXbqUoUOH8pOn/4fa2lpWr3uP3kG1ZPZpQKPqWAzbXSJbTgUhBw7h4cfnAc1V0r/+9a9xOBy8+uqr2Gw2vve975GW9nX7rlX/eIfIosMsqT+CWu54KS+mqYQYSqjKjOR05jQcp2pwhZjpE2qhX/n+Dm2uJrLxIpGNF6k1BnIhPZ2hsmd5ta1R48aglRSM0mjGh8JtAutrqBHD6eXhQIvWqJGwWZXnGep0PkyDAsXFNt8Y18HSmp//DgoLCykpKeG1117j7rvv7nI4UmveeOMNnE4nv/nNUkRRZN26NZjNLsaMSSIgoOMCDUmSOXSohKysCu6//7GW17///e8DsGLFCv71r38xdepUpkyZ0vIb3r9/H8UFh1h8e1KbvNnWBAaoGDMyHIs1lMOne+Guq8Qs1mGzym2iv11hVLuYGF+MQxLZ75zAxPHeizJBEAgMVt5tqKmgBqukxaTy3n+qBQmbVXl6ihsfxjKrld9sruS0f6ci+Drw2X4R/A0gyzKbNm0iKyuLN954g0cffdSjnr8Wi4XXXnuNyMhI3njjDY4ePcovf/lLUlNTWbRoUZs59K3Ztm0bGzdu5Mknn2xJvwgNDWXhPU/R1NTEvz5dTri+gtF9mzBov/5xF9QE8eUxgbvu/0mH7Vi0Wi3/7//9PyRJ4s033+TDDz9k8uTJ5G/fwAzpPL0aLnh0PcKtlUykkvKB0TRYILkyxyO71oTY6rngQ5t9vU65sdPZPHBCiRw0N9Vx3pVML1HZJD6n7bsRwSEhIdTW1hIaGqp4H98IPXxpzU/P4fjx4+Tk5PDb3/6WH/3oRwQGdr9IqNgEAAAgAElEQVQeLkkSH3zwAQUFBfz1r3+lqqqKP/7xj4SEhHSZUlZUVMSrr77KXXfdxbBhw1pev+22BbjdbjZuXI8g1DFqVBJhYV8P4KipsfH55ycYNWoa998/r8N9X+l1vH79ep5++mnGjBlDeWk+40ZFc9vNcR492BoNIiOHh+JwBrP7q3NMjjjZrc3VaEUJNcpFncGkfFnfmldJg7uvIhEMgNuFUjnkiwg2aSUaGhoICAjw2jYqKoqjR48qPvY3Rg/32X4R7CMXL17ko48+Yvr06dx0002cPn2a3/3udxgMBp56qvPBA3v37mXNmjX87Gc/a3GcGRkZZGRkkJeXx29/+1vi4+NZvHhxi3N2OBwsW7aMQYMG8cILL3To3EwmE99b/BgOh4P1n67AzAWGJdk4mG/EZRzIw4/f1u1nEkWRH/zgBwAs+/mP+JlzHxrZe+cWZq2ixNDxMAhPcMmi4sluOq0P04lqbTg1OrRO7x2q1mGj0akDhV03XDbPqpM7QqdT/nMWRZGampprQAR/t4f3c/1jt9v58MMPSUxMZOnSpVRWVvLOO+9QVVXFk08+2aWQ/fOf/8z8+fNZsmQJ0JwGt2zZMurr63n11VdRq9UsXry4TUrZX//6V6xWK88//zx6ffs+uyqViptvnocsy3z11SYslnwyMxMoLq7j+PFSHnzw+x59rtmzZzN79mzee+89pk3sRd9E73v6ajUiap1yMSr5kCdtDtA1tyuyeO93XaVV1DoGEa1V1iFHkJSLYFnBkKsrhOrsFBcXk56e7rWtIAjU1HhWBPgfpYf7bL8IVogsy2zcuJGamhoeffTRltZo/fv357nnnqOwsJCXXnoJp9PJj3/8Y8zm5jxPq9XKa6+9RlhYGH/84x873Hffvn158cUXKS8v5w9/+ANhYWEkJyeze/dunnjiCXr16tXt+Wm1Wm5bcD9ut5sXnl/Kj37ScfS3O2ISk3HmHUbj9l4Ea2QXNpXyHjAulwgK/bFOo7wFkbuwkqb0ALR13jtjAXA7JcUi2G1VLoIlSSI7O5vUVM96DEPztMJVq1YRGRlJcnKy4mP78dMTOHHiBLt372bRokUEX+6NHRkZyY9+9CPq6+tZsWIFRUVFPPTQQy2t02RZZuXKleTm5vLCCy902AYzMDCwJaXsf//3f7FYLEyZMoW1a9eycOFCRowY0e25CYLAtGkzAHjvvX8QGhrusQBuzbx58yjK+9xruyuIah8m0rmVdzsIDtagiYvEea6o+42vxiVhdSiPyIqS8nQIPBy53aGp6Gb/3j2kpaV5lYq2adMmKioqWLhwoeJj+2nGL4IVUFVVxapVq5g2bRqzZs3qcJuEhASefvppKioqeOutt6itrWXChAl8+eWX/OQnP/Goi0R0dHRLlOG5557j5Zdf9jpnU6VSERoe1WlqRXfEJvWlsTgIo8X7vpIAsg8rJW7lOhajyo0kioiS9zsRii9SNzqRkDplOd6CQ3kUuqm+AZvNhV7v+U/T5ZLYti0HjSaaL774gg8++IBbbrmlzbJrR2RnZ7N582YWLFhAeHh4l9t+Kwj0+KU1P9cmDoeDlStXEh8f35JLezWBgYE89thjWK1WVq1axVtvvcW0adPYvHkz8+bN46677ur2OFqtlp/97GdIksRDDz3E66+/3mH0tzsyMoZz6tSp7jfsgKCgIHJsytMSRI1yESy6HIprC8wmEfOAeGqViGDAalfuO3xJ4xBEDTUOM6Fazwc/yTIcq4qiwJVOcGgYzzzzDOPGjeOmm27qMpWypqaGDz/8kEmTJjF9+nTF5/yNcR34bL8I9pIrT2CPPPKIRwVsUVFR/OQnP6Guro5nnnmG119/3etjBgYGEhwcrLhoKSoqioKCAkVN4dPT07l0IJhIS7miYws+LJXILllxRDVIaICYECjxfnlMbLTRpFH20CADTU4BFyJqPBfgNkHPTtUwIlMm8OWXRYSEiGRkxGDqJk+upKSenTvzmD17IWazmUmTbkCSJJYvX86nn37KtGnTmDhxYpvvjsvl4p///CehoaE89thj320x3NX08KU1P9ceWVlZ7Nixg4ULF3qU7mMwGLjvvvtwOp08/fTTvPjii14PQRJFkbi4OCQFD+HQ7LM3btyoyBbAYlMe2dRo1XjhutpgFi04HDK6Tqa5dYVOK2KID6W2+007xIdMMuwONxa7iFHn+QeXJDiUa8Kh78dxuT/u0mMMDisjUtf1J2hw6dmUF82wG+/ntr7Nq2+33HIL27Zt4+mnn2b48OHceuut7fTFV199RUlJCQ899FCnaZbfCT3cZ/tFsBfYbDZKS0u59957vbYNCgpS1EP4Cr70pIyNjSU7O1uRCO7duzdHBeU/OJWCvo0tuJTbBkoNkBSlSARLei1lAWGkiiJqL25idUFhbEseQca93+fTrWuJseQyUsxFS9d/u3xtMlnGDGYveQy1Wk3G8JG43W42bfocg8HO0KG9CA5u+zTgdkts356LKEaycOGDbd4TRZH77rsPgDVr1vDMM88wYcIEZs6cSV5eHps2beKOO+7wuC/0t8Z1EFXwc+2xa9cuHnvsse43vAqNRkNcXJyiKaDQvJJXUVFBUlKS17ZhYWE+5XtarMojm2qtBhQKyjBNLY0WNzqd98pIEAT0wcbuN+yEqlotlt5ajGrPUzLskpqvypIIHzSPL7OK0MtFjE6HIFPXDxFVDVp2nzYw8ca7Wj1YzWbf7u0cPb+LgWEVxOrbriTKMhyvjiLfkcr8Rx5pF3yYPHkykydPbimQT09PZ8GCBdhsNj788EMmTJjA1KlTPf5s3wrXgc/2i2Av0Ov1qHzI//GlqbXb7VY8ljkmJkZxFakoitgE5eetUhpSAHQut9cR1Ss4ZA2qwb1x7zrt1YOqMHEAfdJM9D+5m5PJw3FoNGScPYC2i5xoGTjadyhVY6ax4N4HEASB1LQ0LBYL6z/8P0JqzzBKlYeRtiklNkHPLlUGseMXccuQjDbvqVQqZs2agyzLbNmyCShh8OBoIiKMlJY2sH17LrNnL+i2qvi2227jtttuY8uWLfzkJz9h8uTJ1170tzU93KH6ufZQUgtxBY1GgyRJHnX7uZqkpCTKysoUiWCVSuXTvcZiUR400enUuCygpO14qL6JinonYSHe3zNcLgl1VACo1eDyXMTr+kSTNLovA9/fyKEBKVgHxzNyaBUh2q6nfeZbojhQl8yt9z7VkrIiyzIbP/8Ud2M2I9NFIq6aIidJcDjPRKPQj1sWzG63z9HjJsG4SRw/dpjjWZtID64i0VhGk1vHl+djGHLDvcxPTunyvFoXyD/33HPExMTwgx/84NqK/ramh/tsvwj+FvFFBEdERFBVVUV0dLTXtpGRkZSWejbWuCOsPohgjSdTLjpBtNvJkRNJFzqeQtcZh+ypaAsbmVV+kBOPjaWszI3zs/2IXZyKpNdiunM0mXX5RJzJAmDIqf24VGqyU4bSqDcy9NwhDI62IZK6wFC2JY9g/OM/ZdxVQ0aMRiO3P/AETqeTDR8tR19+lFGa8wRJDZzX9uWkIYOblzzW5fdCEASmTm0ultmzZydbthwhIiKBRYse7NSmI2644QbUajXx8fHXrgD24+caIyIigqKiIhISEry2TU9P58CBA4qP7YvoaWpSXqAWFKihrsJImL77kfFX45ZF8vNrSOyt9+rBIfd8A5XFZdwyzcWJz+7h7K5KCv53U7edIqJuHU1/Sy2xX34FQNzew8j74MywwdQPiSNjWD1RhrbDPpySis3lSYQPvo1FYya0eU8QBGbd3NxBaduWL7iUdZThqQJxYU6qG5ujv+On3klYWFiX5zVk6HCGDB1OzrmzfLztIyR1MAsefNyra9K3b19++9vfsm7dumtXAF8H+EXwt4gvX+TevXtTXl6uSAQbDAbcbuU5Yk2y8q9Jo0pPeVA00XWe5xRLwJ64UfS6WIGpqIGd8amERjoZoOpaDNfJZg5XJTD07HECGprzssbl7sOqM3Li0dEU14g4Vu9DdLVVw8K4/iQOCGBY9k5UV6U/qN0uBpw5hFsUyU0eTI05iEF5xwmw1HOs7xAqR01lwf0PdSksNRoNtyx+EEmS2LhmFXW5hxkweRG3ZHRfMd6asWMn8P77BUyZcqNXdle4Mto7MTFRkf23Qg/PL/NzfREXF0d2drYiEdy7d282bNig+Ni+BE0am5T3G3c43eTVxzJe711/9+PVvZD/P3vnHd5U2f7xz0mb7k13C4XSAZS9ZINsUJmy2oKILHkRRH1/IEtlyFKpDLEsRaCggLwsQVAERDZUlrRQiozu0tI9kub8/iitLV3JCdAWz+e6cgEnuc95EpI739zPPQyNaFPjMud+jUW0cKZVK9dyJ1mq1RpOnrqPt1Mmbevlr7m1n5oW9e242nUUf51OJHL5L2iS04rZGdV2xLO9D43OnMU4tfh9ggjOF6/gdPEKd8/VI7RZbeq3yMXDMpG/sxw5+8iLAW9Mq7BgsUvXXkAvzp87zbEjv+DiUY/+Q/vp9Jp4+9QjO2cQeXl5knYTTE1NycmR/n/5XKjmPlsWwc8Rc3NzkpOTsbW11dnWx8eHqKgomjZtKunaUh2qRqMhKkNFtsIYE432H8ZshRFnnJvTwCSaHEs7TmZ74RYdRZ3EO+XaRVm5E2nqQvNr5zB+3Ke3xsMYUqzs+KNOPUwdFTRVhpf43F3M8cXwXgYdbh0r0VrYNCeTlyLO0dxQyZVxLbmXbkT2jnMgipgHtKVlaiSON8qvxDbQaPC9+ScicMfTj6PerWg/5b+000FQKhQK+r7uz+bNeTTWUQAXYGNjI3mohaOjI+Hh4ZKu+1wQqv8cepmqh0KhIC8vT1J6gYuLi+QCNYVCoVcthz5Bk8SHaSSl5GFnrf1z1mg0nLmUioOdMX7t/Dj1lwtmWfdpbHmH8vRbutqICwnuNHFNxdbkEQAvuTwgJy+G0GMx5Bg50rpNTYyeGKN8++804u7H0N4nE8MnlmmggKbeuTSua0VYp+FcPZNM5KrfUD14iGP/NvhlJ+N2+Jdyn48A2F8Lw/5aGA9P1+FCOz/cBw5h+NAuWr8mAK1at+Xe/WjatpeWj+vo6Miff/4pybbK8xx8tiAIG4FXgXhRFBs+PrYMeA3IBW4Db4qi+OjxfR8CbwF5wBRRFH8u7/yyCH6O1KxZk/DwcNq0aaOzra+vr+StNZVKRXx8PDExMToV5125coXNmzcTGDiOTT/9iK86nlZpYZirym8FE2bjQ7qNFR01VzB4HHWtqXxAnKcTf7h1wD4uEd/Y4uOQNcBpt5dwSoyn7e0/SpzTOjWJ1pdPkWlqybm6fghOxrQwDiNTNOFiYh0ah1/FKrX8IjilWkWL2xdoqjDgr9GNSVBa0v7KCQx06BEpAJ6R1wm3d6dWJURUC0ZlShHBNjY2pKSkPINVPUWqeVRBpupRo0YNHj58KKkQ1MnJidhYaZ1xIL81m1SSk5M5f/48rVq10slm0aJF9O7dmz2HInCw0/BSMwcc7Mv/qo9LyCUsIp1WTawxM83/ELZv60haRg3OXa+FQXI0LaxvlRDDl5Nc0SiM6FQntoQWMjbIo6VzFGpNDNdPxpIi2NOqjQdGSgV/nH6Ap0MmbeuVX4GnUECD2rnU9zDndsdBhP5wj4b7j5aI/laETcQdzD1q0qZ9F53sCnB0dCQuLk5Scbm9vT2JidLabVYLnr3P/hZYBXxX5NgR4ENRFNWCICwBPgSmC4LQABgO+AGuwC+CIPiIoljml7wsgnVEFEXJfRDd3Ny4cOGCJBGcnJzMzZs3yc3N1SlCcP36dTZu3Mi4cePYs2cPd+7cYdy4ceUORtBoNCxduhQHBwcWL16MgYEBTZo0IScnh29Xfk7tzChaZ0ZgnVO8ejlXYcgp55bUN4mhnvpaifM65cXhZBhHUi07Tju3x+JhOn73LxNr5UaEqRvN/7qASW75/YjNstJoce0MOTdNOOfdFI0o0P6vktHf8jDQ5NEoMpTLtZvqJICLrSNPeoTHwMBAcpGjk5MTd+/epV493SfxVYtc4OqwRplqhaOjI/Hx8ZJEsJmZmeRobm5uLpGRkTrv/iUmJvLll1/SunVr7t27x65du+jevTvdu3cv12779u1EREQwe/bsx1NGuz4+vgUjRQqtWzjh7lx8R1Cj0XA2NJUaNsZ0blNyjZbmBrRt7UBWdg0u/lUTdVIMrazDyNUYcDa+Fk3c0rB7HP0tC0OFhiaOMWjEWG6cjScq04qXm6lQ6uD+BAG83HK5Y6DWWQAXoMzOkVzkWBB8kCKCDQwMJLfKqxY8Y58tiuIJQRBqP3HscJF/ngFef/z3/sB2URRzgDuCIEQArYHTZZ1fFsE6YmVlRXp6uqRZ34IgcO3aNZ1F9Pr160lOTmbYsGF89NFHeHl54e/vX+4ADLVazdq1a8nKyuLzzz8HoHnz5mRnZ7N9+3bWrVvHsGHDSgxUuHbtGps2bWL8+PF4e3sXu8/Y2JgJH8xEo9Hw7ZqVOD6MoFXuHRwy4rhp402KjXWx6G9Z2OUl0c4giVRnK47bd8E17B7tbp/U+vUAMM7Npmn4Ra57NpE6WRmVQvrb31QtPcJTEBmQWuR49uxZydeWkfm34eTkRFhYGA0bNtTZNisri4yMDLKzs3UaenH8+HEOHjzI5MmTWb58OZaWlowcObLCz/z+/fv5448/mDNnDmZm+e3CBg0axM8//8zMmTNp0aIFgwcPLmaTkpLCwoUL6dmzZ6kTxIYPDwTgwIF9nDxzh9bNnalT04jEJDV/hafRquk/0d+yMDVR0Lq5HbkqG/687kZq9H26eN7XaSdcIYj42ceRFG2C0lCi1zaX2DgeMM3KJjIyUtJkTCcnJ27cuCH52jLPlDHA94//7ka+KC7gweNjZSKLYB1xdXXlzJkz9OihfXGSKIrs3buXnJwc3nrrLdavX4+Liwu9e/cuNxoYGxvLZ599xuuvv14YPW7ZsiV3795l/vz5uLq6EhgYWDj+s4AbN26wYcMGxo4dWyJiaGJiwujRo1Gr1ezevZvvv/+e3r1707lzZ5YtW4atrW1h9LcsFAoFY/4zFYDt327E4MFl2lvH4FNK9Lc8rDSp1FPGYJgcp5NdAUq1imx9RnwqpLcgMtFDBBf0D5Va5FjlCyWkIjy+ycg8RRwdHdm+fTtdunTRKS/4/PnzXLp0iffff5/t27ejVCrp169fuQEQtVrNvHnz8PPzY9GiRQiCwLx580hPT2f58uUYGBjg7+9fojg1KSmJ5cuX06pVKxYtWlTsPkEQ6N27N7169SoUyHXq1GH06NHs3LmTsLAwZs2ahbW1dbnP55VXXgPg999P8NvvZ2nT0o1ObXQbwmSkVNCyiTWnHsZITgXNb5spzfcqrKSLYPO4BG7cuCFJBFtZWZGamlrxA58RUnefnzlPx2fbC4Jwoci/14qiuFarywvCLEANbC2yoicpd+CALIJ1pHXr1pw7d47g4GDq169Px44dy31zxsTEsHPnTl555ZXCrZSCIrdvv/0WGxsbXnvtNYyNi3+4N27cSEJCAvPnzy8R8fXw8ODTTz8lMTGRzz//HBsbGwIDA6lRowbr168nLS2NpUuXlrvtY2hoyJAhQxg8eDA///wzkyZNYtq0afj6+ur0egwfPYY1C2fgrI7Rya4AK00KD2wcsU2TNidIj1EcaAykJzOZqnJ5+PBhha1ySsPR0ZErV65IvvYLTVV09DLVGqVSyeDBg9m4cSMODg707du33JSyrKwsQkJC8PX1ZcKECQCMHj2atLQ09u3bR25uLv369SuRl3/y5En279/PO++8g5tb8eCThYUFc+bMITc3lxUrVpCZmcngwYPx8/Pjp59+4sSJE8yePRsLC4sy1yUIAh06dKBDhw6EhoYyfvx4hg8fzty5c3V6PTp27ERMTCyuTgpJwkoQBDQK6Z0rFHqIYAMrE0Sk6S7j5EfERERIum5lClBra2vJxdDPBf1fm0RRFHWuFBcE4Q3yC+a6iaJYIAUeADWLPMwdKLc/rCyCJdC6dWtat27NtWvXWLt2LXXq1KF79+7FRKcoiuzfv5/MzEwmTZpUIgLh5ubG2LFjSUxMZNu2bRgbG9OvXz8yMjJYunQpAwcOZMyYMeWuw97envnz55OWlkZQUBAxMTFMnjyZBg0aaP1cFAoFffr04fz58zoL4AI0SnPyUEgajGEqZpNs5w7SxsUjiNJlsD5ZWpbpKVy9epUuXbrobGtvb8/Dh7pPstOXX375hStXrhAcHEyrVq1KpMJUCWQNLPMMqF27NuPGjSM2NpYtW7ZgZmZG//79SwQYLl68yPnz5/H393+cV/sPlpaW+Pv7k52dzb59+0hNTaV37944OTmxYMECfHx8CqO/ZWFkZMQHH3yARqNh7dq1rFmzhu7du7N48WKdnk+zZs1wdXWlffv2OtkV4OPjQ0raTWzLDx6XjYH0HThDPXrHWzoakWdsjKGE3TBlViaZ8QmSr10ZhIeHc+zYMe7du4eXlxc9evTQa4jKM6ESfLYgCL2B6UBnURSLNrTeC4QIgvAF+YVx3kC5HQVkEawHDRs2pGHDhty5c4cNGzbg6OhI3759SUpKYseOHfTu3bvCrRd7e3tGjx5NamoqP/74I5cvX2bevHmF+WDaYGlpyZw5c1i2bJlOArgo+lQxW9g7k5VqgoWoe3N1AzSojKR/qBV6iGB9PrwWKQ+5GxEBEkSwoaGhXn2bPTw8CA4Opnv37tStW7fCx6ekpBAUFET9+vVZtmwZgE67GTIyLwrOzs6MGTOG5ORkduzYgSAI9OvXD2NjY0JCQvDy8mLixInlnsPExIQhQ4agVqs5ePAgx44dY9q0abi7u2u9DoVCwcSJE5k7dy4DBgyQ9FxcXFyIj4+X1MO4Xr163Lx2HZAmZhUSR0kDKMubWlQBDq4GPLK2xCJedxEsiKDMljgPWk/atGnD119/TePGjWnbtm2F/jYvL48NGzaQkpLCZ599hkKh4O+//9Z6N+NFQhCEbUAX8tMmHgAfkd8Nwhg48vi1PCOK4kRRFK8LgvAD8Bf5aRL/Ka8zBMgi+KlQp04dxo0bR0xMDFu2bMHU1JS3335bp19sVlZWjBw5EkAnAVwUfXpS5uXlSe6lWauOF2nXbLBQ6y6CAQSl9LQEAz1EsLGoIU+hKDEgQxtMszJIi4nS2S43N5eQkBD8/Px0ti2gU6dOdOjQgV9++YUjR47QoUOHMot+fvnlFw4fPlwiZ/DJ3YzatWvTo0cPSZXTTxVZjMs8B2xtbRk1ahSZmZmFUd2hQ4dWmFdbFENDQ1577TUePXqkkwAuikqlkpzvWbt2bWJjYyWJYBMTE7KzpItRAz0GeRgbatBoKLf3cFnUsNYQ4+SARby0lmNmKu3HMRcgiiIHDx6UVAxfgLe3N97e3ly5coW1a9dSt25dunbtWqq/vXXrFmvXruWNN94o5tef3M0wNzenX79+5RbIPxeefXeIEaUc3lDO4xcCC7U9vyyCnyIuLi68+eablXZ9fUSwPr00GzZsSPx1C7TvQFwchVL6h8hQo7tTK8Am4xHZJuaYZ+rWckdtYMiF5l3QmJoREhJCv379ys3lK+DatWucOHGC4cOH653fpVAo6NmzJ6Io8scffxAcHEzTpk156aWXAEhNTSUoKAgfHx+WLl1a5nmK7masX78eNzc3XnnlFb3WJhkBuU+wzHPFzMyMYcOGVdr1zc3NSU1N1Ul8F1CvXj0uXbpU+JnXlawc6b7TUJf+Zk9gaZRLSoYltpa6i3BzE5FsK93FqAjc79SOPHc3NmzYQO/evUvkbZdGQkIC33//PT179sTHx0fn6z5J48aNady4MREREYUF8n369CncHfzmm294+PAhS5YsKTMg8eRuBsCwYcNK1BU9F14Any2L4BcIfVIa3N3diY2NlSSCHRwcuCtKjwwY6PEuNMqT5sgzTSyIcnMlurY7TneiqRNxTavsiNiaXvzZuD193plGNzMz0tLS2LNnDyqVqtRiGcj/f9m2bRuurq5MmjRJ0nrLomixzJ9//klwcDCGhoaEh4fz4Ycfat2ftE6dOowfP55NmzZVbiWyHAmW+Rfh5uZGfHy8JBHs4eEheZodQGaW9KCJ0ki607Y2ziI6xUBnEZyngQu3TMnq4sddYyXuv5/FQIvIbpaDPX/1fJk2777Dyx4ehWksBw4coGvXrqWmLIqiyKFDh0hKSmLixImSerqXh5eXF15eXkRFRbFp0yYMDAy4fv06I0eOpHHjxlqdo2A349KlS9y8eZNGjRo91TVqTTX32bIIfoEQBIGcnBxJvwgLOlZo+wF8kix9RLCBtG25ZFsn4p3sSbbvRO3793GNKX8kcwERdfxIaeRKV88oFApIbmrK+eu9sImMwyv8Cgqx5HrUBoZcbN4Z076DGdztn6b1lpaWBAQEkJWVxf79+0lNTaVPnz64uroC8Ndff3Hs2LGnEv2tiKZNm9K0aVPWrl1bbvS3PGxsbCRHpp4K1dufyvwLUSgUkocweHp6EhUVVaInu7bX1SfwkZEhXQSbmhiSm67AyFA3352tNuRSnBMqQwPyRAN83bK1SouIfWRE2AMl7ZubY9TCmtzh7pza3xDV8XBqHj+DMqtkrq8IPOjYltRe3Rky7q3CH/YFaSwajYajR4/y66+/0q5du0IRmZiYyPfff0+3bt3o06ePTs9PV9zc3Hjrrbf47rvv+OSTTySlQjo7O3P9+vVKFMGVc9mnhSyCXyAcHR1JSEiQlJ9WsLUmlUyNtLfSA2VNriaZEd+wMy1jwrF7WPGIUg0C1+q1wMjLlB7WNwGIrm3HqftdcI2KweNeeKmfyywTcy41boVf8ywamP2Tz2trlE2XZtFkNlJy7kZPLCIf4nMjFMPHUeY497r82aQdvSa/h7m5ealrMjU1ZciQIahUKg4ePFZjrmwAACAASURBVEhsbCxKpRI3N7enHv2tCH1yxAqma1WaCJaRqWbY2dmRlJSEvb29zrb169fnyJEjkq+tjwhOz5Rmm5Kq4eLlOC5k2tO2bh4elg+1CgZGptQgOrcGHduZoVBAeib8ccMAa7M8GtTKxrCUcpQ8DVyMMMHaxoKX2/zzHWNkqKDLAGfU/Rw5e6QBGUdv4nb8HCYp+b18s+1rcL1nF16a+g4edeqUuh6FQkH37t3p1q0bp0+fJjg4GFNTUxQKBePHj0epR96zrnh4eJCQkCApv9vBweHFHsv8jJFF8AtEQaGEFBFsZmZGZqbuhW0ajYYFCxZgolZiaNeMVsItLDTpFdqpMeC42IjUGq0ZOzm/IHDr+rWYhP1Jq/hInOLulmr3yMaBa/Ub0ab2XcwU/wzZcFUm4eqZRFItK07X6opDVAJ171xD8biT8O06fiQ3dKNz3QdlRh7MDFV0aRRFrp+Cs749UN5+RLahKUa9BzKoZy+tXo+ChvoajYbMzEytcoWfNgqFQnKRo5OTE9HR0ZIiU/ojVPutNZl/H46OjsTGxkoSwY6Ojjx6VP7Y4fKQKoK//fZbIu8ksPewCS81d8DJvmIpIIoil29kcv7Ph7w1dgoKhYJjvx3lj4u/0dJTg49tQqkf32y1AedjXKnrY0dHx38eYGEGnVooyc5Vcua6IWZKNQ1r52BkmO+z41OM+Ou+krbNzTExKt1pGyoUtO/lgKZHDS6ebMD9wzcxfJhGRrcuDBk/Tqu0LkEQaNeuHe3atSMtLU2vAjipFAQfpIhgpVKJWi09v1s/qr/PlkVwFcPCwoL09HRJ4snX15cbN27QsqVufafT0tJYsGAB1tbWzJgxgzfffFOrnsEXLlzg+++/Z/LkyXh4eKBSqfhm9RfU0kTRWnkXm7ykUu2ilTU5lOrO4LeL56wGjB0PwP9++J7s87/TIuk+NR/cRCB/a+uabwsMvM3oan2rzDXZGabSxSOVNHcTznh0xSommRQrKxo0z6a++QOtXg8jhYaO9R6Q7W3I4aye9NdSABdFoVBUigAG/YocnZycCA0NfQar0pLq7U9l/oU4OzsTHh4u2V6KkNVoNCxbtgyNRsOHH35Ily5d6NWrYj+VmJjI4sWLGTBgAKNHjwZg584fIO8BLzVzoqZr6dHP1HQNB48+wLdBR8aNb1Z4vMvLXeHlroReusR3p/bQvLYGP4d4FEK+kP07xY4HOfZ0eBz9LQ0TI+jYzBC12pBz1w0xEPIFnaV18ehveSgUClp1soVOL7H7qJqBQ8ZrZfcklSGAId/v3r59u1KurTfV3GfLIriK4eDgQFxcnM4C6tGjR4SEhJCcnEx8fDwBAQFlbt0XZc+ePYSGhjJjxgxsbW3Jyclhx44dbNy4kddff51WrVqVsNFoNHz66ae4u7sXq2JVKpWMf3c6Go2G79Z/hV1qBK2No3BU56c4qDHgdxqRaNOcsZPL7qIxYOgwGDqMo0eOcObIPhpkPOSRgw2tPO5jaRCv1ethaZBNF7eb3HV2oU6NdGyNdY9yGyvU5GVV3qhMqTg5OREfHy9JBJubm0vaEXgqCDzzqIIgCNOAseT/rroKvAm4ANsBO+ASMFIURen7zDL/KhwcHPj99991tlOr1axfv57IyEiWLl1KYGBgYS1Bedy4cYMNGzYwbtw4fH19EUWRX3/9ldmzZ9OwYUOGDx9eqt3mzZt58OABn3zySbHvhtdfHwrAoUMHOXnuJq2bOeNZywhBEBBFkSthWZy5EM+4CVPKzHtu1rw5zZo3JyIigo0HNtPUQyQ7z5DaXjXo6KzdZ9rQENo1MSRPY8jpawIN6kqTJxZm1U/W2NrakpwsbWpqpfIcfPazpvq9W15g7t+/z+nTp0lMTKRFixYMGjRIq6rUI0eO8MsvvzBr1iysrKy4f/8+CxYswMXFhZEjR5baISA9PZ0FCxbQuXNnPv7448LjxsbGBAYGolar2bt3L7t27aJ79+50755fDBYaGkpISAiTJk2iTjm5VqPHTwbgh63foYgKxcs0g4tp1gyYMEPrUcNde/SAHj34btVCRlod18rmSeyEFOJyXSWJYEEAQzFL0nUrE0dHR8LCwsrsHfxvRRAEN2AK0EAUxazHTdWHA32B5aIobhcE4WvgLWBNJS5VppqQkZHB1q1biYiIICkpicDAwBKT5kojPDycdevWMWbMGCZOnEhmZibLly9HFEVGjBhR6hAcjUbD559/jqWlJYsXLy78bhAEodBHnz59mrlz5+Lu7s7YsWNRKBQkJSXx6aef0q9fv8Je9KXRu3cfoA+nT5/ij51naeznyM3bD/H0bsuEt/21ej28vLzwmvoJR4/+StOa4dhV/FKUwECRn3ohFeNqOENCHlZUecgiuAogiiK7d+9GFEXef/99FAoFoaGhzJkzB19fX4YNG1ZqsVPBJLAGDRqwZMmSwuM1a9Zk0aJFJCUlsXz5ciwsLBg5ciQuLvmdfPfv38/58+f5v//7vzI7FhgaGjJo0CAGDhzI4cOHmTlzJjk5Ofj5+ZXbw/BJhgaMAkYxf/585syZo/uLA5hZO6DRCIVbbLpgKmSTlG0KEne5jBSVlWslHUdHR0mRqSrBs+85aQiYCoKgAsyAGKArUPAtvwn4GFkEy1TAmTNnuHr1KoGBgZiZmREbG8vSpUuxs7Nj5MiRODg4lLApOgls6dKlhX7UzMyMWbNmoVarWbFiBampqQwePLiw4j88PJy1a9cyduxY6tevX+aa2rZtS9u2bbl27Rrz588nIyMDGxsbPv74Y613F9u2bUfbtu1Ys2YNI0aMwsbGRufXpm5dLx49lCaCAQSki2AjpR5TRGV0R+4TLKMPUVFR7Nq1i/79+xdLim/WrBnNmjXj9u3bzJs3Dw8PDwICAgpzlsqaBFYUOzs75s2bR3p6OkFBQUD+EIXOnTvzySefaLU+QRDo1asXvXr14uOPP2bMmDGSnqc20ZGysHOqSWaMCRboHpU1FDTkqKV/So0U1W9XXKlUSh6ckpSURGpqJaaAPMOIiCiKUYIgfAbcA7KAw8BF4JEoigW/dh4AFXfRl/nXkpmZyZYtW2jcuDHjxo0rPO7s7MyCBQtITU1l+fLlGBkZERgYSM2aNYH8SWDBwcGMHj26zF0aQ0ND3nvvPTQaDevXry8c7ezs7MySJUu07ldbMATn888/5/3335f0PL29vQkPD5c0jMPFxYVrd/MA3YtzAQQ9hJXx82vqUCVQqVQ8eKBdvcszoZpHsWURXEmIolg4ZGHy5MllRlbr1q3LokWLiImJYfHixdSoUYO0tDR8fX217gVrYWHB7Nmzyc3NJTg4WPJEMBMTE0l2kJ9moVarJTUdd3b3IDXKHAsDiakJEsYiF2AsVD8RDPDw4UNu3bqlU5eHI0eOEB0dXeyL/bmjnz+1FwThQpF/rxVFcW3hqQXBFugP1AEeATuA0hqByqEkmVI5d+4coaGhBAYGlllzYWVlxUcffUR2djZffvklubm5WFlZkZWVVSz6Wx4FbboAFixYwNtvvy1pvcbGxpJ7GLu6unLx4kVJItjIyIisLA1SRbCBHsLK2AiysrIqf5ywjiiVSk6ePEn79u21To+4fv06x48fZ8KECc94deVQvTVwdQ9kV08SEhJYtWoVTZs2ZciQIVo5KBcXFxYuXMirr76KtbV1mcUP5WFkZKRXKxUjI+nJVi4uLpKrX93d3UlSV1zkVxYKPTSNsUJ6Q/nKZOrUqdy7d4/g4GAuX75c7mOTkpJYvXo1Li4uvPHGG3r9P+tFQZGF1BskiqLYssht7RNX6A7cEUUxQRRFFfAj0A6wEQSh4NeZOxD9vJ6yTPVArVazbt061Go1EyZM0Kro2MTEhOnTpzNr1iyioqKYMWOGJDGq0eNHfI0aNYiLi6v4gaXg7OxMdLT0j0KOSrrf1WdAm5WFoNe6K4vhw4djY2PDunXrOHz4cLn/7yqVis2bNxMVFcWkSZOe+SCmMtHXZ1eBKHKFn0hBEEwEQTgnCMJlQRCuC4JQ5j66IAivC4IgCoLQssixDwVBiBAEIVwQhF5Fjvd+fCxCEIQZ+j+V6kNoaCiDBw+mdu3aOtvWqlVLr8bYUrfJAb2ah7u5uREWFibJ1srKigw9UhoM9CiyMFKoyMvLk2xfWQiCQLdu3Rg/fjxZWVkEBwdz6tSpEgUnR48eZd++fYwbN+7fUEh3D2gjCIKZkB9q6Qb8BfwGvP74MW8AeyppfU8F2Wc/fTIyMnBwcKBdu3Y62+rbLlEfn12zZk1u3LghydbW1la/HsYq6QJHaSjd1spcJDrqvmT7yqRhw4aMHz8eLy8vNmzYULhbXJSwsDCCg4Pp06cPPXv2rKSVvjho83srB+gqimK6IAhK4KQgCAdFUTxT9EGCIFiSX3l9tsixBuRXX/sBrsAvgiD4PL57NdCD/By884Ig7BVF8S+9n1E1wNnZmYSEBK3a4TyJiYmJXpWz+jhUMzMzyT2MnZ2dOXXqlORrZ4vSo5MGSBOxogipOYZkZmZWWv9IfREEgTZt2tCmTRuuXr3KunXr8PT0pEWLFmzbto0OHTrQtWvXyl5mIc8yMCCK4llBEHaS3wZNDYQCa4EDwHZBEBY8Prbh2a3iuSD77KeMlZUVaWlpku312V1RqVSSUxqcnZ357bffJH3GBUGQlL5WQI5KeuDCxAjUag2GhrqfIyVNAKRHz6sCnp6eeHp6Eh0dzebNm7GwsKBv377873//w97ensmTJ1f2EgupAsFcvajwHS7mK66CEWDKx7fSVNh8YCnwQZFj/YHtoijmAHcEQYgAWj++L0IUxUgAQRC2P37sv8KhOjo6cuXKFcn2+kRk9Rmz6e7uTnh4OC1atNDZ1sHBgfh47Xr8lkaORBGsEQUeZhkQme5AHfPSJxqVRnqeKSfivanfMaDaCuAnadSoEY0aNSIiIoKDBw/y1ltvYWxsXNnLKs4z9qiiKH4EfPTE4Uj+8UvVHtlnP330bWGlj88u6CGrbWvJojg5ORETEyP52vqsW5UnXQSnpKm5cF2kdSNjFArtXnt1nsjpK3kYmPrQsVMXydeuSri6ujJmzBiSkpL48ccf6dOnT6ldRyqVaq6CtXqXCoJgIAjCn0A8cEQUxbNP3N8MqCmK4v4nTN2AovsSBZXXZR0v7drjBUG4IAjChYSEBG2WW+Wxt7fXK6VBL8ekRyTYzc2NmzdvSrI1NjaWHMFOSkridlIeCeg2ljReY8eOlFa85D+HLJ+J7LrrR1iKE+UtQxThRkZtjqW/TN835uJZ10vSmqsyXl5e+Pv7Vz0BDI9zzCTeZAqRfXbVQh+f7e7uTmxsrCRbCwsLsrKk9zqXGsFWqVTcvhNHZLRBuf72SbJyYM+xLMydXqaO32B++Dmbs1eyUeeVf5KYRIH9J0Satw2gXfsuktZclbGzs2PUqFFVTwCDfj67CvhtrUSwKIp5oig2Jb9opLUgCIXJg4IgKIDlQGl9WEp7imI5x0u79tqCQpcq+QaQgKGhoV55pvpsrRkZGUmeCObk5MT9+9JyrSIjI4mNjeXWrbJHHpfGwYMHWbZsGaOmLeCo0IPdyY2JFl3KtdGIAifTvbhsN4jhUxZgZ2dHw0ZNGTJhHiYt32fH3SZcTnJFIxZ/G2bkmXIotjHGjSfw2tC35AbmzxtBAIUeN5lCZJ9dtTAzM5Psd318fCQXegmCIPn7Ii0tjXv37nH8+HGdAhjXr19nxowZDBoykuiMBmz5KY2wuwo0FZzixh0N+/9Q8srgqfj5NcTJyYnhgf+hXvMAfvwlj98vZJKTW/wkeXkiJ0PziEyozYDXx2pVsCjzFNHXZ1cBv61Two8oio8EQTgG9AauPT5sCTQEjj0WDc7AXkEQ+pEfLahZ5BRFK6/LOi5TAfp2aYiLiytz2lt53L59mxs3bhAXF4eTk5NWNhqNhk2bNhEdHc3XX3/N6tWrSU5OZsCAATRr1qxMu+TkZIKCgmjRogWLFi0CYPjINwHYv3c3J2+fopVtMrWFe8V2YxI0dhxN9aRHwPulbh/W8axLnQlziY2NZcfejdQxj6N5jWgiMmtyS92AV0eNkZR7JyNTFZF9dtXAzc2N8PDwcn1eWfj6+rJr1y5J142KiiI6Oprr16/j5+entd3Ro0c5dOgQCxcu5NixY8ycOZMOHTrQp0+fMv2jSqVi7dq15OTk8PnnnwP5vYY7dOjI1atX2LT3IM0bWNGwrohBkVNk5cDh01l4+XVnyIgmJc5rbW3NUP/8At+9e7djZ5FO2yZmpGYqOHMlj+69R+hVeCjz76ZCESwIggOgeuxMTclvM1Q4nkwUxRT4Z5/6scP9QBTFC4IgZAEhgiB8QX6RhTdwjvyogrcgCHWAKPILMbSbyyiDUqmUXChx584dtm3bxpQpU7R2HNnZ2axevRoLCwtWrlzJ8uXLMTAwwN/fv9wOF3///TerVq1ixIgRvPlmvoB999130Wg0fPvtt+zatYtevXrRoUOHYlHXQ4cOcezYMWbPnl3qGl/tNxAYyPFjv3Hqwk+0rJGKl+JvTmfUJcO1E8NHV9w+ztnZmeHjZ/Lo0SO+Dl5G74Ej6edTT6vXQ+YZUvmBgWqP7LOrHm5ubty4cUOSCD527Bjnzp2jZ8+euLlpN8dFFEW2b99OWFgYq1evZuPGjfzwww/06dOHNm3alGlXMFjJ09OzsA/9wIEDGThwIEePHuXDDz+kRYsWDBo0qFjR3F9//cXGjRsZN24cvr6+Jc7bqFFjGjVqzN9//83G/22naT0rmvrA7SgNlyOUDB46tcIiPFNTU4YMexOVSsX2bd9gbm7OoNd1bxUq85Sp5j5bm0iwC7BJEAQD8tMnfhBFcb8gCPOAC6Io7i3LUBTF64Ig/EB+8YQa+I8oinkAgiBMBn4mv5v2RlEUr+v5XP4VqFQqsmIj+Grpx7z5znStt38iIyNZvXo1o0ePxt7enkWLFuHg4MCoUaPK7TF44cIFtm3bxnvvvVfogOfMmUNubi5ffvklWVlZvP766zRo0KDQRqPR8N1333H//v1Sm8MrFIrCyXO7du1i1qxZdOzYkTZt2vDll1/SpEkTFi9eXOFz6tzlZejyMn+GhvLFrk2MeXc29va65Q3b2Njg5t0CF1d3nexknhFyCsrTQPbZzwATExPJQxhOHjjIo8i7RLZqhaenp1Y2mZmZzJs3j44dO7J8+XKWL1+OWq1mxIgReHmVXasQHR3NihUr6NOnDyNGjABg4sSJAISEhLBv3z5efvllunXrViz4cPz4cQ4cOMCMGTNK/U7o2rUrXbt25dKlS8yZMwdfX1+GDBnC5s2bSU9P12oQSO3atRn39gwSExNZvGoJr/QbyjD/Vlq9HgUolUq6dntFcstNmadMNffZgj7ttp43LVu2FC9cuFDxA6s4WVlZLFy4EGdnZyZNmqR1RPe3X46QevkA3U3CEESRi6IP1zJtGDr23XIrh1euXAnAhAkTiqVSJCcn8+WXX2JmZkZAQECxKENOTg5fffUVJiYm5U4r0mg0BAcHExMTw2uvvYaTkxMrV67k9ddf12nS0K+//sp3333HypUrJY1YnjNnDvPnz9fZDuCPP/7A2dmZunXrSrKX+QdBEC6Kotiy4keWpKWntXj+07aSr60Y8bPka8s8G14Uny2KImvWrCEqKorp06dr7aPu3bvHvoXLaHTyCsb3Ykjo2Jw7dZzpNP5NmjQpufVfwIEDBzh79ixTp04t5tvVajWrV68mKSmJgQMH0rRp02Jr3LFjB9euXWP27Nnlps399NNP/P7777z00kt07dqVlStXUqtWLUaOHKnV84L89Lj58+czY8YM6tXTfRdtxYoVDB48WOvodlFyc3P58ccfJQ2NkilOZfpsqHy/LY9Nfs5cvHiR8+fPM336dKKioli4cCFGRkZMmzatTKelUqnY+MU8OlnF0Nk4orAcpb1wldbmhvy5PYldqZb08n8bDw+PQru///6blStXMmrUqFIdrq2tLR9//DGZmZkEBQWh0WgYMWIEKSkphISE8M477xQ7X2koFIpCkbx161Y2bNjAqlWrdO4v2a1bN86fPy9JAEP+aySKoqRiNicnJ+Li4mQRLCMjU4KoqCh27dpFv379sLCwYMOGDSQmJvLOO+/g7Oxcpt2GL4Kw+SOUVj+fRng8/cvp2AUcjwvEhN3nDy9XGo4YTKcuXQptMjMzmT9/Pm3btmXevHklzmloaMjUqVMLay127dpFz5498fLyYsWKFfTs2bNUuyfp27cvffv25dSpU0yZMoXPPvtM5120unXr0qpVK0kCGMDDw4OYmBhJItjIyEivTkcyMgXIIvg5kZ2dTUhICF5eXoVbU/Xq1WPOnDncu3ePL774gtzcXN59991iQvDEb0d5eHEfARbhmOWVrC5WoqaVeJ3mlgquH3jELylWvPTqSI4dP05eXh6LFi2qsJDOzMyMmTNnolarWbZsGXFxcQQFBen8HAMCAkhISJDcYP1p9NKUMj7SycmJa9euVfxAmWdLwQhOGZkqgCiK7N27l5ycHCZPnly4Yzdt2jRSU1PZunUrd+/eZfz48cVSHKKiovjf/CU0/OMqZrdLdtMRRJEapy9jd/oyqdf/Zm29Hbj27Y6hmSknT57kvffeq7AVlkKhKKyz2L17N+vXr2fdunU6F023a9eOX3/9VWcBXICtrS2xsbHl/hgoi3r16hEWFkbLlvLmTbXlBfDZsgh+DoSGhnL27Fn8/f1LjXTWqlWLGTNmEB8fz/r160lKSmLSpEns2/I1nSxj6GByq4xmRP9ggIbGmjAaWgqcO5aJm3s3Bg4cqNM6DQ0NmTZtGgsXLtTJriiVOZY5NjZWkgi2sLDQaxqUzFOkevtTmReEmJgYduzYwauvvlpqDq+VlRVvv/02WVlZfP/99wQHBzNixAj+PP47Fr9fpNWhU4XR37IQAOtLN2h26QbplyM40bER8z9fpvNu1sCBA7ly5YpePX2l4u7uTlhYmCQRXLduXY4dOyb52jJVhGrus2UR/AzJyckhJCQET0/PwuhveTg6OvLee++RkpLCV4tm807N26VGf8tDgYivcJ8HEifDmZiYoKnAeZeHPhPpCnppmpmZ6Wzr7e1NdHR0sQI9bZH7AVchqkDfSJl/L6Iosm/fPrKysvjPf/6DgYFBuY83NTVl9OjRqFQqFi1cSLuDZ7G+qlsvdACL67exat9Isi/Sp21mbm6u5FQyV1dXjh8/TpciKR3aYmhoSE5Ojs52MlWMau6z5Yaoz5Bvv/2WgQMH0rlzZ53srK2tsbS2wVSU1lzdnAySYu9JsoXKG8vs6upKeHi4JNt69epJHuQhU4WoxpOHZKo/e/bsoV69egwbNqxCAVwUpVJJpy5dMMhVS762iR62+ohga2trkpOTJdk6OjpKHuQB+n1fyFQR/g0T42SkYWZmho2NjSRbIys7cpA21laJmrwMaU4NKm8ss7u7u84T5QqwtrYmIyND8rXVaulfQDIyMi8Gpqamkn12o0aNyLa3lnxtk1zpU0T1HcscFxcnydbS0lLyJDzQ7/siLy9Pp0l2MjKlIYvgZ4goipI/pI5udUgXpE3BEQAzA+kpDfpEFZRKpeRZ9fqMZRZFkdTUVJ3tMjMzWbdunaQ0CpmnTEGRhdSbjIyeODo6ShaENWrUINdUWuACwDhXuiA0NTUlOztbkm1BKpkUBEHQS4Cnp6frHIDQaDTs3LkTKysrOZWtstHXZ1eB/z9ZBD9DLC0tSU9Pl2TboEEDUhTSowrmhtJFsD5OzdnZWfKXiJWVFZGRkTrbJSQksHr1apo0aUJwcDBHjx7V6sfHuXPn2Lx5M/7+/jr1NJZ5hlTjbTWZ6o8+IhhAZSJdBBvlSE8NKBjLLIX69evz4MEDyddOS0vTuY4kJyeHb775hpYtW7Jp0yZ27typlYi/f/8+q1atonXr1gwaNEjqkmWeJtU8HUIujHuGFPSftbS01NnWy8uLy0ekR2TNDfTbWpM6lrlWrVrcuXOn3HHKpXH06FGOHDlCly5dmDVrFk2bNmXIkCEV2h06dIiHDx8yceLEwtZsN2/eZN26dbi5udG7d+8SuX1ZWVls2bKF+vXrM2HCBJ3WKfMsqRqRAZl/L46Ojpw4cUKyfa6x9ACCMls/EXzz5s1yB3CUhaWlpSThX9CHvl27dsyfPx8LCwumTq14/PGVK1f4448/GDFiRGHqSWJiItu3b8fIyKiwH3NRRFFk9+7daDSaYu3qZCqb6u+zZRH8DCmIKpQ34rIsFAoFWUh3qGZ6iGBbW1siIiLw8fHRye7UqVP89ttvmJqacurUKfz9/alTp065Nrm5ucybN4+mTZvy6aefIggCQ4YM4dSpU8ydO5eaNWvy1ltvlXB6Dx8+ZNu2bXTr1o3evXsXu8/HxwcfHx/u37/PN998g62tLa+++irGxsacP3+eixcvEhAQIOnHicwzpnr7U5lqjlKp1Ks+QC8RnJVLUlKSpDaP9vb2HDhwQGe7e/fusXLlSqytrZk7dy59+vShbduKJ4B99dVX5OTk8Omnn2JsnB/9joyMZMmSJUB+L+Unu/zk5uYSEhJCrVq1Skwhtbe3Z/To0aSmprJnzx5UKhX9+vXDzs6OqKgodu7cyYABAyoc3iRTCVRzny2L4GeIk5MTV69elWyfpTGQ/AYzQkVubq5O+b0Fv7ZDQ0O5du0aDg4OjBo1qtyRzJAfWV2xYgWOjo589tlnQH6h2YoVK0hNTWXIkCH4+fmVsDt27BiHDh1i6tSpuLi4FLuvXbt2tGvXjsuXLzN//nwsLS2ZMmUKhoaGHD58mPj4eCZMmFBu6kbNmjUZO3YsCQkJbNu2jYyMDBo1aqRVuzoZGRkZXck2kv6VapycysWLF+nRo4dOdpcuXWLr1q2Ym5szf/58hgwZUuEUN1EU2bp1FQTHnwAAIABJREFUK3fu3GHJkiWFQYYffviBffv20aVLF3r06FEi5/b+/fsEBQUREBBA8+bNi93n6enJrFmziImJYfXq1aSlpfHuu+9iZ2fHtWvXOHHiBMOHDy9X5FtZWREQEEBWVhb79+/n4cOH2NnZ8c4778jRX5lngiyCnyEWFhaSc4IB0nIViMa66WARuKKoz8VHtuybOxcvLy/8/f0r7L2bkJBAUFAQnTp1YtmyZQCkpKSwfPlyTExMCAwMxN3dvYTdmTNn2LlzJx988EGxhumGhoa89957aDQa1q1bx/fff0/fvn1p06YNubm5LFiwgIYNG7Jo0aJyixuaNGlCkyZNuH37NkuWLEGlUjFkyBB69uyp9Wvi4ODA6NGjtX68TCUhUO17Tsr8u8lUGiAqBASNbgXRaY19uNrSl4Rjx/jzzz8JDAwsERh4kpycHNasWYNSqeTzzz8H8ovGvvrqK7Zt20b//v1LCFXIF7IrVqxg8ODBBAYGFrtv6NChDB06lCNHjvDhhx/SunVr+vfvj4GBAcHBwWRkZLBw4UJMTEzKXJeLiwv//e9/SUpKYuvWrcTFxdG5c2cmTZqk9ethamqqVTqcTCXzAvhsWQQ/QwRBkNQMPDMzky+//BJRbU1mXgtaWSbjmRdZoRhOVVhxKN0L3+6BTHns/O7evcuCBQtwcXFh5MiRpbb/2bt3L2fPnmXOnDnFnJu1tTUff/wx2dnZBAUFoVKpGD58ON7e3mRlZbFq1SpsbW0Lo7+loVAoCvNut23bxo8//ohKpeKDDz7QaWZ83bp1mTVrFps3by41qizzglDN88tkqj8qlYq8vDyd+gRrNBo2b95MgpkRZ998DY+7cTgdu4BCXX5aWp6Rksg+7THq+zLvjgwA8v3/8uXLAcpMKfvzzz/ZsmULkydPLlZ/oVAomDx5MgCbNm1i9+7ddO/enU6dOgEQEhLCrVu3WLRoUbm5uz169KBHjx6cPXuWmTNnkp2dTWBgIK1atdL6NSmI4G7evFnn6LZMNaKa+2xZBD9jmjdvztq1a6lXrx4dO3assKXLqVOn+PHHH/m///s/HB0dAThz+hSnT+ymhdUjfPMiUDwxQ1kErinqcSbTjXEfzCq2beTh4cGnn35KYmIiX3zxBdbW1gQEBODs7ExiYiJBQUG0a9eu3FHJJiYmzJgxg7y8PFatWkViYiJpaWlMnz69wmhFUUaMGEHPnj05cOCATgJY5l9ENXeoMtWfnj178s0331CjRg1eeeWVClPKCvJqhw4dyhtvvAFAVFQUOz4LotbdeFx/O49BKZ0f0ht5c6V1Pd6Y/1Gx4ISZmRmzZs0qTClLS0vj9ddfx8/Pj9zcXNasWYNCoSg3+AAUrmXPnj3MnDmTnJwcBg0aREBAgNavxUsvvcRLL73EZ599ppMAlvkXUc19tiyCnzHNmzenefPmXL9+nXXr1uHh4UGPHj1K5DcV5NU6ODiUcG5t2rajTdt23Lhxg+/2bKKpZQqNxHAM0JCmsORQuhd1X/ZnQjlOyt7ennnz5pGens7y5ctRqVSoVCrmzJmj9ZhiAwMDpk6dSmhoKHfu3NFJABdQo0YNSf18ZWRkZJ4H7u7ujB07lri4OLZu3YqZmRn9+vXD1NS02OPKyquF/G4N7y5fRnJyMpsWf4brnWjcT1xCmZqBRmlIZN/2KHp2Yuqbb5S5jidTyrZt20Z6ejpTpkzB09NT6+fTv39/+vXrx4cffkiHDh10f0HQb6gFIHkss4zMs0YWwc8JPz8//Pz8uHPnDhs2bMDR0ZG+ffuiVCrLzKt9kvr161O//mIePHjAN5u/orZpOnfU9oz7YLbWRQMWFhbMmTOHb775hsGDB2stgIvi5OTEsWPHdLYrQB6VKVMm8helTBXBycmJN998k0ePHrFz504AXnvtNWxsbAr71Q4cOLBEXm1RbG1teXfJQrKysli/eCm2N+/z0M6CUQs+wtbWVqt1FKSU3blzh8OHD+skgAsQBEGvIUj6iGBra2sePXqk9fOVqWZUc58ti+DnTJ06dRg3bhyxsbFs3ryZqKgoXFxcKtzaKoq7uzvjP/yUjz76iE8+mStpHe7u7ty4cUPSkAgHBwcSExMlXRf0jyrIvKgIIMgV4DJVCxsbG0aOHElmZib79u3j7t27ZGZmsnDhwgp74hZgamrKO598xLJlyxg9erQkQejk5CR5shvoNwRJn8CFs7Mz8fHxsgh+Ian+Prt6r74a4+zszJgxY6hduzZjx46VdA59HJOrqysRERGSbPVxpqCfCDY2NpY8llmmilNQaSz1JiPzDDEzM2PYsGG4uLjw8ccfay2Ai+Lu7i55Ip2ZmZlevlOfSHBeXh55edJ6z+s7hU+mCqOvz9bCbwuCsFEQhHhBEK4VOWYnCMIRQRBuPf7T9vFxQRCEFYIgRAiCcEUQhJLtUZ5AFsHVGBMTE8kt2JydnfUalamPQ9VHvDs5OZGQkCDZXqaKU41n0Mv8OzAyMpLsw3x8fIiKipJ8bX0CEPrY1qhRQ/LuX8HkVJkXFH18tnZ++1ug9xPHZgC/iqLoDfz6+N8AfQDvx7fxwJqKTi6L4GqMq6urZOdia2vLo0ePJF9b3/wyUdStj2YBclRBRkamMrG3t5f8Q9zX17fSgg/62NaqVUuy3zU1NSU7O1vytWX+3YiieAJIeuJwf2DT479vAgYUOf6dmM8ZwEYQhHIr+GURXAWQKgg9PT2JiYmRZKtQKCRt5xWgT1TB3NxcUocIlUrF6dOnsba2lnxtmarM4/wyqTcZmeeAPpFNCwsLMjMzJV9bX7+bkpIiydbb21uyeD98+DBOTk6SbGWqOnr67Hy/bS8IwoUit/FaXNhJFMUYgMd/Oj4+7gbcL/K4B4+PlYn8zVHJ2NrakpycLMm2Xr16ekUV9HGoUqMKsbGx3L9/n88//5zbt29rbRcWFkZwcDD9+vXDx8dH0rVlqgFyOoRMFUff7X198nr18dk1a9YkLCxMZzuVSsXOnTs5dOgQJ06c0Dpok5SUxOrVq3F3d9dpwqdMNUP/dIhEURRbFrmt1Wc1pRwr9w0rd4eoZAocannz1MvCxcWFhw8fSr62VCF7+fJlbt26xYwZMxg9enSFc+oL2LBhA0lJSQQFBaFUKlm5ciUpKSkMHDiQJk2alGqjVqvZvn079vb2hZOQZF5QXoARnDIvPjVq1CAp6cndWe3RpyZCqVSi0Wi0bolZQHJyMgcOHADyxWmfPn20svv99985cOAA77zzDm5ubuzevZtZs2bRsWNHevXqVeY6jhw5QkxMDOPHj9e7kFqmClN5PjtOEAQXURRjHqc7xD8+/gCoWeRx7kC5LVXkSHAl4+TkRHx8fMUPLAWFQiE5qvDTTz8RFxfH7Nmztb6+RqNh0aJFXLhwgaCgID755BMuXLjA9OnTOXv2bJl28fHxfPDBB9SvX5///ve/mJqaYmhoyLRp05g7dy7nzp1jzpw5nDp1qpjdzZs3WbNmDb169aJ37yfz4mVePOR0CJmqj0KhQKPRSLaXKoJv3bpFWFgYM2bMIDQ0VGu7kJAQVq9ezbx58/jiiy9QKpXMmjWLbdu2lWmjVquZO3cuDx48YNGiRYUTPgcOHMinn36KkZERM2fOZOfOnajV6kK7pKQkvvrqK1xcXBg1apQsgF94nko6hBT2AgWTZt4A9hQ5Pupxl4g2QEpB2kRZyJHgSsbR0ZEzZ85Isv3tt9+4desWP/zwA4MGDdIqxzc7O5t58+bRpk0bli9fTmpqKlu2bOH+/ftMmDCh2Bz6oly9epVvv/2WiRMn4u3tDeRPkAsMDCQvL489e/YwY8YMunXrVmxO/DfffENcXBzz5s0rdTCHQqFg3LhxAOzYsYNZs2bRqVMnEhMTsbW1ZfLkyfKkIRkZmReCiIgIoqOj+eqrrwgMDMTKykoru4Lds+XLlwOwe/dutm/fTp8+fejSpUupNsnJySxatIhevXrh7+9feLx79+50796dM2fOMHfuXNzc3Bg3blxhVPfUqVP873//Y8qUKbi7u5d67m7dutGtWzcuXrzI7NmzqV+/Pi4uLsTExDB27Fi9ivBkZIoiCMI2oAv5ucMPgI+AxcAPgiC8BdwDhjx++E9AXyACyATerPD8UouyKoOWLVuKFy5cqOxlPDVEUWTv3r2cPn0aDw8PJkyYoNU2V3p6OkFBQdSpU4eAgABCQ0PZvn079evXZ9iwYSXGexZw6NAhTp48ydSpU3FwcCh2X1ZWFtu2bSM8PBx/f//C9ASNRsOyZcuws7NjzJgxGBgYlPt8jhw5wrFjx/D29ub69esMGDBA51GdmzdvxsPDg06dOulkJ1P5CIJwURTFllJsW/rWEM8HS4/4K14OkXxtmWfDi+azAc6fP8++ffswMDBg2rRpWglZjUbDt99+S1xcHNOnTycxMZEVK1ZgZ2fHyJEjS/jjAiIjI1m9ejWjR4+mUaNGJc558OBBTp48SZs2bejfv3/hfT/88ANhYWG8++67Fa7v+vXr7Ny5E3Nzc9LT0/H29sbf31+n4MOtW7fYv38/06ZN09pGpmpQmT4bKt9vyyK4koiJiWHnzp307duXunXrFjoiU1NT3n333TJ/SR8/fpwDBw4wY8aMEnnEt2/fZv369dSqVYvAwEAsLS2B/Ojv/PnzadmyJQMGDCjXualUKnbt2kVoaChNmjTh0qVLjBs3Dl9fX52e39KlS3njjTckVQVHRUVx48YNunfvrrOtTOWit0Ndq12uYmkoumyVRXAV40Xy2VlZWYSEhODr60uHDh2Ii4tjy5YtJCcnM2XKFBwdHUu1i4yMZM2aNfj7+9OsWbNi96WmphIUFISRkREBAQHUrPlPOuOKFSsQBIEJEyaUG1kVRZETJ05w5MgRatWqRUREBD179tTZf547d44zZ84wZcoUnewK2Lx5MyNHjpRkK1N5VKbPhsr323I6xHNGFEX2799PZmYmkyZNKoys+vn54efnx507d/jss8/Iy8tj2rRpWFhYAJCRkcGXX35JzZo1Wbp0aannrlu3LosWLSI2NpbFixdjb2+Ph4cHFy5cYOrUqVoJUqVSyfDhwxk6dChz5sxh8eLFklqpNW3alHv37kkSwQ4ODvz+++8628lUcwTk3F6ZKsnFixc5f/48AQEBhcEFJycn3n//fVJSUtiyZQsPHjwollKm0WjYtGkT0dHRLFmypNRdPisrK+bOnUtOTg5BQUHk5ubSoUMH9u/fz8iRI2natGmFaxMEgc6dO9O5c2fWrVvHiBEjSohtbWjQoAEHDx7U2U7mX8wL4LNlEfwciYuLY8eOHfTu3RsvL69SH1OnTh1mzpxJbGwsX3/9NY8ePaJdu3b89ttvTJ8+HXt7+wqv4+zszMKFC0lJSWHBggUsXbpU57xahUKBi4uL5F7Crq6uXL58mVatWulsq89EJhkZGZmnRXZ2NiEhIXh5eTFx4sRSH2Ntbc1//vOfYillXbt25ZdffmH48OG8+WaFaYkYGxszffp0NBoNEyZMYNWqVRgbG+u83mbNmnHv3j1JItjCwkIeaiHzr0MWwc8BURT56aefSE1N5e233y43r7YAZ2dnPvjgAx49esSsWbNYvXq1zte1trbGyspKcmGZra0t8fHxZW7zlYezszM//fSTpOvK/FvRbpa8jMzzIDQ0lLNnz+Lv769V3q+pqSljxowhNzeXDz/8kGXLluncykyhUODo6KizXQHOzs4luuzoglzQJqMb1d9nV+84djUhNTWV1NRURowYoZUALoqNjY1e03b0aczu7v7/7d15XJRV+/jxz0FkEQVR2RRNcTc1IzPzSSuzMg3UchfXzMqltOXb4tKTYS6VqFmK+liGaFbqzyVtccksH80lTVNx3xdAUERknfP7g5EHlWXmHpEZuN6v17xilmvuM5AXF+c+97kCOXDggKHYSpUq2bSXpuwIUUpJswxhJ7Zt28bLL79s8Q4ON7i4uBAYGGi4kPXx8TG8baYtsSBFsDDA9mYZxUqK4LvA09Pzpr0UrWXLXovp6emG2zJXrVrVqq5uuTk5OckekcJ6sk+wsBMeHh6GY23JfTVr1jTckc7V1dVwvgcML3+D7AI6LS3NcLxwUMWzT/AdU/wjKAVsndW0JaF6enpy+fJlQ7G+vr6cO1dgs5UCSREsrKJw6BkFIW7w8vIyfCasXr16nD171vCxbZnNtSXWx8eHuLg4w/HCAdmas+0gb0sR7ABsSUzVqlUzPKvg6enJtWvXDB/blnGfO3eOw4cPG44XQojiEhgYyP79+w3F1qlTp9gmH2zJ2VprNmzYYNNMtBB3W6FFsFLKTSn1p1Jqj1LqH6XUB3m85mWl1F6l1G6l1O9KqUbmx/uYH7txMymlmpmf+1UpFZPrOeuvviolXF1dDe+WULduXcMJVSllOKFqrYmLi2Pfvn1WxV25coXx48cTFBTEyZMniYyM5O+//zY0BuFozBdZGL0JQHL2nWTLUrIjR44YirV1WYEtRXBqairfffedVTFZWVnMnj2bnTt30qxZM+bMmcPPP/9sU2tp4ShszNl2kLctWQCUBrTVWicrpcoCvyul1mqtc/f6XaS1ng2glAoFpgLttdbRQLT58SbACq317lxxfbTWJWMn9SJUrVo1Dh06ROPGja2ObdiwIatWrTJ03LS0NC5dukRycnLOfsWWOH36NDNmzKBz587s3buXqKgoOnXqRKtWrQqMW7duHb/88gvvvfceXl5eQHZ7zq1btxIZGUmTJk14+OGH5aK5kswO1oiVAJKz7wBPT0+SkpJycpE1/Pz8bFrSYHTSQ2tNQkICZ86cybflcV5SUlKYMWMG/v7+VK5cmbFjx9KgQQP69OlTYNyhQ4eYM2cOAwYMyPn91LRp05zGTX5+fnTo0EGWxpVkDp6zCy2Cdfafwsnmu2XNN33La5Jy3fW49XmzXsBiY8Ms3WwpgpOSkti7dy8ZGRlWJaLdu3cTFRVFv379mDhxIj4+PvTr1++2LnW5aa1ZtGgRhw8fZuLEiTkXWXTv3j2ny91jjz1G+/Y3t1m80TWpfv36TJ48+abnlFI8/PDDPPzww/z999/MmTOH2rVr07ZtW8NXXws7Jn/g2Exy9p3h6+tLbGysoSLYy8uLq1evGjpuamoqx44d4/Lly1SsWNHiuIsXLzJ9+nTatGlDVFQUmZmZ9OzZk7p16xYY98cff7B8+XL+7//+L2c7zLZt27J9+3bef/99/P39eemll27Kt1lZWcyfP5+EhASmTJlyWy6uXbs2tWvX5ty5c3z99ddUqFCBkJAQ3N3drfhOCIfg4DnboktBlVJlgJ1AHeBzrfW2PF4zDHgdcAHa5vE2PYBOtzz2pVIqC1gKhOsSvJjoxikuIxugx8XF8fPPP9O5c2erCr9Zs2aRmppKjx49GDNmDPXr16dXr14FJqL09HRmzZqFUopPP/0UgIceeojExESmTZtGuXLlCAsLo1q1ajfFnT17lunTpxMaGnrb7EGZMmUIDQ0lJCSE9evXM3r0aBo3bkyvXr3YuHEja9as4b333sPb27vAz9O0aVOaNm3KkSNHmDJlCq+//rps6VOS3LjIQthMcrbt/Pz8OHfuXKFFZF7i4uI4ffo0SUlJVm2x9ssvv7Bx40ZGjBjB1KlT8fLyIiwsrMBtMrXWLF++nJ07dzJu3Djc3Nx45plnyMzMZObMmSQmJtK5c+fbGmhcv36dGTNm4OvryyeffHLb+z744IM8+OCD7N+/n/DwcNzd3Xnttdc4deoUs2fPpn///jRp0qTAz1O1alVeeOEFEhIS+Prrr3nssceoX7++xd8PYedKQM5W1uQwpVRFYDkwQmud52JPpVRv4Gmtdf9cjz0EzNNaN8n1WDWt9VmlVAWyE+pCrfXXebzfEGAIQI0aNR44efKkxeO1J5s3b0ZrTZs2bSyOMZlMhIeHU7NmTZo3b87SpUtxdXVl5MiRBRZ/Z86cISIigl69etG8+f9ach87doy5c+cSGBhIWFjYbTMce/fuZcGCBbzyyivUrl07z/dOSUkhIiICrTW9evUiKCiIJUuWcODAAcaOHWvxFjtbt24lKiqKVq1aFXrKLS/r1q2jYcOGtxXjonjZ1Ie+kY/e/vXzho/t9GBksfagt0eSs41LSUlh7ty5jBgxwqrJhwULFnD+/Hl69erFsmXLiI+PZ8SIEfj7++cbk56ezvjx4wkODqZLly45S76Sk5OJiIjA2dmZ3r17c88999wUFxcXx7Rp02jTpg1PP/10nu9tMpn46quvOH78OE8++SStW7dm69atLFu2jDfeeKPAceV28uRJZs6ciYuLCx9++KHVZ+KuXr3K+vXr6dy5s1VxomgVZ86G4s/bVhXBAEqp94FrWuvb/3TMft4JSNRae+V6LAKI01p/lE/MAKC51np4Qcdu3ry53rHDMZejaa3Ztm0be/bsoXHjxrRq1arAta07d+5k8eLFDB8+PKcXPcCpU6dYuHAhGRkZjBo16rZZhsjISJKTkxk2bBhubm55vndsbCwzZszA29ubvn374u3tzezZs8nMzGTUqFEWfZ7MzEw+++wzjhw5Qvfu3Xn00Uctistt5syZDB9e4I88X/v27SM9PZ3g4GBD8aJo2JZQffX2KBuK4OazpQjOg+Rs444dO8a6dessWtsaHx/PpEmT6Ny5M4888kjO40lJSSxcuJBTp04xZMgQgoKCborbuHEjP//8M6+99lq+BWl6ejrTp0/n+vXrdOvWjYYNG7Jy5Uq2bdvG2LFj8831t1q+fDm//PILwcHBDB482KKY3P7880/i4uLo2LGj1bFaaxYuXEjfvn2tjhVFpzhzNhR/3i502k4p5QNkaK0vK6XcgXbA5FteU1drfWM/q47A4VzPOQHdgDa5HnMGKmqt480XbjwLrLP1w9gzpRQtW7akZcuW7N27lzlz5hAUFMQTTzxx01/UJpOJjz76iMDAwDzXWtWoUYP33nuP2NhY5s6dS2JiIq+++iqZmZlMnTqV7t2706JFiwLH4uvrS3h4eM5a3HPnzvHGG29YddrP2dmZUaNGER4ebqgAhux1c/Hx8VSpUsXqWF9fX3bt2mXouMKOyTpvm0nOvnOCgoIYMmRIztrW8uXLExoaetuSsqioKM6cOcMHH3xwW5MNT09Phg4dyvXr11myZAmRkZH07t2bhg0bMn78eO677z4++uijAidFXFxceOuttzCZTERGRvL555/ToUMHJkyYYNXn6dKlCzt27KB///6FvzgPAQEBbN++3VCsXNBcQjl4zrbk3HUAsMC8xswJ+FZrvVopNR7YobVeCQxXSrUDMoBEIPe/sDbAGa31sVyPuQI/mZNpGbKT6VzbP45jaNKkCU2aNOHIkSPMmzcPf39/OnTowN69e1m0aBFDhw6lVq1aBb6Hr68vb7zxBleuXGHBggUcPHiQTz/91KoLDzw9PRk3bhwffPCBoXVvYPwqZoDq1atz4MABWrdubXVs5cqViY+PN3xsYafkF+WdIDn7Dsu9tvXbb7/Nuc4hMzOTjz76iNDQ0EJnON3d3RkwYAAZGRksX76cWbNmMW7cOKpWrWrxOJycnHjllVeYOnUqHTp0MPRZqlSpQlxcnFXHvcHHx4fz588bOq4ooRw8Z1uyO8TfwP15PD4u19evFRD/K9DylseuAQ9YM9CSqE6dOtSpU4ezZ88SERGBq6srkydPtmqtlZeXFyNGjODdd981fOWtLReX3WjLbOSv/KpVq/LHH38YKoLLlCkj+1AKkQfJ2UWnUqVK9O/fn+TkZFatWsWOHTv44IMPrNpCsmzZsnTv3p3Dhw8bKkTBtpx9zz33cPHiRUPHdnNzk2YYokQx3ihc3DHVqlXjueee48KFC4a2/bKlqQXYtrl6hQoVuHLlilVb+dzg6+tr016aooQpAVcai9KhfPny9OrVi8zMTKsK4NxsKWQrVKhAYmJioTvq5KVevXqcOHHitt0iLCV7/oocJSBnO/ZijhLEz8/PcHtjKL5+8ba0Zfby8iI5ObnwF4pSQmVvvG70JoQDsaWYvLGUzIh69erZNPkg21KK/7ExZ9tB3i7+EQgge2bh2rVrhuNtSai2tGW2pc+9UsqmhCoXWpRADtx+UwhrlC1b1vCSrqpVq3L48OHCX5iH4mzL7O7uTkpKiuF4YYccvG2yFMElhC3FZEBAgOE+9w0bNuTMmTOGj200oa5fv15mJEoipYzfhHAgVapUMTwja+tSMlsuaDaad/fv38/FixcpU6aM4WMLO2RLzraDvC1FcAlhy1/ngYGBHDp0yFBs5cqVuXLlitVxNzZwT0pK4u2332bz5s0WxSUmJvLFF1/g5+dHjx49rD6uEELYg8DAQGJiYgzFent7k5SUVPgL85GRkWEo7tdff+XkyZOMHTuWBQsWWHysqKgoTp8+zbBhwwx1TRWiqMiFcSVE+fLlrW7ReYOfnx+//fab1XFXrlxhwoQJZGVlERERQb9+/ahcuXKhcSdOnODzzz+nZ8+eDBgwAJPJxA8//MC7775Lq1atCAkJyTNuw4YNnDp1isGDB8sscEmklF2sERPCUkopTCaToQuaAwICWLfO2FbLRpeSmUwmJk6cyJUrVxg/fjzdu3enQYMGhcYlJyczffp0atWqxdSpUwHYtWsX//73v6lcuTLDhg3L83tw8OBB1q9fT48ePQztBy/sXAnI2VIElxDVqlUjJiaGBx980Kq4hIQEZs+ezaVLl5g3bx69e/emXLlyhcZ9++23xMTEMGbMGDw9Pbly5QoRERG4ubkRFhZGYGDgbTEmk4moqChOnTp101ZwTk5OhISE8Oyzz/Lrr78yZswYGjRoQFhYGACXL19m8eLF/Otf/6Jt27ZWfT7hYOzg9JgQlqpUqRIJCQmGCjw/Pz9D11NkZGQQGRlJYmIiH3/8MWFhYQQEBBQat2fid9MxAAAgAElEQVTPHqKionjllVeoXbs2JpOJL774gsWLF9OpU6d8u29u2rSJH374gXfeeYdKlSrlPB4cHExwcDAHDx5kwoQJuLi4MGrUKFxcXMjMzOSbb76hSpUqDB06VK7fKMkc/Gdrddvk4uToLTgLExUVZail5PHjx1m2bBkuLi7UqlWL9u3b4+xc+N83a9as4bfffmPMmDGUL1+ekydPEhkZSUBAAH379s1z27OkpCTCw8N58sknefLJJ297PjU1lWnTppGRkUHPnj1zmnCcOnWKzz77jK5du/LQQw8VOrYdO3awcuVK3N3dCQgIoFevXnIazQHY1IKzsb/e/r3xlqpODT+Rtsl2pqTn7B07duDu7s69995rVdzVq1eJjo7m+vXr+Pr6EhoaSoUKFQqN++eff/jyyy956aWXqFu3LikpKURERADQu3fvPJssmUwmJk+ejK+vLwMGDMhzTe6CBQs4cuQI7dq1o02bNiiluHbtGtOnT6d69eoW/V46ffo00dHRpKSk4OvrS/fu3fH19bXguyGKU3HmbCj+vC1FsB250ae+YcOGPPLII4X+9WwymVi6dClly5alU6dOKKU4e/Ysa9eupWLFioSEhORZOCYmJjJt2jTuv/9+OnfufNvz8fHxzJgxAy8vL8LCwvDz8wNg6dKl7N27l5EjRxa6L3BWVhYzZ87MmSVJSEhg9OjRFhXnuc2fP59BgwZZFSOKj80JdZmxdq4ATvWnSBFsZ0p6zr527RqLFi3Cx8eHjh07WnRtxu+//05MTAy9e/fG3d2dq1evsmrVKtLT0wkJCclzSVlmZiZz5swhNTWV119/Pc/nZ8yYwdWrV+natWtOUb5v3z6++uqrnKK5MCtWrGDbtm1Ur16dY8eO8fbbb1s9y/3111/Tq1cv2U/YQRRnzobiz9tSBNuhffv2sWXLFmrWrEm7du3yXGt16tQpVqxYwXPPPUe1atVuez4+Pp7Vq1fj4uJCaGhozobuP/30Exs2bGD06NGFrh9OTk4mIiICJycnLl++zBNPPEH79u2t+iwmk4nRo0czceJEq+JuMDo7LoqHFMEit9KSsy9cuMCaNWvw8PAgNDQ0z+6dycnJLFy4kAceeCDPZWupqamsXr2aK1eu0L59+5y8fuDAAebNm8eQIUOoX79+geMwmUzMnTs3Z+eIwMBAXnjhBat3ZPjoo4947733rIq54eeff6ZJkyYWLdEQxa+0F8GyJtgONW7cmMaNG3Ps2DH+85//4OvrS4cOHXL2lVy2bBlOTk4MHz4839niKlWqMGDAAJKSklixYgUZGRmcOHGCxo0bM3nyZIvGUb58ecaOHUtqairz58+3ugCG7PW+Ri7Wy81oW2bhaJT5JoRj8ff3Z9CgQSQmJvLdd9+hlCI0NBQvLy8AtmzZwv79++nfv3++7e3d3Nzo2rUrmZmZrF27lh9++IHU1FTS09P5+OOPLbr4zsnJiZdeegnILmSHDBli6PPYkrP9/PyIjY2VIrhUcPycLUWwHQsKCiIoKIhz584RFRWFm5sbly5dolOnTtSoUcOi9/D09KRPnz78888/1KxZk8cee8zqcbi5uRneUgds277N09OTpKSknF8mogQrAS04Renm7e1Nv379uHbtGqtWreL69etkZGRw3333MXjwYIvew9nZmZCQEEwmE/PmzePVV181NBajjTggewLEaFtmX19f9u7da/jYwoGUgJwtRbADqFq1KoMGDeLq1at4eHgY2o6nRo0ahjsMQfFsrg7/ayctRXAp4eDb7QgB4OHhQc+ePUlLS8NkMuU7+1sQJycnQ3E32DJxUb16dWJiYmjZsqXVsT4+PsTHxxs+tnAwDp6zHXv0pUyFChUMFcCQ/Zf91atXDR/bloTq4uJiuIj29fXl4sWLho8thBDFxdXV1aZC1hbp6ekYvebHlrbMzs7OZGVlGYoV4m6TIriUsHVNrS1FcEBAAMeOHTMUe2N9mSgtlA03IcQN5cqVMzzx4efnx5kzZ+7wiETJZEvOLv68LUWwsIgtyyFsaQ/q4eFBSkqK4WMLR1L0PeiVUhWVUt8rpQ4qpQ4opR5WSlVSSv2ilDps/q/1CyGFsDMBAQGGJxC8vb25cuXKHR6RKHlszNl2sJ5YimBhEaWU4ULY39+fEydOGIo9fPgwe/bs4ejRo4bihYNRTsZvlpkO/Ki1bgDcBxwA3gHWa63rAuvN94UodjfaMhtRp04dQx3pbhzX6LUcly9fZs+ePWzZssXwcgzhQGzJ2Xawnrj4RyAcgq+vL3FxcYZijx49yt69e62alcjKymLu3LksXbqUyZMnc/ToUWbPns2+ffsMjUE4iqI7raaU8gTaAP8B0Fqna60vA52ABeaXLQBu7yAjRDGoVKkSiYmJhmIbNGhgeEnD6dOnOXfuHPv377cqbt26dUycOJFx48ZRvnx55syZw7p162zaqULYO1kOIUo4k8nEmTNn+PTTT60qZK9fv87HH3/MkSNHmDp1KrNnz2bSpEmcOnWqwLijR4/y9ttv06JFC9555x3KlCnDU089xUsvvcTly5eJjIxk27Zttn4sUfJUUUrtyHW7dZPUICAO+FIp9ZdSap5SygPw01qfBzD/V3q9Crtgy4XBW7ZsYf369ezevdviGK010dHRfPnll8yaNYtNmzbx/vvvs3379gLjkpKSGD9+PImJiUyePBlPT0+aNm3KSy+9RM2aNZk3bx6rVq0iMzPT0GcRoqjIFmmlRFZWFmfPnmXChAm88cYbuLm5WRQXExPD3LlzGTRoENWqVSM6OprTp0/nJLf8bNu2je+++44333wTf39/AMaNG0d6ejrTpk0jLS2Nbt260aBBg5wYk8nEV199xYULF5gyZcptO2EopXjkkUd45JFH2L17N5GRkdStW5fHH39cmmmUFLb9HOML6TzkDAQDI7TW25RS05GlD8KOnT17lo0bN/LWW2/l2Rk0L1evXuXDDz+kbdu2zJ49m2XLlrF48WI6dOjAo48+mm/cmTNn+Oyzz+jSpQt9+vQB4JVXXgFg4cKFrFixgrZt296Wb9evX8/PP//Me++9l+dWlnXq1KFOnTqcPXuWBQsW4OXlxbPPPmvx7yBh5xz8d6+0TS4Fjh49ypo1a+jWrRtZWVlER0dz7do1Xn/99QL3342IiMDV1ZUXX3zxpoYX169fZ/HixcTExBAWFkaTJk1uem7mzJl4eXkV2K3IZDIxa9YsYmNj6dSpE97e3nzxxRf06tWL4OBgiz/boUOH+OOPPxg4cKDFMaLo2NSCs0k1vX3ly4aP7RQ0rsBjK6X8ga1a65rm+63JLoLrAI9prc8rpQKAX7XWBfenFRaRnG3MlStXWLRoEQ8//DB16tRh0aJFHDt2jIEDBxbYOnnlypXs3LmTkSNH3tTowmQysWbNGv744w8efvhhQkNDc57TWvPNN98QExPDmDFjcHbOf25s9erVOe/x6KOP8tlnn1GnTh169uxp8We7dOkSX331FW+88YbFMaLoFGfOBovy9ihgMKCBvcBAIAD4BqgE7AL6aq0NXbQkRXAJlpWVxffff0+5cuV49tlnb/rr/dKlS0RFRREXF8fw4cNvanF59OhRZs2axcCBA7n33nvzff/09HSWLVvGX3/9RUhICK6urixZsoRRo0ZZPGsBsGDBArZu3crnn39uaB/kqKgo+vbta3WcuPNsSqhNq+ntK18xfGynWmMLPbZSajMwWGsdo5T6N+BhfuqS1nqSUuodoJLW+v8MD0TkkJxtvU2bNnHs2DF69ep102xpWloa3377Lfv27aNr1648+OCDOc8lJycTHh5OmzZt6NChQ77vrbXmt99+Y926ddSrV4+2bdvy2Wef0bFjR1q3bm3xGDdv3sz8+fOZOnWqoa5ykrPtR3HmbCg4byulqgG/A4201teVUt8Ca4AOwDKt9TdKqdnAHq31LCPHl+UQJdSxY8f44Ycf6Nq1a5493CtXrszIkSO5evUq0dHRHD9+nMGDB/PDDz/g7OzMxIkTC2137OLiQs+ePenevTvR0dH89ddfTJ061eqx9u/fn8uXLxtuBCJKkKI/tTYCiFZKuQDHyJ5VcAK+VUq9AJwCuhX1IIS4VVJSEtHR0Tz00EN5ntlydXWlb9++ZGZmsnLlSpYuXUq7du1ITU1l27ZtvPXWW1SuXLnAYyilePTRR3n00UfZuXMnY8eOJTIy0urW9q1bt2bTpk2GCmDI/t2RmpoqSyJKgqLP2c6Au1IqAygHnAfaAr3Nzy8A/g1IESyyT3t9//33uLq6Mnz48ELXylaoUIGXX36Z1NRUJk6cyCOPPMKTTz5p1TGdnJzo1KmT4W3QALlgQtwVWuvdQF6zDk/c7bEIccNvv/3GkSNHGDBgQKEd5pydnXnuuefo0qULK1eu5I8//mDKlClWH/OBBx7gnnvusboAvsGWBkp+fn7ExcVRvXp1w+8hSowqSqncp4vmaK3nAGitzyqlPiF7cuI68DOwE7istb5RNJwBLD/1fAspgkuYRYsW8fjjj1u1HAHAzc2NDh06kJCQYOi4FSpU4Nq1a4ZiwbZmHDf20pSZ5JLAsS+yEMJau3fvJjU1lUGDBlkVp5Ti2WefZevWrYaPbbQABttytq+vLxcuXJAiuESwOWfne0GzuXFRJ6AWcBn4Dngmj5caXtcrVUMJ4+TkhJ+fn6FYf39/jh8/bihWKYWrq6uhWLAtoVaqVMlw8S7siXLoTdeFMKJcuXIFXqBckDJlylCmTBnDx7alCHZ3dyc5OdlQrJ+fn+FudsKe2JizC8/b7YDjWus4rXUGsAxoBVRUSt2YxA0EjHWFQYrgEqdKlSrEx8cbivX19eX8+fOGj13QVcWFsaUI9vPzM7yXprAzDtx+UwgjbsyKGmW0s5utsQEBAYbH7e3tbbgJiLAzRds2+RTQUilVTmWv7XwC2A9sBLqaX9MfWGF0+FIElzC+vr6G/8J2d3e3qbOPLQkVjBfCUgQLIRyVl5cXSUlJhuNtmc11cXExnPNr165tuAh2cnKSlsqiUFrrbcD3ZG+DtpfsmnUO8DbwulLqCFAZcxdQI6QILmFsLQhtTahG2dKWuVKlSnJqrcRw3PabQhhha6MfW/Kun5+f4SVwDRs2NNyWGeRi6JKjaNsma63f11o30Fo31lr31Vqnaa2Paa1baK3raK27aa3TjI5eiuASxsfHx3AxCbYVwbbE+vj4sHfvXqvjYmJimD9/vtU7Wgh7JGuChbCWLXk3MDCQgwcPGoqtUqWKodiEhAS++OILHnroIUPHFfakyNcEF7niH4G4o5ydncnKyjIcb8usgoeHB4cOHbI6bv78+Zw+fZqLFy/y9ttvs2XLlkJjMjMziY6O5tixYwwbNqzQ/TGFA1DZs2JGb0KURu7u7qSmphqKrVKlCnv27LE67vfff2f06NHce++9jB49mujoaIvi1q1bx6pVqxg8eDCNGjWy+rjCztiYs+0hb8sWaeI2GRkZVs0uaK1ZunQphw4dIjU1lS+//JIXX3yRoKCgAuNiY2OZMmUKXbp0ydkeKCsri1WrVvHuu+/y6KOP0r59+9viDh8+zI8//kiPHj3w9fW17sMJO1f8SVEIR+Lr68vevXtv6iBnie3bt/PNN9/QqFEj3nnnHZ5++mkef/zxAmMyMzOZMGECdevWZfLkySil6NatG9u2bWPcuHEEBATw0ksv3bZdZWJiIosWLaJNmza0a9fO6s8o7Jlj52wpgkugpKQktNZW/ZWVmZnJvHnzuHLlCmPHjqV+/fr07Nmz0I3bL168yLRp03jyySeZOHEiANevX2fJkiVERkbSq1cvmjVrdlvcggULOHfuHOPHj6dcuXI5j5cpU4bOnTvTqVMn1q9fz+jRo2ncuDG9evUiKyuLJUuW4OnpaVEjECGEcAQpKSmkpaVZvc3kmjVr+P333ylbtiy//fYbffv2LXRiIDU1lc8//5xy5crx6aefAtlNln766SfeffddWrRoQZcuXW6L27p1K0uXLuXVV1+9bX/fhx56iIceeoh//vmH8PBw3NzcGDlyJC4uLmzcuJETJ04wePBgm7bRFKIoKEe6QlP60Bfs2rVrLFy4EJPJxKlTp3jooYcICQkpdB/JgwcPMm/ePAYPHkyDBg2A7LbLc+fOpXr16oSFheHp6XlTjNaa5cuXs2vXLsaMGZNn+8uMjIyc1zzzzDM8+uijxMfHM2nSJEJDQ2nTpo1Fn2vr1q2sXr0aDw8PBg4ciL+/v4XfEXG32dSH/r7qesfaN4wfu9oow8cWRUNydsFudPi8cuUKR48eJSgoiD59+uDh4VFgXGJiIhERETRv3pzQ0FAge/Jj2rRplC1blj59+lCjRo3b4nbt2sWiRYt47bXX8mxUobVm8+bN/PLLLwQFBdG/f39MJhMTJkygdu3a9OnTx6LJhxMnTrB48WJSU1Pp0qVLnhMhwj4UZ86G4s/bUgSXEFu3bmXv3r306dMnZ2Z169at/L//9/9o2rQpXbt2vW29b1ZWFnPnziUpKYk333wzz45rFy9e5LPPPqNy5cqEhYXh4+NDbGwsM2bMoE2bNjz11FOFjs1kMrF27VrWrl2Lj48Pb7zxBuXLl7fq8yUlJbFx40Y6depkVZy4u2xLqDVsLIJHShFsZyRn5+/kyZOsXLmS5557LqfD59mzZ/niiy/w9fWlX79+eHt73xb3448/8uuvvzJmzJg882h6ejrTp0/n+vXrdOvWjYYNG5KWlsasWbNwdXXllVdesWh8u3btYvny5Vy+fJk333yTe+65x+rPGBUVRd++fa2OE3dPceZsKP68XehyCKWUG/Ab4Gp+/fda6/dvec3LwDAgC0gGhmit9yulagIHgBjzS7dqrV82xzwAfAW4A2uA17QjVeR2IiUlhejoaJo2bcqLL75403MtW7akZcuW7N+/n3HjxuX8JV+uXDkOHTrEnDlzGDBgAI0bN873/f38/AgPDycpKYmIiAiysrLIyspi7Nixec7+5sXJyYmOHTuSkJDAgw8+aHUBDNltmW3ZS1M4CFniYjPJ2fbNZDKxbNkynJ2db1vWVa1aNSZMmEBCQgIRERGUL1+esLAwqlatyuXLl5k2bRr33XcfkyZNyvf9XVxceOuttzCZTMyePZvo6GiuXbvGyJEjrSpkg4ODady4MV9++aWhAliUEg6esy1ZE5wGtNVaJyulygK/K6XWaq1zNyxfpLWeDaCUCgWmAjeuaDqqtc7rXMgsYAiwleyE2h5Ya/BzlEp//vknu3fvLvT0WaNGjZg0aRKnTp0iPDycsmXL4ubmxpQpU/Kc/c2Lp6cn77//Pu+++y7h4eGGWnXWrl2bs2fP5iy5sIas/xXCYpKz7dTp06dZvnw5zz33HIGBgfm+rlKlSowfP56UlBQiIiJIS0sjLS2N0aNH37Y0LT9OTk4MHTqUH3/8kYoVKxoqZF1cXEhLM7wFqxB2r9AKSGe70SC8rPmmb3lN7ik6j1ufv5VSKgDw1Fr/1zyT8DXQ2ZqBl3aLFi0iMzOTIUOGFLp+7IYaNWrw0Ucf4eXlxbvvvmtxAZybj4+P4bbMtm6uLkoBB95v0l5IzrZPv/76K3/++SfDhw8vsADOrVy5cjkXBw8fPtziAji3Bg0a2JR3bWlpL0qB0rBPsFKqjFJqNxAL/GJuZXfra4YppY4CU4BXcz1VSyn1l1Jqk1KqtfmxakDuf5VnzI/ldewhSqkdSqkdtjSBKGmysrJo1aqVoVhb9gIODAy0qV/81atXDR9blAbSMe5OkJxtf06fPs3zzz9vaPIhMDCQmJiYwl+Yhxo1atjUUdPWIlhWzJR0RdsxrqhZ9K9Ra51lPj0WCLRQSt22iFRr/bnWujbZPZ3HmB8+D9TQWt8PvA4sUkp5kvcnz/NfitZ6jta6uda6uY+PjyXDFYXw8PAwvL62fv36nD171vCxZVZB5E9lry8zehM5JGfbH6UUJpPJUKy/vz9Hjx41FOvk5ERGRoahWMCm2IoVK5KYmGg4Xtg7G3O2HeRtq/4k1VpfBn7lf2vH8vIN5tNk5h7Pl8xf7wSOAvXInkXIfT4oEDhnzViEcbbMKtSvX59z54z/qGxJqKKEUzj0aTV7JDnbflSuXJlLly4ZivXz8+P8+fOGj23L5IMtOdvf39+mWWhh52zN2XaQtwsdgVLKRylV0fy1O9AOOHjLa+rmutsROJwrtoz56yCgLnBMa30euKqUaqmyr3jqB6y4A59HWKBq1aqG2hsDuLm5kZKSYvjYtiRUNzc3rl+/bjheiNJAcrZ98vX15eLFi4ZiPTw8bLpAzZa8m5WVRWZmpqFYWz6zEHeDJbtDBAALzInRCfhWa71aKTUe2KG1XgkMV0q1AzKARKC/ObYNMF4plUn2Vjwva60TzM+9wv+221mLXGVsFQ8PD5KTkw1tN+bv72/ThRK2JFSjMxJJSUmcOnWKrKwsw8cWjqD4T4+VAJKz7ZC/v7/hM3Bg27UctswE37gY2tomRVlZWWzevJn77rvP8LGFI3DsnF1oEay1/hu4P4/Hx+X6+rV8YpcCS/N5bgeQ/wa1okA3/sI2UgR7e3tz+fJlw8c2WgSfPXuWzMxMvvrqK0JCQqhcubJFcZs3b+bw4cMMHTq00DbOwsHZwRoxRyc52z75+PiwefNmw/G2FMFGc3ZKSgopKSksWrSIkJAQ6tatW3gQcPToUdasWUO3bt2kw2dJ5+A525KZYGGH/Pz8iI2NpXbt2lbHOjk5UbZsWUPHXblyJWfOnGH+/Pn06tXLoqJUa83KlStJS0sjPDyc9PR0Vq9ezZUrV2jfvn1Ot6RbJScns3DhQh544AEGDRpkaLzCkSisvExBCIfh4uJi01k0ozk7JiaG2NhYPvnkE8LCwiwuSrdv386uXbsYOXIkHh4ebNiwgQ0bNtCyZct8Z3ezsrL4/vvvcXd3v60RiCiJHD9nSxHsoPz8/Ni/f7+h2MjISK5du8bkyZMZNWqURTMMKSkpjB8/ntatWzNz5kxOnDjB+PHjCQwMJCwsDC8vrzzjzp8/z/fff0/Hjh0JCgoCstf2du3alczMTNauXcsPP/xA27ZtqVOnTk7cli1bOHDgAP3795fZ39JEfmkKcZuNGzdy9uxZRo8ezYgRIywuZD/99FPKlSvHrFmzSE1NZdq0aTg5OdG7d29q1qyZZ8z169dZtGgR9evX56WXXsp5/IknnqBt27Zs27aNyMhImjRpwsMPP5xT6B4/fpxVq1bRtWtXqlatavNnFg7CwXO2cqQ9/KQPfTatNStWrGDTpk107tyZNm3aWPQX9/nz5/n000/p3r07LVq04NSpUyxatIi0tDRGjRqV70bsa9asYcuWLYwcOZIqVarc9FxsbCwzZszA29ubvn374uvrmzPG1atXk5KSQteuXQvsMGcymdiwYQNHjx4lODiYv/76i2bNmtGiRQsrvivCHtjUh75ZTb1j3Vjjx/YZXKw96MXtJGf/z59//smyZcto1qwZzz//vEUzu+np6YwfP5777ruPrl27cvXqVaKjozl58iQvvvhivmcCDx8+zOzZsxk0aBD33nvvbe85ffp0UlJS6NatG40aNcp5bufOnWzfvp0+ffpQoUKFAse2d+9e/vvf/xIUFERCQgKurq6EhobK7K+DKc6cDcWft6UIdjDnzp1j6dKlPPvss9SqVYvly5ezfft2HnnkEdq3b5/vRuzz5s0jMTGR4cOH3zazGhsby8KFC0lISODVV1/NKWRTU1P54IMPePjhhwkNDS1wXElJSURERFC2bFnat2/Pli1baN++/U2zu4XRWrNz504aNWpEuXLlLI4T9sPmhLp+XOEvzO/YVV6QItjOSM7OnlmNjo6mUaNGtGrVij179rB48WLq169Pz5498z3TtWnTJtasWcNrr71228zq9evXWbJkCQcOHKBnz57cf///loBPmzaNsmXLMmTIkAILbZPJRGRkJOfPn6d9+/YcPHiQOnXq0KZNG6s+39GjR3Fzc8t3WZuwb8WZs6H487YUwQ7ixrra1NRUunXrdluxu2HDBn766SceeOABnnvuOZyds1e6XLhwgU8++YSuXbvSsmXLAo+RlJTEwoULOX36NI0aNeLgwYO89tprOUWxJdLT03n//fcJDw8vcPZXlEy2JdRaesf6940fu8pAKYLtTGnO2fC/dbV9+vS57SLmY8eOMXfuXKpXr05YWFjOmbjMzEzGjx9Po0aN6NGjR4EzqxkZGSxfvpydO3cSHBzM9u3b6devH02bNrVqnBMmTKB///4Wt3MWJUdx5mwo/rwta4IdwIULF/j222/p2LFjvqe/2rZtS9u2bdm1axejR4+mYcOGZGRkkJiYyIcffmjRulpPT0+GDh3K9evXGT16NJ9++qnVp7ZcXFxo1KiRFMDCGDmVKkqA/NbV5hYUFMTEiRO5cOECU6ZMoVKlSjRo0IDffvuNESNGWDSzWrZsWbp3707Xrl0ZN24cH330kaFdJNq2bUtqaqrVcUI4es6WItiO3VhXe+3aNYYNG2ZRYRkcHExwcDBHjhxhyZIljB492urjuru74+fnZ3htl4uLC2lpabi6uhqKF0IIR7Vr1y7+/PNPevfune91Frn5+/sTHh5OUlIS77//PlOnTrU69zo5OVGtWrV8l8MVxtfXl/Pnz1u1fE2IksCx97Yo4eLj40lLS6Nnz55Wz6zWqVPHpl0VbNmT0sfHh7i4OMPxojRTNtyEKH5//vknL7/8skUFcG6enp5UrFjR8ORDQEAAR48eNRR7Y8tNIaxnS84u/rwtRbAd8/b2Lrae77YUwX5+ftIqU1hPKYfuQS8EZHfzNMqWfF+tWjXDHenKly/PtWvXDB9blFK25mw7yNvFPwKRL2dnZ5vaBNtSBHt7exsuZKUIFoYpZfwmRAlgNG8HBARw6tSpOzwaIQphS3k39sEAABQPSURBVM62g7wtRXAJZuuswoEDBwzFVqpUiYSEBMPHFqWZ455WE8JWtiwl8/Hx4cKFC3d4REIURpZDCDuVnp6O0S3wqlatyrFjxwzFOjk5GT6uEEKUVjVq1DB8Fs3V1VXyrhBWkiK4BKtQoQKXL182FOvn58e5c+cMxaalpcn6MmGMA68tEwKyi9Hr168biq1Xrx5nzpwxfGxLutDlRWtt+HeFKOVkTbCwV4GBgYav+DWZTMTExFi9d+Tff//N/Pnz6dmzp6HjitLMsU+rCQG27bRQr149zp8/byg2MzOTuLg4q+MvXrzIzJkzeeaZZwwdV5Rmtubs4s/bsk+wnXNyciIrK8tw84nNmzdTr149q7bd+fnnn9mwYQPvvfce06ZNIzU1lVGjRuHl5ZVvTHp6OosWLaJGjRq88sorhsYqhD1cKCGELXx9fYmNjeWee+6xOjYhIYFDhw6RkZFh1azugQMHmDdvHgMHDmTFihUcP36cwYMHU7du3XxjtNasXbuWK1euMHToUGlwJIxx8JwtRbCd8/X1ZdeuXTz44IMWx5hMJj788EOCgoLw8fFh9OjRtGrVig4dOhS4mfqVK1eIiIigSZMmTJo0CYCGDRsSHx/P/PnziY+PZ8SIEfj7+98Ut2/fPn777Td69uxJpUqVjH1QIQA5OSUcnb+/P0uWLKF58+ZWTT7Mnz+fuLg4unfvzpgxY6hXrx69e/cucL/3zMxM5s2bx9WrV/n4449xcnKiefPmpKamsmTJEubNm0ePHj0IDg6+KS4uLo4lS5bw1FNPUa9ePcOfVQhHz9nKkRbSl8Y+9FprNm/ezIEDBwgODi60GN6+fTvfffcdw4YNu2kmYtOmTaxZs4bg4GCee+6522YZ1q1bxy+//MJ7772X74xvUlIS0dHRnDx5kiFDhlC9enUWL15MQEAATz75pO0fVjg8m/rQ319b79g02fixvboVaw96cbvSmLMBDh06xKZNm6hWrRpPP/10gbOssbGxTJkyhS5duvCvf/0r5/ETJ04QGRlJYGAgYWFht+XlQ4cOERkZycCBA2ncuHGe752Zmcny5cvZuXMnTz/9NI8//jg//vgjly5dokePHjg7yzxYaVecORuKP29LEexAdu7cyc6dO6lfvz5t2rS5aZbBZDIxYcIEatSoQd++ffOd8d29ezfffPMN9evXp2fPnmRkZDBt2jTq169Pjx49LBrH9evXWbJkCUePHmXUqFEy+yty2JRQg2vrHZumGD+2Z1cpgu1Mac/Zp0+f5ueff8bb25uOHTve1kr+66+/5ty5c7z66quUK1cuz/eIjY1lxowZVKpUibCwMCpXrsz8+fNJSEjgrbfesqhVsslk4qeffmLTpk0MGDCABg0a3JHPJxxfceZsKP68LX8GOpAHHniABx54gIMHDzJ37lyqV6/OU089xZ49e1i8eDFDhw6lVq1aBb5Hs2bNaNasGceOHWPs2LGkpqby4Ycf4u3tbfE43N3dGTBgAFFRUVIAiztI2cXVwkLcKdWrV+eFF14gLi6OxYsX4+rqSmhoKKmpqUyaNImQkBD69etX4Hv4+voSHh5OUlISERERxMbG8vLLL9OkSROLx+Hk5MQzzzxDfHy8FMDiDnL8nC1FsANq0KABDRo04OTJk3zyySeUL1+eyZMnWzQjcENQUBDh4eFERUVZVQALUbQc+yILIfLi4+PDgAEDuHLlCkuXLmXPnj188MEHlC9f3uL38PT05P333+fjjz+2qgAWomg5ds527BK+lLvnnnsICQmhRYsWVhXAN7i5uRnez1IIIYR1vLy86NevH82aNbOqAM7Nlk6gzs7OZGZmGo4XoqSRItjB+fv7G96TEoz3qReiSDhwD3oh7gZbcnaVKlUMt2UWIk+25Gw7yNtSBDs4b29vEhMTDcfbMqsASJtOcQcpslOS0ZsQJZ8tRbCfn5/htsxC3M7WnF143lZKVVRKfa+UOqiUOqCUelgpVUkp9YtS6rD5v4bXdMpvDgenlLKpELUloXp5eUmrTXFnOfCMghB3g5OTk+FlbLZ0sxMiT0U/Ezwd+FFr3QC4DzgAvAOs11rXBdab7xsiRXApZ8tMsCRUcefJTLAQBQkICDCcd2U5hLjzim4mWCnlCbQB/gOgtU7XWl8GOgELzC9bAHS2ZfSiFMvKyiIrK8tQrJxaE0IIY4yewbvnnnu4cOGCodgyZcpgMpkMxQpRRKoopXbkug3J9VwQEAd8qZT6Syk1TynlAfhprc8DmP/ra/TgUgSXcpUrV+bSpUtWxyUkJLB69epC9yUWwiqyHEKUAhUqVODq1auGYhs2bMiZM2esjsvIyODrr7+WNsnizrJ9OUS81rp5rtucXO/uDAQDs7TW9wPXsGHpQ15kn+BS7Pjx42RmZvLdd99Rp04dnnzySYu2Wlu3bh1nz55lyJAhuLi43IWRilJBillRStxYSubp6WlVXFJSEj/99BNpaWksWbKEkJCQfDvN5bZ//342btxIz549qVy5stFhC3Gzos/ZZ4AzWutt5vvfk10EX1RKBWitzyulAgDD6zKlCC4BWrZsSWRkJPfffz8tWrQo9PUmk4mlS5fi4uLC22+/jVKK48eP85///AcfHx86duxI2bJlb4tLTExk8eLFtG7dmnbt2hXFRxGlnpycEiXfvffeyzfffMPBgwdp3749zs6F/yr+/fffiYmJYcCAAbi7u3P58mWWLl0KQEhICBUrVrwtJjMzk2+++QZfX1+GDRt2xz+HEEWZs7XWF5RSp5VS9bXWMcATwH7zrT8wyfzfFUaPIUVwCVC3bl3q1q3LX3/9RWRkJPXq1eOxxx5D5fEX2smTJ1mxYgXPP/881apVy3m8Vq1avPjii1y4cIGoqCg8PDwIDQ3F3d0dgI0bN3Ly5EleeOEFXF1d79pnE6WMzASLUsDT05MhQ4Zw5swZvvrqKypWrEhISEieuTU5OZno6GiCg4N54YUXch6vWLEiffv2JSUlhVWrVpGSkkLHjh3x9c1eHhkTE8O6devo0aMHVapUuWufTZQyRZ+zRwDRSikX4BgwkOzK+1ul1AvAKaCb0TeXIrgEuf/++7n//vuJiYlh7ty5BAYG8vTTT+dcDLF8+XKcnJwYMWJEngUyZDffGDRoEImJiXz77bc4OTmRnJxMq1atePzxx+/yJxJCiJIrMDCQwYMHEx8fz+LFi3FxcSE0NDSnm9yWLVvYv38//fr1y5mQuFW5cuXo0aMH6enp/PDDD8THx+Pi4oKvry9Dhw7NN9cL4Qi01ruB5nk89cSdeH8pgkug+vXrU79+fU6fPs2XX35J+fLliYuLo3PnzlSvXt2i9/D29qZ///6kpKRQpkwZmf0Vd4n8whalT5UqVRgwYABJSUmsWLGCjIwMMjIyuO+++xg8eLBF7+Hi4kKXLl3Iysri2rVrVq83FsIYx87ZUgSXYNWrV2fw4MFcvnwZT09Piy56u5UlF10IcWcoULImWJRenp6e9OnTJ6cZRn6zvwUpU6aMFMDiLnH8nC1FcCmQ1wUTQtgnx55VEOJOMFL8ClE8HDtnF1rCK6XclFJ/KqX2KKX+UUp9kMdrXlZK7VVK7VZK/a6UamR+/Eml1E7zczuVUm1zxfyqlIoxx+xWShne7FgIIUQ2ydlCCGEZS2aC04C2WutkpVRZ4Hel1Fqt9dZcr1mktZ4NoJQKBaYC7YF4IERrfU4p1Rj4CaiWK66P1nrHHfkkQgjH5+Cn1uyE5GwhxN3h4Dm70CJYZ/d2TDbfLWu+6Vtek5TrrseN57XWf+V6/B/ATSnlqrVOs2XQQoiSSOHop9bsgeRsIcTd4fg526I1wUqpMsBOoA7wea7uHblfMwx4HXAB2t76PPA88NctyfRLpVQWsBQI10abqQshSgbZzumOkJwthLgrHDxnWzSPrbXO0lo3AwKBFubTZLe+5nOtdW3gbWBM7ueUUvcCk4GXcj3cR2vdBGhtvvXN69hKqSFKqR1KqR1xcXGWDFcI4YgU2afWjN5EDsnZQogiZ2vOtoO8bdUItNaXgV/JXjuWn2+AzjfuKKUCgeVAP6310Vzvddb836vAIiDPfr9a6zla6+Za6+Y+Pj7WDFcIIUo1ydlCCJE/S3aH8FFKVTR/7Q60Aw7e8pq6ue52BA6bH68I/AC8q7X+I9frnZVSVcxflwWeBfbZ9lGEEI5P2XATIDlbCHE32ZKziz9vW7ImOABYYF5j5gR8q7VerZQaD+zQWq8Ehiul2gEZQCLQ3xw7nOw1aWOVUmPNjz0FXAN+MifTMsA6YO6d+lBCCEekHH59mZ2QnC2EuAscP2dbsjvE38D9eTw+LtfXr+UTGw6E5/PWD1g4RiFEqVH8a8QcneRsIcTd49g527FHL4QoWZQyfrP4EKqMUuovpdRq8/1aSqltSqnDSqklSimXIvt8QghRktiSs+1gFlmKYCFEafMacCDX/clAhNa6LtlLA14ollEJIYS4q6QIFkLYCUV2SjJ6s+AI2TsfdATmme8rsvfI/d78kgXk2ilBCCFEfmzN2cVfglrULEMIIe6Koj89Ng34P6CC+X5l4LLWOtN8/ww3twkWQgiRHztY0mAL5UgNf5RSccBJg+FVgPg7OByj7GUcIGPJj4wlb5aM5R6ttaHNYZVSP5qPYZQbkJrr/hyt9Zxc7/8s0EFrPVQp9RjwJjAQ+K/Wuo75NdWBNeamEMJGJSRng4wlL/YyDpCx5MfeczZAvNa6oH3Mi5RDzQQb/UEBKKV2aK2b38nxOPI4QMaSHxlL3op6LHchEf4LCFVKdSC7YPYke2a4olLK2TwbHAicK+JxlBolIWeDjMWexwEylvyUgJxd5Ip/QYYQQtwFWut3tdaBWuuaQE9gg9a6D7AR6Gp+WX9gRTENUQghxF0kRbAQorR7G3hdKXWE7DXC/ynm8QghhLgLHGo5hI3mFP6Su8JexgEylvzIWPJmT2Oxidb6V+BX89fHgBbFOR6RJ3v6/03Gcjt7GQfIWPJjT2OxSw51YZwQQgghhBB3giyHEEIIIYQQpY5DF8HmFqe7zbcTSqnduZ5rqpT6r1LqH6XUXqWUWx7x/1ZKnc31Hh3Mj5dVSi0wxx1QSr1bXGOxNP5ujcX8fA2lVLJS6s3i+r4opZ5USu00x+1USrUtrrGYn3tXKXVEKRWjlHq6qMeS67VvKqW0UqqK+b6XUmqVUmqPOX5gcYzD/Nhj5vf9Rym1qbDviSj5ijAfSM6WnF0qcnZRjsX8WOnK21rrEnEDPgXGmb92Bv4G7jPfrwyUySPm38CbeTzeG/jG/HU54ARQs5jGYlH83RhLrueXAt8V9Jq78H25H6hq/roxcLYYx9II2AO4ArWAo0X9MzI/Vx34iex9WKuYH3sPmGz+2gdIAFyKYRwVgf1ADfN9X2t+PnIr+bc7/G9Qcrbk7FKXs4tgLKUub5eIC+OUUgroTnb7U4CngL+11nsAtNaXrHxLDXgopZwBdyAdSCqmsRiOL4KxoJTqDBwDrlkZd0fHorX+K9fdfwA3pZSr1jrtbo8F6ET2L+A04LjK3mWgBfDfIh5LBNndz3Jv6aWBCub3LU92Qs3MI7aox9EbWKa1PmWOjy1sDKL0kJx918YiOTtvDp+zi2gspS5vO/RyiFxaAxe11ofN9+sBWin1k1Jql1Lq/wqIHa6U+lspNV8p5W1+7HuyE8Z54BTwidY6oZjGYk18kY5FKeVB9nZSH1gxhiIZyy2eB/6yJJkW0ViqAadzvcaa1ruGxqKUCiV7JmXPLU/NBBqS3fBhL/Ca1tpUDOOoB3grpX5V2ac++1kwBlF6SM6+C2ORnJ3vWEpCzi6KsZS+vF3cU9GF3YB1wL48bp1yvWYW8Eau+28Cx8lu51eO7L/unsjjvf2AMmT/MTABmG9+/F9ANFAW8AVigKBiGkue8cU0lk+A7uav/435NFNxjCXX8/eSfSqrdjH+//I5EJbrdf8hO8kXyVjMj28DvMz3T/C/01ldyf4LXwF1zO+1sRjGMRPYCniY3+MwUK+484nciv5WTP8GJWdLzi5JOduzmMZS6vK23S+H0Fq3K+h58+mv54AHcj18BtiktY43v2YNEAysv+W9L+Z6n7nAavPd3sCPWusMIFYp9QfQvJjGkmd8MY3lIaCrUmoK2WuHTEqp1GIaC0qpQGA50E9rfdT8+uL6GVXP9dJA4FwRjqU22evY9mSfDSMQ2KWUagEMBCbp7Ix2RCl1HHhHa/3nXR7HGbJ7wl8DrimlfgPuAw4V9D0Rjk9ytuTsfMYiOdvynN2gmMZS6vJ2SVgO0Q44qLU+k+uxn4CmSqly5v9RHiV7sfdNlFIBue52IfuvLMg+ndZWZfMAWgIHi2ksFsXfjbForVtrrWvq7Laz04CPtNYzi2MsSqmKwA/Au1rrPywYQ5GNBVgJ9FRKuSqlagF1gXyLTlvHorXeq7X2zfWzOEP2L9kLZP+/+4R5vH5AfbLXA97tcawAWiulnJVS5cj+ZXzAgu+JKPkkZ9+lsUjOLrE5u6jGUvrydnFPRdt6A74CXs7j8TCyF9/vA6bkenwe2TMEAFFkr8H5m+x/FAHmx8uTfSXtP2T/D/RWcY2loPjiGEuu1/8bC680LqKf0Riy1wDuznUr9ErWIvwZjSb7FF8M8ExRf19uef0J/nc6qyrws3mc+8h1yu9ujsN8/y2y//3sA0Za8j2RW8m/FVE+kJwtOdvan5HD5uyiGov5fqnK29IxTgghhBBClDolYTmEEEIIIYQQVpEiWAghhBBClDpSBAshhBBCiFJHimAhhBBCCFHqSBEshBBCCCFKHSmChRBCCCFEqSNFsBBCCCGEKHWkCBZCCCGEEKXO/wdwNBo5D2c+wQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_test = 15\n",
    "n_hexa = 3\n",
    "name_error = 'LSTM predictions - 15 Days - 3 hex - 4q'\n",
    "path_error = \"imgs/ips_predictions/prediction_lstm_15_days_3hex_4q.png\"\n",
    "name_map = '15 days - 3 hex -4q '\n",
    "path_map = \"imgs/ips_predictions/maps_LSTM_prediction_15_total_data_3_hex_4q.png\"\n",
    "\n",
    "dict_pred,pred_lstm,pred_lstm_cases = lstm_hex_selected(data_days, dict_pred_sel_hexa,n_test,n_hexa,name_error,path_error,name_map,path_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
